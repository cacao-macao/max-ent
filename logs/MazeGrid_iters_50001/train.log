Using device: cpu

Using optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)

Iteration (0/50001) took 0.157 seconds.
                Mean final reward:        5.8438
                Mean return:              -2.7500
                Policy entropy:           1.5815
                Pseudo loss:              -4.58569
                Total gradient norm:      0.67548
                Solved trajectories:      8 / 32
                Avg steps to solve:       2.375
                
Iteration (100/50001) took 0.114 seconds.
                Mean final reward:        3.9062
                Mean return:              -5.2500
                Policy entropy:           1.5824
                Pseudo loss:              -5.91345
                Total gradient norm:      0.65034
                Solved trajectories:      7 / 32
                Avg steps to solve:       3.571
                
Iteration (200/50001) took 0.082 seconds.
                Mean final reward:        12.3438
                Mean return:              5.3125
                Policy entropy:           1.5796
                Pseudo loss:              0.95369
                Total gradient norm:      1.10675
                Solved trajectories:      16 / 32
                Avg steps to solve:       4.062
                
Iteration (300/50001) took 0.071 seconds.
                Mean final reward:        11.4375
                Mean return:              4.6250
                Policy entropy:           1.5675
                Pseudo loss:              1.42363
                Total gradient norm:      0.92364
                Solved trajectories:      18 / 32
                Avg steps to solve:       4.556
                
Iteration (400/50001) took 0.079 seconds.
                Mean final reward:        14.9062
                Mean return:              7.8125
                Policy entropy:           1.5598
                Pseudo loss:              4.76326
                Total gradient norm:      1.19715
                Solved trajectories:      18 / 32
                Avg steps to solve:       5.056
                
Iteration (500/50001) took 0.079 seconds.
                Mean final reward:        12.3438
                Mean return:              5.1250
                Policy entropy:           1.5402
                Pseudo loss:              1.64088
                Total gradient norm:      1.02428
                Solved trajectories:      17 / 32
                Avg steps to solve:       4.882
                
Iteration (600/50001) took 0.084 seconds.
                Mean final reward:        13.3125
                Mean return:              5.0625
                Policy entropy:           1.5203
                Pseudo loss:              4.54686
                Total gradient norm:      1.08024
                Solved trajectories:      17 / 32
                Avg steps to solve:       6.824
                
Iteration (700/50001) took 0.110 seconds.
                Mean final reward:        16.8438
                Mean return:              10.5625
                Policy entropy:           1.4856
                Pseudo loss:              2.65622
                Total gradient norm:      1.11860
                Solved trajectories:      19 / 32
                Avg steps to solve:       4.053
                
Iteration (800/50001) took 0.077 seconds.
                Mean final reward:        15.5938
                Mean return:              9.1250
                Policy entropy:           1.4364
                Pseudo loss:              3.07815
                Total gradient norm:      0.92994
                Solved trajectories:      19 / 32
                Avg steps to solve:       4.368
                
Iteration (900/50001) took 0.082 seconds.
                Mean final reward:        15.5938
                Mean return:              8.2812
                Policy entropy:           1.4169
                Pseudo loss:              5.26613
                Total gradient norm:      1.05886
                Solved trajectories:      20 / 32
                Avg steps to solve:       6.100
                
Iteration (1000/50001) took 0.076 seconds.
                Mean final reward:        18.8438
                Mean return:              12.8125
                Policy entropy:           1.3844
                Pseudo loss:              4.11660
                Total gradient norm:      0.89062
                Solved trajectories:      22 / 32
                Avg steps to solve:       4.773
                
Iteration (1100/50001) took 0.140 seconds.
                Mean final reward:        17.4688
                Mean return:              10.3125
                Policy entropy:           1.3541
                Pseudo loss:              4.95413
                Total gradient norm:      0.96122
                Solved trajectories:      20 / 32
                Avg steps to solve:       5.850
                
Iteration (1200/50001) took 0.085 seconds.
                Mean final reward:        22.0938
                Mean return:              16.7188
                Policy entropy:           1.2910
                Pseudo loss:              7.54870
                Total gradient norm:      0.94994
                Solved trajectories:      28 / 32
                Avg steps to solve:       5.571
                
Iteration (1300/50001) took 0.065 seconds.
                Mean final reward:        19.1250
                Mean return:              13.6875
                Policy entropy:           1.2587
                Pseudo loss:              2.66464
                Total gradient norm:      0.77631
                Solved trajectories:      23 / 32
                Avg steps to solve:       4.261
                
Iteration (1400/50001) took 0.097 seconds.
                Mean final reward:        20.4375
                Mean return:              14.5000
                Policy entropy:           1.2448
                Pseudo loss:              7.33546
                Total gradient norm:      0.75784
                Solved trajectories:      25 / 32
                Avg steps to solve:       5.520
                
Iteration (1500/50001) took 0.068 seconds.
                Mean final reward:        18.3438
                Mean return:              13.6250
                Policy entropy:           1.1721
                Pseudo loss:              6.42971
                Total gradient norm:      0.58780
                Solved trajectories:      27 / 32
                Avg steps to solve:       4.556
                
Iteration (1600/50001) took 0.066 seconds.
                Mean final reward:        22.1562
                Mean return:              18.8750
                Policy entropy:           1.1208
                Pseudo loss:              5.63319
                Total gradient norm:      0.60667
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.032
                
Iteration (1700/50001) took 0.089 seconds.
                Mean final reward:        24.0312
                Mean return:              19.6875
                Policy entropy:           1.1245
                Pseudo loss:              8.24377
                Total gradient norm:      0.63728
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.900
                
Iteration (1800/50001) took 0.083 seconds.
                Mean final reward:        22.4375
                Mean return:              17.3125
                Policy entropy:           1.0975
                Pseudo loss:              6.83632
                Total gradient norm:      0.67270
                Solved trajectories:      29 / 32
                Avg steps to solve:       5.517
                
Iteration (1900/50001) took 0.084 seconds.
                Mean final reward:        22.4375
                Mean return:              18.0625
                Policy entropy:           1.0643
                Pseudo loss:              5.45712
                Total gradient norm:      0.63229
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.933
                
Iteration (2000/50001) took 0.098 seconds.
                Mean final reward:        20.9062
                Mean return:              16.3125
                Policy entropy:           1.0097
                Pseudo loss:              6.39421
                Total gradient norm:      0.56000
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.387
                
Iteration (2100/50001) took 0.093 seconds.
                Mean final reward:        22.4375
                Mean return:              18.7500
                Policy entropy:           0.9729
                Pseudo loss:              4.55405
                Total gradient norm:      0.52612
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.200
                
Iteration (2200/50001) took 0.126 seconds.
                Mean final reward:        20.8438
                Mean return:              16.6562
                Policy entropy:           0.9813
                Pseudo loss:              4.08201
                Total gradient norm:      0.46667
                Solved trajectories:      27 / 32
                Avg steps to solve:       3.926
                
Iteration (2300/50001) took 0.144 seconds.
                Mean final reward:        23.9688
                Mean return:              19.3750
                Policy entropy:           0.9374
                Pseudo loss:              5.33823
                Total gradient norm:      0.50553
                Solved trajectories:      28 / 32
                Avg steps to solve:       4.679
                
Iteration (2400/50001) took 0.097 seconds.
                Mean final reward:        25.2812
                Mean return:              21.2188
                Policy entropy:           0.9050
                Pseudo loss:              6.36170
                Total gradient norm:      0.50013
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.600
                
Iteration (2500/50001) took 0.055 seconds.
                Mean final reward:        22.7812
                Mean return:              19.0625
                Policy entropy:           0.8983
                Pseudo loss:              4.05126
                Total gradient norm:      0.42457
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.484
                
Iteration (2600/50001) took 0.109 seconds.
                Mean final reward:        24.0312
                Mean return:              20.3750
                Policy entropy:           0.8502
                Pseudo loss:              5.40804
                Total gradient norm:      0.41803
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.419
                
Iteration (2700/50001) took 0.099 seconds.
                Mean final reward:        23.4062
                Mean return:              18.5000
                Policy entropy:           0.8674
                Pseudo loss:              7.56498
                Total gradient norm:      0.58690
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.500
                
Iteration (2800/50001) took 0.079 seconds.
                Mean final reward:        21.1875
                Mean return:              17.2812
                Policy entropy:           0.8883
                Pseudo loss:              4.12562
                Total gradient norm:      0.46205
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.433
                
Iteration (2900/50001) took 0.094 seconds.
                Mean final reward:        23.4062
                Mean return:              19.9062
                Policy entropy:           0.7979
                Pseudo loss:              4.62480
                Total gradient norm:      0.38866
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.258
                
Iteration (3000/50001) took 0.128 seconds.
                Mean final reward:        23.4062
                Mean return:              19.5312
                Policy entropy:           0.8056
                Pseudo loss:              4.23626
                Total gradient norm:      0.47454
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.645
                
Iteration (3100/50001) took 0.116 seconds.
                Mean final reward:        27.1562
                Mean return:              23.5938
                Policy entropy:           0.7774
                Pseudo loss:              5.11119
                Total gradient norm:      0.47366
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.323
                
Iteration (3200/50001) took 0.051 seconds.
                Mean final reward:        25.6250
                Mean return:              21.9688
                Policy entropy:           0.7404
                Pseudo loss:              4.91968
                Total gradient norm:      0.38174
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.656
                
Iteration (3300/50001) took 0.069 seconds.
                Mean final reward:        24.3750
                Mean return:              21.3125
                Policy entropy:           0.7231
                Pseudo loss:              3.99500
                Total gradient norm:      0.49064
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.062
                
Iteration (3400/50001) took 0.080 seconds.
                Mean final reward:        25.6250
                Mean return:              21.8750
                Policy entropy:           0.7350
                Pseudo loss:              5.79604
                Total gradient norm:      0.53499
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.750
                
Iteration (3500/50001) took 0.098 seconds.
                Mean final reward:        21.5312
                Mean return:              17.4062
                Policy entropy:           0.7549
                Pseudo loss:              4.61451
                Total gradient norm:      0.38017
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.903
                
Iteration (3600/50001) took 0.179 seconds.
                Mean final reward:        26.8750
                Mean return:              22.4062
                Policy entropy:           0.7576
                Pseudo loss:              7.93443
                Total gradient norm:      0.69372
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.469
                
Iteration (3700/50001) took 0.098 seconds.
                Mean final reward:        22.7812
                Mean return:              18.2500
                Policy entropy:           0.7308
                Pseudo loss:              5.96103
                Total gradient norm:      0.46451
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.323
                
Iteration (3800/50001) took 0.125 seconds.
                Mean final reward:        23.4062
                Mean return:              19.8750
                Policy entropy:           0.6888
                Pseudo loss:              2.95127
                Total gradient norm:      0.37787
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.290
                
Iteration (3900/50001) took 0.089 seconds.
                Mean final reward:        25.0000
                Mean return:              20.2500
                Policy entropy:           0.7151
                Pseudo loss:              6.85447
                Total gradient norm:      0.64573
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.548
                
Iteration (4000/50001) took 0.129 seconds.
                Mean final reward:        22.5000
                Mean return:              19.0625
                Policy entropy:           0.6501
                Pseudo loss:              4.20865
                Total gradient norm:      0.38323
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.438
                
Iteration (4100/50001) took 0.113 seconds.
                Mean final reward:        21.8750
                Mean return:              18.0938
                Policy entropy:           0.6244
                Pseudo loss:              4.30712
                Total gradient norm:      0.28126
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.781
                
Iteration (4200/50001) took 0.064 seconds.
                Mean final reward:        26.2500
                Mean return:              21.7188
                Policy entropy:           0.6466
                Pseudo loss:              7.03886
                Total gradient norm:      0.49176
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.100
                
Iteration (4300/50001) took 0.076 seconds.
                Mean final reward:        24.3750
                Mean return:              20.0000
                Policy entropy:           0.6653
                Pseudo loss:              6.17251
                Total gradient norm:      0.42130
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.375
                
Iteration (4400/50001) took 0.069 seconds.
                Mean final reward:        23.7500
                Mean return:              20.4688
                Policy entropy:           0.5765
                Pseudo loss:              3.89298
                Total gradient norm:      0.40619
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.281
                
Iteration (4500/50001) took 0.095 seconds.
                Mean final reward:        23.7500
                Mean return:              21.2812
                Policy entropy:           0.5095
                Pseudo loss:              1.31628
                Total gradient norm:      0.28545
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.469
                
Iteration (4600/50001) took 0.125 seconds.
                Mean final reward:        26.2500
                Mean return:              22.9688
                Policy entropy:           0.5909
                Pseudo loss:              4.07555
                Total gradient norm:      0.38028
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.281
                
Iteration (4700/50001) took 0.091 seconds.
                Mean final reward:        26.8750
                Mean return:              23.4688
                Policy entropy:           0.5601
                Pseudo loss:              3.68510
                Total gradient norm:      0.40106
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.406
                
Iteration (4800/50001) took 0.133 seconds.
                Mean final reward:        21.2500
                Mean return:              18.3750
                Policy entropy:           0.5291
                Pseudo loss:              2.67144
                Total gradient norm:      0.24609
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.875
                
Iteration (4900/50001) took 0.076 seconds.
                Mean final reward:        23.1250
                Mean return:              19.5312
                Policy entropy:           0.5933
                Pseudo loss:              4.82571
                Total gradient norm:      0.31945
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.594
                
Iteration (5000/50001) took 0.082 seconds.
                Mean final reward:        25.0000
                Mean return:              21.3438
                Policy entropy:           0.5084
                Pseudo loss:              3.80811
                Total gradient norm:      0.29656
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.656
                
Iteration (5100/50001) took 0.045 seconds.
                Mean final reward:        19.3750
                Mean return:              16.9062
                Policy entropy:           0.4672
                Pseudo loss:              2.44706
                Total gradient norm:      0.22252
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.469
                
Iteration (5200/50001) took 0.103 seconds.
                Mean final reward:        25.0000
                Mean return:              21.0938
                Policy entropy:           0.5319
                Pseudo loss:              4.41738
                Total gradient norm:      0.33176
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.906
                
Iteration (5300/50001) took 0.066 seconds.
                Mean final reward:        24.3750
                Mean return:              21.3438
                Policy entropy:           0.5100
                Pseudo loss:              3.62649
                Total gradient norm:      0.29574
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.031
                
Iteration (5400/50001) took 0.076 seconds.
                Mean final reward:        27.5000
                Mean return:              23.7812
                Policy entropy:           0.5211
                Pseudo loss:              4.41792
                Total gradient norm:      0.40540
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.719
                
Iteration (5500/50001) took 0.084 seconds.
                Mean final reward:        24.3750
                Mean return:              20.5938
                Policy entropy:           0.5288
                Pseudo loss:              4.78285
                Total gradient norm:      0.39206
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.781
                
Iteration (5600/50001) took 0.096 seconds.
                Mean final reward:        23.1250
                Mean return:              19.6875
                Policy entropy:           0.5220
                Pseudo loss:              2.94503
                Total gradient norm:      0.33714
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.438
                
Iteration (5700/50001) took 0.117 seconds.
                Mean final reward:        27.1562
                Mean return:              23.0938
                Policy entropy:           0.5240
                Pseudo loss:              5.96722
                Total gradient norm:      0.44332
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.839
                
Iteration (5800/50001) took 0.071 seconds.
                Mean final reward:        23.1250
                Mean return:              20.3438
                Policy entropy:           0.4677
                Pseudo loss:              2.60535
                Total gradient norm:      0.23237
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.781
                
Iteration (5900/50001) took 0.074 seconds.
                Mean final reward:        21.2500
                Mean return:              18.9688
                Policy entropy:           0.3573
                Pseudo loss:              0.88515
                Total gradient norm:      0.20684
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.281
                
Iteration (6000/50001) took 0.087 seconds.
                Mean final reward:        26.8750
                Mean return:              24.1562
                Policy entropy:           0.4854
                Pseudo loss:              4.29698
                Total gradient norm:      0.32647
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.719
                
Iteration (6100/50001) took 0.086 seconds.
                Mean final reward:        24.3750
                Mean return:              20.9062
                Policy entropy:           0.4558
                Pseudo loss:              4.17479
                Total gradient norm:      0.35008
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.469
                
Iteration (6200/50001) took 0.133 seconds.
                Mean final reward:        24.6562
                Mean return:              21.1562
                Policy entropy:           0.4724
                Pseudo loss:              3.10385
                Total gradient norm:      0.31023
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.258
                
Iteration (6300/50001) took 0.105 seconds.
                Mean final reward:        22.5000
                Mean return:              19.3750
                Policy entropy:           0.4075
                Pseudo loss:              1.79577
                Total gradient norm:      0.22522
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.125
                
Iteration (6400/50001) took 0.116 seconds.
                Mean final reward:        22.5000
                Mean return:              19.1250
                Policy entropy:           0.4388
                Pseudo loss:              3.08849
                Total gradient norm:      0.23454
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.375
                
Iteration (6500/50001) took 0.098 seconds.
                Mean final reward:        25.0000
                Mean return:              21.7812
                Policy entropy:           0.4396
                Pseudo loss:              4.01014
                Total gradient norm:      0.31844
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.219
                
Iteration (6600/50001) took 0.051 seconds.
                Mean final reward:        23.7500
                Mean return:              20.7812
                Policy entropy:           0.4175
                Pseudo loss:              3.00045
                Total gradient norm:      0.29952
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.969
                
Iteration (6700/50001) took 0.121 seconds.
                Mean final reward:        25.6250
                Mean return:              22.6250
                Policy entropy:           0.3798
                Pseudo loss:              2.68720
                Total gradient norm:      0.29909
                Solved trajectories:      31 / 32
                Avg steps to solve:       3.742
                
Iteration (6800/50001) took 0.111 seconds.
                Mean final reward:        23.1250
                Mean return:              19.5000
                Policy entropy:           0.4453
                Pseudo loss:              3.79178
                Total gradient norm:      0.30258
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.625
                
Iteration (6900/50001) took 0.110 seconds.
                Mean final reward:        24.3750
                Mean return:              20.9688
                Policy entropy:           0.4250
                Pseudo loss:              4.34098
                Total gradient norm:      0.38189
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.406
                
Iteration (7000/50001) took 0.064 seconds.
                Mean final reward:        25.0000
                Mean return:              21.9688
                Policy entropy:           0.4344
                Pseudo loss:              3.80331
                Total gradient norm:      0.40536
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.031
                
Iteration (7100/50001) took 0.158 seconds.
                Mean final reward:        24.3750
                Mean return:              21.1562
                Policy entropy:           0.4126
                Pseudo loss:              4.35447
                Total gradient norm:      0.40811
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.219
                
Iteration (7200/50001) took 0.091 seconds.
                Mean final reward:        25.6250
                Mean return:              22.5000
                Policy entropy:           0.3726
                Pseudo loss:              3.24145
                Total gradient norm:      0.26713
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.125
                
Iteration (7300/50001) took 0.112 seconds.
                Mean final reward:        26.2500
                Mean return:              23.0625
                Policy entropy:           0.3659
                Pseudo loss:              2.38679
                Total gradient norm:      0.27769
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.188
                
Iteration (7400/50001) took 0.138 seconds.
                Mean final reward:        23.7500
                Mean return:              20.9375
                Policy entropy:           0.3446
                Pseudo loss:              2.14584
                Total gradient norm:      0.24314
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.812
                
Iteration (7500/50001) took 0.100 seconds.
                Mean final reward:        21.2500
                Mean return:              18.5938
                Policy entropy:           0.3776
                Pseudo loss:              2.01947
                Total gradient norm:      0.27708
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.656
                
Iteration (7600/50001) took 0.102 seconds.
                Mean final reward:        25.0000
                Mean return:              21.9375
                Policy entropy:           0.4080
                Pseudo loss:              2.88973
                Total gradient norm:      0.30444
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.062
                
Iteration (7700/50001) took 0.142 seconds.
                Mean final reward:        26.2500
                Mean return:              22.9062
                Policy entropy:           0.3522
                Pseudo loss:              2.48311
                Total gradient norm:      0.24667
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.344
                
Iteration (7800/50001) took 0.141 seconds.
                Mean final reward:        26.2500
                Mean return:              22.7188
                Policy entropy:           0.4097
                Pseudo loss:              3.65175
                Total gradient norm:      0.24183
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.531
                
Iteration (7900/50001) took 0.114 seconds.
                Mean final reward:        25.6250
                Mean return:              22.0312
                Policy entropy:           0.3492
                Pseudo loss:              3.39602
                Total gradient norm:      0.39333
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.594
                
Iteration (8000/50001) took 0.067 seconds.
                Mean final reward:        24.3750
                Mean return:              21.6562
                Policy entropy:           0.3483
                Pseudo loss:              2.65348
                Total gradient norm:      0.29685
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.719
                
Iteration (8100/50001) took 0.073 seconds.
                Mean final reward:        24.3750
                Mean return:              21.2500
                Policy entropy:           0.3605
                Pseudo loss:              2.92487
                Total gradient norm:      0.24107
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.125
                
Iteration (8200/50001) took 0.107 seconds.
                Mean final reward:        24.3750
                Mean return:              21.5938
                Policy entropy:           0.3531
                Pseudo loss:              2.55745
                Total gradient norm:      0.25692
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.781
                
Iteration (8300/50001) took 0.082 seconds.
                Mean final reward:        25.0000
                Mean return:              22.0312
                Policy entropy:           0.3849
                Pseudo loss:              3.57485
                Total gradient norm:      0.36113
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.969
                
Iteration (8400/50001) took 0.066 seconds.
                Mean final reward:        23.7500
                Mean return:              21.1875
                Policy entropy:           0.3406
                Pseudo loss:              2.12861
                Total gradient norm:      0.22462
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.562
                
Iteration (8500/50001) took 0.133 seconds.
                Mean final reward:        26.8750
                Mean return:              24.0000
                Policy entropy:           0.3569
                Pseudo loss:              2.70332
                Total gradient norm:      0.31333
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.875
                
Iteration (8600/50001) took 0.112 seconds.
                Mean final reward:        24.3750
                Mean return:              21.3125
                Policy entropy:           0.3304
                Pseudo loss:              3.00200
                Total gradient norm:      0.27439
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.062
                
Iteration (8700/50001) took 0.076 seconds.
                Mean final reward:        23.1250
                Mean return:              20.3125
                Policy entropy:           0.3642
                Pseudo loss:              2.16427
                Total gradient norm:      0.25620
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.812
                
Iteration (8800/50001) took 0.120 seconds.
                Mean final reward:        26.2500
                Mean return:              23.4375
                Policy entropy:           0.3712
                Pseudo loss:              2.92601
                Total gradient norm:      0.31300
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.812
                
Iteration (8900/50001) took 0.105 seconds.
                Mean final reward:        23.1250
                Mean return:              20.4375
                Policy entropy:           0.3569
                Pseudo loss:              2.23839
                Total gradient norm:      0.25469
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.688
                
Iteration (9000/50001) took 0.075 seconds.
                Mean final reward:        23.1250
                Mean return:              19.8750
                Policy entropy:           0.3882
                Pseudo loss:              2.97334
                Total gradient norm:      0.30110
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.250
                
Iteration (9100/50001) took 0.121 seconds.
                Mean final reward:        27.5000
                Mean return:              24.1562
                Policy entropy:           0.3998
                Pseudo loss:              4.02509
                Total gradient norm:      0.32272
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.344
                
Iteration (9200/50001) took 0.133 seconds.
                Mean final reward:        26.8750
                Mean return:              23.3125
                Policy entropy:           0.3632
                Pseudo loss:              3.26119
                Total gradient norm:      0.28962
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.562
                
Iteration (9300/50001) took 0.103 seconds.
                Mean final reward:        25.6250
                Mean return:              22.5938
                Policy entropy:           0.3531
                Pseudo loss:              2.18226
                Total gradient norm:      0.26632
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.031
                
Iteration (9400/50001) took 0.139 seconds.
                Mean final reward:        23.7500
                Mean return:              19.7188
                Policy entropy:           0.3154
                Pseudo loss:              4.55177
                Total gradient norm:      0.39985
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.031
                
Iteration (9500/50001) took 0.118 seconds.
                Mean final reward:        24.3750
                Mean return:              21.1875
                Policy entropy:           0.3786
                Pseudo loss:              2.74275
                Total gradient norm:      0.31706
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.188
                
Iteration (9600/50001) took 0.138 seconds.
                Mean final reward:        28.1250
                Mean return:              24.4062
                Policy entropy:           0.3717
                Pseudo loss:              4.58242
                Total gradient norm:      0.38182
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.719
                
Iteration (9700/50001) took 0.076 seconds.
                Mean final reward:        25.0000
                Mean return:              22.1562
                Policy entropy:           0.2899
                Pseudo loss:              1.56479
                Total gradient norm:      0.25133
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.844
                
Iteration (9800/50001) took 0.093 seconds.
                Mean final reward:        27.5000
                Mean return:              23.7812
                Policy entropy:           0.3102
                Pseudo loss:              2.96899
                Total gradient norm:      0.27761
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.719
                
Iteration (9900/50001) took 0.110 seconds.
                Mean final reward:        22.5000
                Mean return:              19.9688
                Policy entropy:           0.2913
                Pseudo loss:              1.85180
                Total gradient norm:      0.19876
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.531
                
Iteration (10000/50001) took 0.096 seconds.
                Mean final reward:        25.6250
                Mean return:              22.3438
                Policy entropy:           0.3783
                Pseudo loss:              3.94408
                Total gradient norm:      0.31119
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.281
                
Iteration (10100/50001) took 0.079 seconds.
                Mean final reward:        26.8750
                Mean return:              23.6250
                Policy entropy:           0.2961
                Pseudo loss:              2.37582
                Total gradient norm:      0.25281
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.250
                
Iteration (10200/50001) took 0.089 seconds.
                Mean final reward:        26.8750
                Mean return:              23.6875
                Policy entropy:           0.3230
                Pseudo loss:              2.59083
                Total gradient norm:      0.28478
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.188
                
Iteration (10300/50001) took 0.071 seconds.
                Mean final reward:        27.5000
                Mean return:              24.3750
                Policy entropy:           0.3086
                Pseudo loss:              2.88943
                Total gradient norm:      0.32079
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.125
                
Iteration (10400/50001) took 0.147 seconds.
                Mean final reward:        26.8750
                Mean return:              23.3750
                Policy entropy:           0.3386
                Pseudo loss:              3.21871
                Total gradient norm:      0.39634
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.500
                
Iteration (10500/50001) took 0.115 seconds.
                Mean final reward:        25.6250
                Mean return:              22.6875
                Policy entropy:           0.2846
                Pseudo loss:              1.61471
                Total gradient norm:      0.17501
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.938
                
Iteration (10600/50001) took 0.070 seconds.
                Mean final reward:        20.0000
                Mean return:              17.1250
                Policy entropy:           0.2686
                Pseudo loss:              1.32578
                Total gradient norm:      0.18606
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.875
                
Iteration (10700/50001) took 0.068 seconds.
                Mean final reward:        27.5000
                Mean return:              24.6562
                Policy entropy:           0.3157
                Pseudo loss:              2.85874
                Total gradient norm:      0.23619
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.844
                
Iteration (10800/50001) took 0.109 seconds.
                Mean final reward:        26.2500
                Mean return:              23.0312
                Policy entropy:           0.3398
                Pseudo loss:              2.07067
                Total gradient norm:      0.26259
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.219
                
Iteration (10900/50001) took 0.111 seconds.
                Mean final reward:        27.5000
                Mean return:              24.6562
                Policy entropy:           0.3054
                Pseudo loss:              1.61334
                Total gradient norm:      0.26068
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.844
                
Iteration (11000/50001) took 0.110 seconds.
                Mean final reward:        26.2500
                Mean return:              23.3438
                Policy entropy:           0.2587
                Pseudo loss:              1.13996
                Total gradient norm:      0.17929
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.906
                
Iteration (11100/50001) took 0.069 seconds.
                Mean final reward:        26.2500
                Mean return:              22.6562
                Policy entropy:           0.3076
                Pseudo loss:              2.83098
                Total gradient norm:      0.27107
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.594
                
Iteration (11200/50001) took 0.058 seconds.
                Mean final reward:        23.1250
                Mean return:              19.7500
                Policy entropy:           0.2747
                Pseudo loss:              1.98351
                Total gradient norm:      0.23554
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.375
                
Iteration (11300/50001) took 0.077 seconds.
                Mean final reward:        25.6250
                Mean return:              23.9062
                Policy entropy:           0.2478
                Pseudo loss:              0.87896
                Total gradient norm:      0.18089
                Solved trajectories:      32 / 32
                Avg steps to solve:       2.719
                
Iteration (11400/50001) took 0.172 seconds.
                Mean final reward:        24.3750
                Mean return:              21.4062
                Policy entropy:           0.3125
                Pseudo loss:              2.61927
                Total gradient norm:      0.32830
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.969
                
Iteration (11500/50001) took 0.107 seconds.
                Mean final reward:        26.8750
                Mean return:              23.0312
                Policy entropy:           0.2593
                Pseudo loss:              2.81987
                Total gradient norm:      0.30583
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.844
                
Iteration (11600/50001) took 0.150 seconds.
                Mean final reward:        27.5000
                Mean return:              23.1562
                Policy entropy:           0.3355
                Pseudo loss:              4.57033
                Total gradient norm:      0.38984
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.344
                
Iteration (11700/50001) took 0.086 seconds.
                Mean final reward:        25.6250
                Mean return:              22.0000
                Policy entropy:           0.3070
                Pseudo loss:              2.47881
                Total gradient norm:      0.30740
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.625
                
Iteration (11800/50001) took 0.065 seconds.
                Mean final reward:        25.0000
                Mean return:              21.8750
                Policy entropy:           0.2406
                Pseudo loss:              1.52671
                Total gradient norm:      0.21010
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.125
                
Iteration (11900/50001) took 0.086 seconds.
                Mean final reward:        26.2500
                Mean return:              22.6562
                Policy entropy:           0.2692
                Pseudo loss:              2.96462
                Total gradient norm:      0.28055
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.594
                
Iteration (12000/50001) took 0.062 seconds.
                Mean final reward:        24.3750
                Mean return:              21.4062
                Policy entropy:           0.2908
                Pseudo loss:              2.23414
                Total gradient norm:      0.29802
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.969
                
Iteration (12100/50001) took 0.079 seconds.
                Mean final reward:        23.1250
                Mean return:              19.8125
                Policy entropy:           0.2989
                Pseudo loss:              2.39308
                Total gradient norm:      0.22896
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.312
                
Iteration (12200/50001) took 0.083 seconds.
                Mean final reward:        25.6250
                Mean return:              22.7812
                Policy entropy:           0.2607
                Pseudo loss:              1.45883
                Total gradient norm:      0.22036
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.844
                
Iteration (12300/50001) took 0.102 seconds.
                Mean final reward:        25.6250
                Mean return:              22.4688
                Policy entropy:           0.2814
                Pseudo loss:              2.68353
                Total gradient norm:      0.26279
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.156
                
Iteration (12400/50001) took 0.117 seconds.
                Mean final reward:        21.8750
                Mean return:              19.0000
                Policy entropy:           0.2245
                Pseudo loss:              1.16334
                Total gradient norm:      0.21095
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.875
                
Iteration (12500/50001) took 0.160 seconds.
                Mean final reward:        25.6250
                Mean return:              22.8750
                Policy entropy:           0.2856
                Pseudo loss:              2.60760
                Total gradient norm:      0.31511
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.750
                
Iteration (12600/50001) took 0.096 seconds.
                Mean final reward:        25.0000
                Mean return:              21.7500
                Policy entropy:           0.2677
                Pseudo loss:              1.83556
                Total gradient norm:      0.25331
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.250
                
Iteration (12700/50001) took 0.068 seconds.
                Mean final reward:        28.1250
                Mean return:              24.9688
                Policy entropy:           0.2670
                Pseudo loss:              1.41611
                Total gradient norm:      0.18224
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.156
                
Iteration (12800/50001) took 0.118 seconds.
                Mean final reward:        23.7500
                Mean return:              20.6562
                Policy entropy:           0.2476
                Pseudo loss:              2.04662
                Total gradient norm:      0.20963
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.094
                
Iteration (12900/50001) took 0.069 seconds.
                Mean final reward:        23.1250
                Mean return:              20.2812
                Policy entropy:           0.2632
                Pseudo loss:              2.27273
                Total gradient norm:      0.29759
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.844
                
Iteration (13000/50001) took 0.132 seconds.
                Mean final reward:        28.1250
                Mean return:              24.7188
                Policy entropy:           0.2688
                Pseudo loss:              2.05877
                Total gradient norm:      0.22745
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.406
                
Iteration (13100/50001) took 0.112 seconds.
                Mean final reward:        24.3750
                Mean return:              21.2500
                Policy entropy:           0.2498
                Pseudo loss:              2.79927
                Total gradient norm:      0.27301
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.125
                
Iteration (13200/50001) took 0.112 seconds.
                Mean final reward:        22.5000
                Mean return:              19.5000
                Policy entropy:           0.2060
                Pseudo loss:              1.31051
                Total gradient norm:      0.19655
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.000
                
Iteration (13300/50001) took 0.092 seconds.
                Mean final reward:        26.2500
                Mean return:              23.0000
                Policy entropy:           0.2784
                Pseudo loss:              2.08997
                Total gradient norm:      0.16118
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.250
                
Iteration (13400/50001) took 0.140 seconds.
                Mean final reward:        24.3750
                Mean return:              20.9375
                Policy entropy:           0.2345
                Pseudo loss:              2.00585
                Total gradient norm:      0.25867
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.438
                
Iteration (13500/50001) took 0.074 seconds.
                Mean final reward:        25.0000
                Mean return:              21.5312
                Policy entropy:           0.2369
                Pseudo loss:              1.45867
                Total gradient norm:      0.14988
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.469
                
Iteration (13600/50001) took 0.076 seconds.
                Mean final reward:        25.9062
                Mean return:              23.0625
                Policy entropy:           0.2898
                Pseudo loss:              2.19350
                Total gradient norm:      0.41551
                Solved trajectories:      31 / 32
                Avg steps to solve:       3.581
                
Iteration (13700/50001) took 0.076 seconds.
                Mean final reward:        23.1250
                Mean return:              20.1875
                Policy entropy:           0.2896
                Pseudo loss:              2.80820
                Total gradient norm:      0.34699
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.938
                
Iteration (13800/50001) took 0.167 seconds.
                Mean final reward:        26.8750
                Mean return:              23.5625
                Policy entropy:           0.2694
                Pseudo loss:              1.88563
                Total gradient norm:      0.18829
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.312
                
Iteration (13900/50001) took 0.102 seconds.
                Mean final reward:        23.7500
                Mean return:              20.6562
                Policy entropy:           0.2722
                Pseudo loss:              3.91723
                Total gradient norm:      0.38909
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.094
                
Iteration (14000/50001) took 0.132 seconds.
                Mean final reward:        25.6250
                Mean return:              22.2188
                Policy entropy:           0.2168
                Pseudo loss:              1.79711
                Total gradient norm:      0.30787
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.406
                
Iteration (14100/50001) took 0.113 seconds.
                Mean final reward:        26.2500
                Mean return:              22.3438
                Policy entropy:           0.2491
                Pseudo loss:              3.09900
                Total gradient norm:      0.28844
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.906
                
Iteration (14200/50001) took 0.069 seconds.
                Mean final reward:        27.5000
                Mean return:              25.1875
                Policy entropy:           0.2800
                Pseudo loss:              2.08211
                Total gradient norm:      0.32834
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.312
                
Iteration (14300/50001) took 0.045 seconds.
                Mean final reward:        26.2500
                Mean return:              22.9375
                Policy entropy:           0.2208
                Pseudo loss:              0.97300
                Total gradient norm:      0.15270
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.312
                
Iteration (14400/50001) took 0.071 seconds.
                Mean final reward:        24.3750
                Mean return:              21.5625
                Policy entropy:           0.3153
                Pseudo loss:              3.61612
                Total gradient norm:      0.30624
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.812
                
Iteration (14500/50001) took 0.130 seconds.
                Mean final reward:        25.0000
                Mean return:              22.3438
                Policy entropy:           0.1825
                Pseudo loss:              0.85523
                Total gradient norm:      0.12646
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.656
                
Iteration (14600/50001) took 0.141 seconds.
                Mean final reward:        25.0000
                Mean return:              21.3125
                Policy entropy:           0.3509
                Pseudo loss:              4.60306
                Total gradient norm:      0.41652
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.452
                
Iteration (14700/50001) took 0.131 seconds.
                Mean final reward:        26.2500
                Mean return:              23.5312
                Policy entropy:           0.2539
                Pseudo loss:              1.69338
                Total gradient norm:      0.19292
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.719
                
Iteration (14800/50001) took 0.133 seconds.
                Mean final reward:        25.6250
                Mean return:              22.5312
                Policy entropy:           0.2510
                Pseudo loss:              3.16530
                Total gradient norm:      0.26970
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.094
                
Iteration (14900/50001) took 0.118 seconds.
                Mean final reward:        26.8750
                Mean return:              23.3438
                Policy entropy:           0.2412
                Pseudo loss:              2.53432
                Total gradient norm:      0.22428
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.290
                
Iteration (15000/50001) took 0.063 seconds.
                Mean final reward:        28.1250
                Mean return:              25.0312
                Policy entropy:           0.2548
                Pseudo loss:              1.59561
                Total gradient norm:      0.22229
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.094
                
Iteration (15100/50001) took 0.111 seconds.
                Mean final reward:        24.3750
                Mean return:              21.5312
                Policy entropy:           0.2352
                Pseudo loss:              1.67860
                Total gradient norm:      0.21028
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.844
                
Iteration (15200/50001) took 0.148 seconds.
                Mean final reward:        26.8750
                Mean return:              23.5938
                Policy entropy:           0.2079
                Pseudo loss:              2.04129
                Total gradient norm:      0.21244
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.281
                
Iteration (15300/50001) took 0.111 seconds.
                Mean final reward:        25.0000
                Mean return:              22.2188
                Policy entropy:           0.1808
                Pseudo loss:              1.48688
                Total gradient norm:      0.21347
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.781
                
Iteration (15400/50001) took 0.061 seconds.
                Mean final reward:        25.0000
                Mean return:              21.9375
                Policy entropy:           0.2460
                Pseudo loss:              1.21452
                Total gradient norm:      0.16945
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.062
                
Iteration (15500/50001) took 0.149 seconds.
                Mean final reward:        28.7500
                Mean return:              25.1562
                Policy entropy:           0.2395
                Pseudo loss:              2.84205
                Total gradient norm:      0.26510
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.594
                
Iteration (15600/50001) took 0.051 seconds.
                Mean final reward:        26.2500
                Mean return:              23.0000
                Policy entropy:           0.2627
                Pseudo loss:              3.59696
                Total gradient norm:      0.31096
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.250
                
Iteration (15700/50001) took 0.136 seconds.
                Mean final reward:        23.7500
                Mean return:              20.6875
                Policy entropy:           0.2335
                Pseudo loss:              1.54787
                Total gradient norm:      0.17807
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.062
                
Iteration (15800/50001) took 0.112 seconds.
                Mean final reward:        25.0000
                Mean return:              22.4062
                Policy entropy:           0.2248
                Pseudo loss:              0.96339
                Total gradient norm:      0.27160
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.594
                
Iteration (15900/50001) took 0.096 seconds.
                Mean final reward:        23.7500
                Mean return:              20.6250
                Policy entropy:           0.1799
                Pseudo loss:              1.23825
                Total gradient norm:      0.19581
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.125
                
Iteration (16000/50001) took 0.119 seconds.
                Mean final reward:        28.1250
                Mean return:              24.5000
                Policy entropy:           0.2635
                Pseudo loss:              3.12242
                Total gradient norm:      0.29582
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.625
                
Iteration (16100/50001) took 0.070 seconds.
                Mean final reward:        26.8750
                Mean return:              23.3750
                Policy entropy:           0.2319
                Pseudo loss:              1.61241
                Total gradient norm:      0.26085
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.500
                
Iteration (16200/50001) took 0.066 seconds.
                Mean final reward:        23.1250
                Mean return:              20.5312
                Policy entropy:           0.1974
                Pseudo loss:              1.30732
                Total gradient norm:      0.18379
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.594
                
Iteration (16300/50001) took 0.097 seconds.
                Mean final reward:        26.8750
                Mean return:              23.5312
                Policy entropy:           0.2613
                Pseudo loss:              2.25426
                Total gradient norm:      0.23335
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.344
                
Iteration (16400/50001) took 0.095 seconds.
                Mean final reward:        26.2500
                Mean return:              23.3438
                Policy entropy:           0.2756
                Pseudo loss:              1.95620
                Total gradient norm:      0.26776
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.906
                
Iteration (16500/50001) took 0.072 seconds.
                Mean final reward:        27.5000
                Mean return:              24.2188
                Policy entropy:           0.2394
                Pseudo loss:              1.59711
                Total gradient norm:      0.21992
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.281
                
Iteration (16600/50001) took 0.111 seconds.
                Mean final reward:        28.1250
                Mean return:              24.9375
                Policy entropy:           0.2446
                Pseudo loss:              2.39978
                Total gradient norm:      0.27618
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.188
                
Iteration (16700/50001) took 0.090 seconds.
                Mean final reward:        26.2500
                Mean return:              23.1562
                Policy entropy:           0.2433
                Pseudo loss:              2.15330
                Total gradient norm:      0.23429
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.094
                
Iteration (16800/50001) took 0.068 seconds.
                Mean final reward:        27.5000
                Mean return:              24.5000
                Policy entropy:           0.2252
                Pseudo loss:              2.02064
                Total gradient norm:      0.29928
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.000
                
Iteration (16900/50001) took 0.076 seconds.
                Mean final reward:        27.5000
                Mean return:              24.0312
                Policy entropy:           0.2067
                Pseudo loss:              2.56949
                Total gradient norm:      0.28853
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.469
                
Iteration (17000/50001) took 0.083 seconds.
                Mean final reward:        25.0000
                Mean return:              21.7812
                Policy entropy:           0.2328
                Pseudo loss:              1.99269
                Total gradient norm:      0.20420
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.219
                
Iteration (17100/50001) took 0.050 seconds.
                Mean final reward:        25.6250
                Mean return:              22.3750
                Policy entropy:           0.2072
                Pseudo loss:              1.56097
                Total gradient norm:      0.20885
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.250
                
Iteration (17200/50001) took 0.108 seconds.
                Mean final reward:        26.8750
                Mean return:              23.2188
                Policy entropy:           0.2409
                Pseudo loss:              2.67866
                Total gradient norm:      0.26853
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.656
                
Iteration (17300/50001) took 0.089 seconds.
                Mean final reward:        28.1250
                Mean return:              25.5000
                Policy entropy:           0.2761
                Pseudo loss:              2.50074
                Total gradient norm:      0.23889
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.625
                
Iteration (17400/50001) took 0.089 seconds.
                Mean final reward:        27.5000
                Mean return:              24.7812
                Policy entropy:           0.2101
                Pseudo loss:              1.56753
                Total gradient norm:      0.16358
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.719
                
Iteration (17500/50001) took 0.080 seconds.
                Mean final reward:        28.1250
                Mean return:              24.7500
                Policy entropy:           0.2405
                Pseudo loss:              1.76173
                Total gradient norm:      0.16901
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.375
                
Iteration (17600/50001) took 0.071 seconds.
                Mean final reward:        26.8750
                Mean return:              23.3750
                Policy entropy:           0.2078
                Pseudo loss:              1.85170
                Total gradient norm:      0.18933
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.500
                
Iteration (17700/50001) took 0.065 seconds.
                Mean final reward:        26.2500
                Mean return:              23.5312
                Policy entropy:           0.2188
                Pseudo loss:              1.24794
                Total gradient norm:      0.19172
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.719
                
Iteration (17800/50001) took 0.116 seconds.
                Mean final reward:        26.8750
                Mean return:              23.0312
                Policy entropy:           0.2184
                Pseudo loss:              1.98433
                Total gradient norm:      0.21999
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.844
                
Iteration (17900/50001) took 0.128 seconds.
                Mean final reward:        25.0000
                Mean return:              21.7812
                Policy entropy:           0.1673
                Pseudo loss:              1.45663
                Total gradient norm:      0.18971
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.219
                
Iteration (18000/50001) took 0.133 seconds.
                Mean final reward:        28.1250
                Mean return:              24.1562
                Policy entropy:           0.1878
                Pseudo loss:              1.25430
                Total gradient norm:      0.17679
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.969
                
Iteration (18100/50001) took 0.095 seconds.
                Mean final reward:        28.1250
                Mean return:              24.8125
                Policy entropy:           0.1956
                Pseudo loss:              3.72649
                Total gradient norm:      0.32137
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.312
                
Iteration (18200/50001) took 0.098 seconds.
                Mean final reward:        25.0000
                Mean return:              22.1250
                Policy entropy:           0.1904
                Pseudo loss:              2.08443
                Total gradient norm:      0.27553
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.875
                
Iteration (18300/50001) took 0.046 seconds.
                Mean final reward:        25.6250
                Mean return:              22.2500
                Policy entropy:           0.1845
                Pseudo loss:              2.16975
                Total gradient norm:      0.19317
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.375
                
Iteration (18400/50001) took 0.107 seconds.
                Mean final reward:        25.6250
                Mean return:              22.0000
                Policy entropy:           0.1892
                Pseudo loss:              1.71478
                Total gradient norm:      0.24787
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.625
                
Iteration (18500/50001) took 0.124 seconds.
                Mean final reward:        25.0000
                Mean return:              22.4062
                Policy entropy:           0.2344
                Pseudo loss:              2.32293
                Total gradient norm:      0.23276
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.594
                
Iteration (18600/50001) took 0.068 seconds.
                Mean final reward:        25.0000
                Mean return:              22.2188
                Policy entropy:           0.1965
                Pseudo loss:              1.15383
                Total gradient norm:      0.15754
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.781
                
Iteration (18700/50001) took 0.126 seconds.
                Mean final reward:        27.5000
                Mean return:              24.1250
                Policy entropy:           0.2386
                Pseudo loss:              2.22709
                Total gradient norm:      0.24911
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.375
                
Iteration (18800/50001) took 0.120 seconds.
                Mean final reward:        26.8750
                Mean return:              23.8125
                Policy entropy:           0.2145
                Pseudo loss:              1.61022
                Total gradient norm:      0.22370
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.062
                
Iteration (18900/50001) took 0.077 seconds.
                Mean final reward:        26.2500
                Mean return:              22.7812
                Policy entropy:           0.2151
                Pseudo loss:              2.87736
                Total gradient norm:      0.26643
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.469
                
Iteration (19000/50001) took 0.125 seconds.
                Mean final reward:        24.3750
                Mean return:              21.6562
                Policy entropy:           0.2313
                Pseudo loss:              0.69724
                Total gradient norm:      0.17482
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.719
                
Iteration (19100/50001) took 0.099 seconds.
                Mean final reward:        25.6250
                Mean return:              21.5625
                Policy entropy:           0.2145
                Pseudo loss:              3.71504
                Total gradient norm:      0.34393
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.062
                
Iteration (19200/50001) took 0.094 seconds.
                Mean final reward:        25.0000
                Mean return:              21.9062
                Policy entropy:           0.1792
                Pseudo loss:              1.96265
                Total gradient norm:      0.29234
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.094
                
Iteration (19300/50001) took 0.144 seconds.
                Mean final reward:        24.6562
                Mean return:              21.0312
                Policy entropy:           0.2118
                Pseudo loss:              1.16234
                Total gradient norm:      0.16607
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.387
                
Iteration (19400/50001) took 0.155 seconds.
                Mean final reward:        24.3750
                Mean return:              20.9062
                Policy entropy:           0.1649
                Pseudo loss:              1.56869
                Total gradient norm:      0.19359
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.469
                
Iteration (19500/50001) took 0.129 seconds.
                Mean final reward:        27.5000
                Mean return:              24.0625
                Policy entropy:           0.2330
                Pseudo loss:              2.70145
                Total gradient norm:      0.28788
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.438
                
Iteration (19600/50001) took 0.104 seconds.
                Mean final reward:        22.5000
                Mean return:              20.1562
                Policy entropy:           0.1354
                Pseudo loss:              0.54675
                Total gradient norm:      0.11083
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.344
                
Iteration (19700/50001) took 0.083 seconds.
                Mean final reward:        28.1250
                Mean return:              25.0000
                Policy entropy:           0.2131
                Pseudo loss:              1.51416
                Total gradient norm:      0.22865
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.125
                
Iteration (19800/50001) took 0.079 seconds.
                Mean final reward:        26.2500
                Mean return:              22.3125
                Policy entropy:           0.2192
                Pseudo loss:              1.97232
                Total gradient norm:      0.20895
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.710
                
Iteration (19900/50001) took 0.071 seconds.
                Mean final reward:        27.5000
                Mean return:              24.4375
                Policy entropy:           0.2606
                Pseudo loss:              2.55239
                Total gradient norm:      0.34494
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.062
                
Iteration (20000/50001) took 0.161 seconds.
                Mean final reward:        25.6250
                Mean return:              22.5938
                Policy entropy:           0.2081
                Pseudo loss:              1.88472
                Total gradient norm:      0.18818
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.031
                
Iteration (20100/50001) took 0.127 seconds.
                Mean final reward:        26.2500
                Mean return:              23.2500
                Policy entropy:           0.2273
                Pseudo loss:              1.84923
                Total gradient norm:      0.15396
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.000
                
Iteration (20200/50001) took 0.078 seconds.
                Mean final reward:        25.6250
                Mean return:              22.5625
                Policy entropy:           0.1842
                Pseudo loss:              2.51597
                Total gradient norm:      0.28779
                Solved trajectories:      31 / 32
                Avg steps to solve:       3.806
                
Iteration (20300/50001) took 0.076 seconds.
                Mean final reward:        26.8750
                Mean return:              23.7500
                Policy entropy:           0.2260
                Pseudo loss:              1.31531
                Total gradient norm:      0.17807
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.125
                
Iteration (20400/50001) took 0.144 seconds.
                Mean final reward:        26.2500
                Mean return:              23.0938
                Policy entropy:           0.2034
                Pseudo loss:              2.42946
                Total gradient norm:      0.30188
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.156
                
Iteration (20500/50001) took 0.133 seconds.
                Mean final reward:        26.2500
                Mean return:              23.1562
                Policy entropy:           0.2092
                Pseudo loss:              1.92527
                Total gradient norm:      0.18465
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.094
                
Iteration (20600/50001) took 0.118 seconds.
                Mean final reward:        26.2500
                Mean return:              23.5000
                Policy entropy:           0.2184
                Pseudo loss:              2.19434
                Total gradient norm:      0.21590
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.750
                
Iteration (20700/50001) took 0.094 seconds.
                Mean final reward:        26.8750
                Mean return:              23.1250
                Policy entropy:           0.2413
                Pseudo loss:              2.99516
                Total gradient norm:      0.26114
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.750
                
Iteration (20800/50001) took 0.073 seconds.
                Mean final reward:        23.7500
                Mean return:              20.1250
                Policy entropy:           0.2071
                Pseudo loss:              2.52884
                Total gradient norm:      0.33503
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.625
                
Iteration (20900/50001) took 0.079 seconds.
                Mean final reward:        25.0000
                Mean return:              21.3438
                Policy entropy:           0.1939
                Pseudo loss:              2.18134
                Total gradient norm:      0.25609
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.656
                
Iteration (21000/50001) took 0.092 seconds.
                Mean final reward:        27.5000
                Mean return:              23.6562
                Policy entropy:           0.1776
                Pseudo loss:              1.37040
                Total gradient norm:      0.15910
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.844
                
Iteration (21100/50001) took 0.076 seconds.
                Mean final reward:        22.5000
                Mean return:              19.5000
                Policy entropy:           0.2003
                Pseudo loss:              1.52579
                Total gradient norm:      0.17048
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.000
                
Iteration (21200/50001) took 0.131 seconds.
                Mean final reward:        23.1250
                Mean return:              19.9062
                Policy entropy:           0.1876
                Pseudo loss:              1.23466
                Total gradient norm:      0.17985
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.219
                
Iteration (21300/50001) took 0.133 seconds.
                Mean final reward:        26.2500
                Mean return:              23.0625
                Policy entropy:           0.2146
                Pseudo loss:              1.97422
                Total gradient norm:      0.23488
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.188
                
Iteration (21400/50001) took 0.058 seconds.
                Mean final reward:        26.2500
                Mean return:              22.9375
                Policy entropy:           0.2154
                Pseudo loss:              1.99796
                Total gradient norm:      0.17886
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.312
                
Iteration (21500/50001) took 0.060 seconds.
                Mean final reward:        21.2500
                Mean return:              18.9375
                Policy entropy:           0.2050
                Pseudo loss:              1.03534
                Total gradient norm:      0.16385
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.312
                
Iteration (21600/50001) took 0.087 seconds.
                Mean final reward:        26.8750
                Mean return:              24.2188
                Policy entropy:           0.2382
                Pseudo loss:              1.65694
                Total gradient norm:      0.25821
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.656
                
Iteration (21700/50001) took 0.081 seconds.
                Mean final reward:        24.3750
                Mean return:              21.7500
                Policy entropy:           0.1503
                Pseudo loss:              1.05432
                Total gradient norm:      0.22190
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.625
                
Iteration (21800/50001) took 0.113 seconds.
                Mean final reward:        28.1250
                Mean return:              24.8750
                Policy entropy:           0.1491
                Pseudo loss:              0.75444
                Total gradient norm:      0.15098
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.250
                
Iteration (21900/50001) took 0.101 seconds.
                Mean final reward:        23.1250
                Mean return:              19.7500
                Policy entropy:           0.1624
                Pseudo loss:              1.32265
                Total gradient norm:      0.20179
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.375
                
Iteration (22000/50001) took 0.080 seconds.
                Mean final reward:        25.6250
                Mean return:              22.1875
                Policy entropy:           0.1539
                Pseudo loss:              0.92577
                Total gradient norm:      0.15324
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.438
                
Iteration (22100/50001) took 0.082 seconds.
                Mean final reward:        27.5000
                Mean return:              23.8125
                Policy entropy:           0.1846
                Pseudo loss:              1.06525
                Total gradient norm:      0.22580
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.688
                
Iteration (22200/50001) took 0.068 seconds.
                Mean final reward:        23.7500
                Mean return:              21.2188
                Policy entropy:           0.1557
                Pseudo loss:              1.46304
                Total gradient norm:      0.17243
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.531
                
Iteration (22300/50001) took 0.076 seconds.
                Mean final reward:        26.8750
                Mean return:              22.8438
                Policy entropy:           0.1674
                Pseudo loss:              1.96558
                Total gradient norm:      0.27732
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.031
                
Iteration (22400/50001) took 0.075 seconds.
                Mean final reward:        27.5000
                Mean return:              24.5000
                Policy entropy:           0.1818
                Pseudo loss:              1.12073
                Total gradient norm:      0.17406
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.000
                
Iteration (22500/50001) took 0.114 seconds.
                Mean final reward:        24.3750
                Mean return:              21.1875
                Policy entropy:           0.1787
                Pseudo loss:              1.61155
                Total gradient norm:      0.17616
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.188
                
Iteration (22600/50001) took 0.105 seconds.
                Mean final reward:        24.3750
                Mean return:              21.8750
                Policy entropy:           0.1751
                Pseudo loss:              1.44650
                Total gradient norm:      0.19933
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.500
                
Iteration (22700/50001) took 0.125 seconds.
                Mean final reward:        27.5000
                Mean return:              24.0000
                Policy entropy:           0.2094
                Pseudo loss:              1.78874
                Total gradient norm:      0.21047
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.500
                
Iteration (22800/50001) took 0.114 seconds.
                Mean final reward:        26.8750
                Mean return:              22.8125
                Policy entropy:           0.1922
                Pseudo loss:              1.27597
                Total gradient norm:      0.20922
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.062
                
Iteration (22900/50001) took 0.081 seconds.
                Mean final reward:        25.6250
                Mean return:              22.1562
                Policy entropy:           0.1993
                Pseudo loss:              1.37068
                Total gradient norm:      0.20525
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.469
                
Iteration (23000/50001) took 0.118 seconds.
                Mean final reward:        23.1250
                Mean return:              19.8125
                Policy entropy:           0.2069
                Pseudo loss:              1.52287
                Total gradient norm:      0.21510
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.312
                
Iteration (23100/50001) took 0.097 seconds.
                Mean final reward:        25.0000
                Mean return:              21.8750
                Policy entropy:           0.2211
                Pseudo loss:              2.86221
                Total gradient norm:      0.33495
                Solved trajectories:      31 / 32
                Avg steps to solve:       3.871
                
Iteration (23200/50001) took 0.137 seconds.
                Mean final reward:        23.7500
                Mean return:              20.7500
                Policy entropy:           0.2147
                Pseudo loss:              1.62954
                Total gradient norm:      0.17574
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.000
                
Iteration (23300/50001) took 0.064 seconds.
                Mean final reward:        26.2500
                Mean return:              23.1875
                Policy entropy:           0.2090
                Pseudo loss:              1.85296
                Total gradient norm:      0.26532
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.062
                
Iteration (23400/50001) took 0.051 seconds.
                Mean final reward:        28.1250
                Mean return:              24.3750
                Policy entropy:           0.1711
                Pseudo loss:              0.76936
                Total gradient norm:      0.14102
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.750
                
Iteration (23500/50001) took 0.093 seconds.
                Mean final reward:        26.8750
                Mean return:              23.2812
                Policy entropy:           0.2092
                Pseudo loss:              1.66965
                Total gradient norm:      0.24743
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.594
                
Iteration (23600/50001) took 0.094 seconds.
                Mean final reward:        26.2500
                Mean return:              22.7500
                Policy entropy:           0.2298
                Pseudo loss:              2.37046
                Total gradient norm:      0.26663
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.500
                
Iteration (23700/50001) took 0.072 seconds.
                Mean final reward:        26.2500
                Mean return:              23.0938
                Policy entropy:           0.1513
                Pseudo loss:              1.17130
                Total gradient norm:      0.12861
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.156
                
Iteration (23800/50001) took 0.076 seconds.
                Mean final reward:        26.8750
                Mean return:              23.3125
                Policy entropy:           0.1804
                Pseudo loss:              2.22648
                Total gradient norm:      0.26883
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.562
                
Iteration (23900/50001) took 0.087 seconds.
                Mean final reward:        26.8750
                Mean return:              23.2188
                Policy entropy:           0.2171
                Pseudo loss:              2.25308
                Total gradient norm:      0.33175
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.656
                
Iteration (24000/50001) took 0.097 seconds.
                Mean final reward:        26.8750
                Mean return:              23.3750
                Policy entropy:           0.1693
                Pseudo loss:              1.15523
                Total gradient norm:      0.20096
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.258
                
Iteration (24100/50001) took 0.063 seconds.
                Mean final reward:        25.0000
                Mean return:              21.7812
                Policy entropy:           0.1460
                Pseudo loss:              1.45951
                Total gradient norm:      0.20387
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.219
                
Iteration (24200/50001) took 0.097 seconds.
                Mean final reward:        26.8750
                Mean return:              22.9375
                Policy entropy:           0.1529
                Pseudo loss:              1.56568
                Total gradient norm:      0.28109
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.938
                
Iteration (24300/50001) took 0.152 seconds.
                Mean final reward:        26.2500
                Mean return:              23.2812
                Policy entropy:           0.1948
                Pseudo loss:              0.69919
                Total gradient norm:      0.16835
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.969
                
Iteration (24400/50001) took 0.065 seconds.
                Mean final reward:        26.8750
                Mean return:              23.8125
                Policy entropy:           0.1730
                Pseudo loss:              0.59645
                Total gradient norm:      0.14356
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.062
                
Iteration (24500/50001) took 0.082 seconds.
                Mean final reward:        23.4062
                Mean return:              19.7812
                Policy entropy:           0.1546
                Pseudo loss:              1.33180
                Total gradient norm:      0.25040
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.387
                
Iteration (24600/50001) took 0.131 seconds.
                Mean final reward:        26.8750
                Mean return:              23.8438
                Policy entropy:           0.1484
                Pseudo loss:              0.62781
                Total gradient norm:      0.18159
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.031
                
Iteration (24700/50001) took 0.094 seconds.
                Mean final reward:        26.8750
                Mean return:              23.5000
                Policy entropy:           0.1503
                Pseudo loss:              2.29263
                Total gradient norm:      0.21815
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.129
                
Iteration (24800/50001) took 0.094 seconds.
                Mean final reward:        22.5000
                Mean return:              18.8750
                Policy entropy:           0.1547
                Pseudo loss:              1.07734
                Total gradient norm:      0.14685
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.625
                
Iteration (24900/50001) took 0.113 seconds.
                Mean final reward:        27.5000
                Mean return:              23.9062
                Policy entropy:           0.2031
                Pseudo loss:              1.37148
                Total gradient norm:      0.29444
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.355
                
Iteration (25000/50001) took 0.077 seconds.
                Mean final reward:        28.1250
                Mean return:              24.4688
                Policy entropy:           0.2072
                Pseudo loss:              1.44327
                Total gradient norm:      0.18450
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.656
                
Iteration (25100/50001) took 0.072 seconds.
                Mean final reward:        28.1250
                Mean return:              25.2500
                Policy entropy:           0.1864
                Pseudo loss:              1.57965
                Total gradient norm:      0.27893
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.875
                
Iteration (25200/50001) took 0.081 seconds.
                Mean final reward:        26.8750
                Mean return:              23.2812
                Policy entropy:           0.1677
                Pseudo loss:              1.47601
                Total gradient norm:      0.18414
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.594
                
Iteration (25300/50001) took 0.077 seconds.
                Mean final reward:        23.1250
                Mean return:              21.0312
                Policy entropy:           0.1224
                Pseudo loss:              0.23889
                Total gradient norm:      0.08127
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.094
                
Iteration (25400/50001) took 0.074 seconds.
                Mean final reward:        27.5000
                Mean return:              24.1562
                Policy entropy:           0.1308
                Pseudo loss:              1.11426
                Total gradient norm:      0.16065
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.344
                
Iteration (25500/50001) took 0.069 seconds.
                Mean final reward:        24.3750
                Mean return:              21.6875
                Policy entropy:           0.1625
                Pseudo loss:              0.69560
                Total gradient norm:      0.12986
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.688
                
Iteration (25600/50001) took 0.075 seconds.
                Mean final reward:        26.8750
                Mean return:              23.7188
                Policy entropy:           0.1877
                Pseudo loss:              1.19155
                Total gradient norm:      0.16390
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.156
                
Iteration (25700/50001) took 0.044 seconds.
                Mean final reward:        27.5000
                Mean return:              24.2500
                Policy entropy:           0.1520
                Pseudo loss:              0.78954
                Total gradient norm:      0.13275
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.250
                
Iteration (25800/50001) took 0.065 seconds.
                Mean final reward:        25.0000
                Mean return:              22.0938
                Policy entropy:           0.1551
                Pseudo loss:              1.14667
                Total gradient norm:      0.17971
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.906
                
Iteration (25900/50001) took 0.071 seconds.
                Mean final reward:        26.8750
                Mean return:              23.4375
                Policy entropy:           0.1970
                Pseudo loss:              2.00115
                Total gradient norm:      0.19408
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.438
                
Iteration (26000/50001) took 0.062 seconds.
                Mean final reward:        26.8750
                Mean return:              24.1875
                Policy entropy:           0.1345
                Pseudo loss:              0.63393
                Total gradient norm:      0.14945
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.688
                
Iteration (26100/50001) took 0.056 seconds.
                Mean final reward:        28.7500
                Mean return:              25.1562
                Policy entropy:           0.1714
                Pseudo loss:              1.51448
                Total gradient norm:      0.18252
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.594
                
Iteration (26200/50001) took 0.082 seconds.
                Mean final reward:        26.8750
                Mean return:              23.8125
                Policy entropy:           0.1617
                Pseudo loss:              1.71466
                Total gradient norm:      0.28100
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.062
                
Iteration (26300/50001) took 0.077 seconds.
                Mean final reward:        28.1250
                Mean return:              23.6875
                Policy entropy:           0.1627
                Pseudo loss:              2.43801
                Total gradient norm:      0.27702
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.226
                
Iteration (26400/50001) took 0.061 seconds.
                Mean final reward:        28.1250
                Mean return:              25.7812
                Policy entropy:           0.1487
                Pseudo loss:              0.90970
                Total gradient norm:      0.13589
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.344
                
Iteration (26500/50001) took 0.082 seconds.
                Mean final reward:        26.2500
                Mean return:              22.3438
                Policy entropy:           0.1266
                Pseudo loss:              1.34167
                Total gradient norm:      0.21722
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.906
                
Iteration (26600/50001) took 0.065 seconds.
                Mean final reward:        27.5000
                Mean return:              23.8125
                Policy entropy:           0.1298
                Pseudo loss:              1.57378
                Total gradient norm:      0.21190
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.688
                
Iteration (26700/50001) took 0.077 seconds.
                Mean final reward:        25.6250
                Mean return:              22.1875
                Policy entropy:           0.1539
                Pseudo loss:              1.69283
                Total gradient norm:      0.22089
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.438
                
Iteration (26800/50001) took 0.061 seconds.
                Mean final reward:        27.5000
                Mean return:              24.6562
                Policy entropy:           0.1675
                Pseudo loss:              1.96825
                Total gradient norm:      0.25811
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.844
                
Iteration (26900/50001) took 0.062 seconds.
                Mean final reward:        25.6250
                Mean return:              22.8438
                Policy entropy:           0.2066
                Pseudo loss:              1.28827
                Total gradient norm:      0.14697
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.781
                
Iteration (27000/50001) took 0.052 seconds.
                Mean final reward:        26.2500
                Mean return:              22.7500
                Policy entropy:           0.1507
                Pseudo loss:              2.57212
                Total gradient norm:      0.20531
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.500
                
Iteration (27100/50001) took 0.070 seconds.
                Mean final reward:        25.6250
                Mean return:              22.4375
                Policy entropy:           0.1408
                Pseudo loss:              0.44655
                Total gradient norm:      0.13531
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.188
                
Iteration (27200/50001) took 0.047 seconds.
                Mean final reward:        25.6250
                Mean return:              22.5938
                Policy entropy:           0.1115
                Pseudo loss:              0.59782
                Total gradient norm:      0.11694
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.031
                
Iteration (27300/50001) took 0.072 seconds.
                Mean final reward:        26.2500
                Mean return:              22.7188
                Policy entropy:           0.1671
                Pseudo loss:              1.48673
                Total gradient norm:      0.15860
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.531
                
Iteration (27400/50001) took 0.079 seconds.
                Mean final reward:        26.8750
                Mean return:              23.0938
                Policy entropy:           0.1996
                Pseudo loss:              2.34737
                Total gradient norm:      0.28978
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.781
                
Iteration (27500/50001) took 0.072 seconds.
                Mean final reward:        25.0000
                Mean return:              21.7500
                Policy entropy:           0.1511
                Pseudo loss:              1.27588
                Total gradient norm:      0.18452
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.250
                
Iteration (27600/50001) took 0.068 seconds.
                Mean final reward:        23.1250
                Mean return:              20.3438
                Policy entropy:           0.1416
                Pseudo loss:              1.47508
                Total gradient norm:      0.26960
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.781
                
Iteration (27700/50001) took 0.077 seconds.
                Mean final reward:        26.8750
                Mean return:              23.4688
                Policy entropy:           0.1582
                Pseudo loss:              1.66439
                Total gradient norm:      0.18610
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.406
                
Iteration (27800/50001) took 0.082 seconds.
                Mean final reward:        28.7500
                Mean return:              25.1250
                Policy entropy:           0.1591
                Pseudo loss:              1.00266
                Total gradient norm:      0.15161
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.625
                
Iteration (27900/50001) took 0.060 seconds.
                Mean final reward:        28.7500
                Mean return:              25.5000
                Policy entropy:           0.1820
                Pseudo loss:              1.83987
                Total gradient norm:      0.18980
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.250
                
Iteration (28000/50001) took 0.047 seconds.
                Mean final reward:        25.6250
                Mean return:              22.2812
                Policy entropy:           0.1637
                Pseudo loss:              1.82179
                Total gradient norm:      0.22228
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.344
                
Iteration (28100/50001) took 0.067 seconds.
                Mean final reward:        26.2500
                Mean return:              22.5000
                Policy entropy:           0.1257
                Pseudo loss:              1.97451
                Total gradient norm:      0.23019
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.516
                
Iteration (28200/50001) took 0.051 seconds.
                Mean final reward:        28.7500
                Mean return:              24.7812
                Policy entropy:           0.1503
                Pseudo loss:              2.43904
                Total gradient norm:      0.19523
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.969
                
Iteration (28300/50001) took 0.076 seconds.
                Mean final reward:        25.6250
                Mean return:              22.3125
                Policy entropy:           0.1147
                Pseudo loss:              0.64569
                Total gradient norm:      0.13979
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.312
                
Iteration (28400/50001) took 0.080 seconds.
                Mean final reward:        28.1250
                Mean return:              23.9375
                Policy entropy:           0.1303
                Pseudo loss:              0.71322
                Total gradient norm:      0.13377
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.188
                
Iteration (28500/50001) took 0.076 seconds.
                Mean final reward:        29.3750
                Mean return:              25.5938
                Policy entropy:           0.2009
                Pseudo loss:              1.74713
                Total gradient norm:      0.13460
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.781
                
Iteration (28600/50001) took 0.045 seconds.
                Mean final reward:        28.1250
                Mean return:              25.0938
                Policy entropy:           0.1805
                Pseudo loss:              2.22679
                Total gradient norm:      0.30516
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.031
                
Iteration (28700/50001) took 0.050 seconds.
                Mean final reward:        26.8750
                Mean return:              23.4062
                Policy entropy:           0.1670
                Pseudo loss:              0.86043
                Total gradient norm:      0.13878
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.469
                
Iteration (28800/50001) took 0.069 seconds.
                Mean final reward:        25.0000
                Mean return:              21.9688
                Policy entropy:           0.1259
                Pseudo loss:              0.83967
                Total gradient norm:      0.13439
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.031
                
Iteration (28900/50001) took 0.070 seconds.
                Mean final reward:        25.6250
                Mean return:              22.3750
                Policy entropy:           0.1262
                Pseudo loss:              0.96394
                Total gradient norm:      0.15993
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.250
                
Iteration (29000/50001) took 0.069 seconds.
                Mean final reward:        26.8750
                Mean return:              23.2188
                Policy entropy:           0.1458
                Pseudo loss:              1.42206
                Total gradient norm:      0.19337
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.656
                
Iteration (29100/50001) took 0.074 seconds.
                Mean final reward:        28.1250
                Mean return:              23.9688
                Policy entropy:           0.1768
                Pseudo loss:              1.60942
                Total gradient norm:      0.19139
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.156
                
Iteration (29200/50001) took 0.077 seconds.
                Mean final reward:        24.3750
                Mean return:              20.5312
                Policy entropy:           0.1338
                Pseudo loss:              1.24629
                Total gradient norm:      0.15864
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.844
                
Iteration (29300/50001) took 0.073 seconds.
                Mean final reward:        26.8750
                Mean return:              23.5625
                Policy entropy:           0.1231
                Pseudo loss:              0.93166
                Total gradient norm:      0.17574
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.312
                
Iteration (29400/50001) took 0.051 seconds.
                Mean final reward:        26.8750
                Mean return:              23.5625
                Policy entropy:           0.1405
                Pseudo loss:              1.04871
                Total gradient norm:      0.16868
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.312
                
Iteration (29500/50001) took 0.048 seconds.
                Mean final reward:        26.2500
                Mean return:              22.7812
                Policy entropy:           0.1538
                Pseudo loss:              2.71519
                Total gradient norm:      0.29989
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.469
                
Iteration (29600/50001) took 0.050 seconds.
                Mean final reward:        27.5000
                Mean return:              23.9375
                Policy entropy:           0.1814
                Pseudo loss:              1.68027
                Total gradient norm:      0.18057
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.562
                
Iteration (29700/50001) took 0.052 seconds.
                Mean final reward:        27.5000
                Mean return:              23.9688
                Policy entropy:           0.1393
                Pseudo loss:              1.51222
                Total gradient norm:      0.21716
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.531
                
Iteration (29800/50001) took 0.052 seconds.
                Mean final reward:        26.8750
                Mean return:              23.3438
                Policy entropy:           0.1527
                Pseudo loss:              1.16121
                Total gradient norm:      0.17917
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.531
                
Iteration (29900/50001) took 0.054 seconds.
                Mean final reward:        28.1250
                Mean return:              24.5000
                Policy entropy:           0.1588
                Pseudo loss:              1.24786
                Total gradient norm:      0.15999
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.625
                
Iteration (30000/50001) took 0.072 seconds.
                Mean final reward:        26.8750
                Mean return:              23.4375
                Policy entropy:           0.1929
                Pseudo loss:              1.88061
                Total gradient norm:      0.19800
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.438
                
Iteration (30100/50001) took 0.062 seconds.
                Mean final reward:        26.2500
                Mean return:              23.2812
                Policy entropy:           0.1651
                Pseudo loss:              1.34236
                Total gradient norm:      0.13454
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.969
                
Iteration (30200/50001) took 0.063 seconds.
                Mean final reward:        28.1250
                Mean return:              25.0312
                Policy entropy:           0.1320
                Pseudo loss:              0.77066
                Total gradient norm:      0.12644
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.094
                
Iteration (30300/50001) took 0.074 seconds.
                Mean final reward:        25.6250
                Mean return:              22.5312
                Policy entropy:           0.1472
                Pseudo loss:              1.77219
                Total gradient norm:      0.18437
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.094
                
Iteration (30400/50001) took 0.065 seconds.
                Mean final reward:        26.2500
                Mean return:              22.8125
                Policy entropy:           0.1725
                Pseudo loss:              2.30330
                Total gradient norm:      0.24723
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.438
                
Iteration (30500/50001) took 0.072 seconds.
                Mean final reward:        27.5000
                Mean return:              23.8125
                Policy entropy:           0.1491
                Pseudo loss:              1.17344
                Total gradient norm:      0.17763
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.688
                
Iteration (30600/50001) took 0.080 seconds.
                Mean final reward:        26.2500
                Mean return:              22.7500
                Policy entropy:           0.1510
                Pseudo loss:              1.44455
                Total gradient norm:      0.19589
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.258
                
Iteration (30700/50001) took 0.054 seconds.
                Mean final reward:        25.0000
                Mean return:              21.5312
                Policy entropy:           0.1560
                Pseudo loss:              1.03770
                Total gradient norm:      0.17444
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.469
                
Iteration (30800/50001) took 0.072 seconds.
                Mean final reward:        26.8750
                Mean return:              23.5938
                Policy entropy:           0.1331
                Pseudo loss:              1.38054
                Total gradient norm:      0.21588
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.281
                
Iteration (30900/50001) took 0.079 seconds.
                Mean final reward:        29.3750
                Mean return:              25.4062
                Policy entropy:           0.1208
                Pseudo loss:              1.36948
                Total gradient norm:      0.17293
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.969
                
Iteration (31000/50001) took 0.067 seconds.
                Mean final reward:        25.6250
                Mean return:              22.7188
                Policy entropy:           0.1531
                Pseudo loss:              1.22399
                Total gradient norm:      0.19103
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.906
                
Iteration (31100/50001) took 0.075 seconds.
                Mean final reward:        25.6250
                Mean return:              22.8125
                Policy entropy:           0.1679
                Pseudo loss:              0.70045
                Total gradient norm:      0.16108
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.812
                
Iteration (31200/50001) took 0.071 seconds.
                Mean final reward:        26.2500
                Mean return:              22.5312
                Policy entropy:           0.1309
                Pseudo loss:              0.60778
                Total gradient norm:      0.23397
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.719
                
Iteration (31300/50001) took 0.067 seconds.
                Mean final reward:        26.8750
                Mean return:              23.5625
                Policy entropy:           0.1556
                Pseudo loss:              1.34801
                Total gradient norm:      0.13733
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.312
                
Iteration (31400/50001) took 0.075 seconds.
                Mean final reward:        28.7500
                Mean return:              25.1562
                Policy entropy:           0.1821
                Pseudo loss:              1.34055
                Total gradient norm:      0.23144
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.594
                
Iteration (31500/50001) took 0.045 seconds.
                Mean final reward:        25.6250
                Mean return:              22.5312
                Policy entropy:           0.2170
                Pseudo loss:              2.44439
                Total gradient norm:      0.35218
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.094
                
Iteration (31600/50001) took 0.051 seconds.
                Mean final reward:        24.3750
                Mean return:              20.7812
                Policy entropy:           0.1195
                Pseudo loss:              0.73239
                Total gradient norm:      0.13866
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.594
                
Iteration (31700/50001) took 0.045 seconds.
                Mean final reward:        25.0000
                Mean return:              21.8125
                Policy entropy:           0.1195
                Pseudo loss:              1.34959
                Total gradient norm:      0.17247
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.188
                
Iteration (31800/50001) took 0.047 seconds.
                Mean final reward:        25.0000
                Mean return:              21.7812
                Policy entropy:           0.1787
                Pseudo loss:              2.20677
                Total gradient norm:      0.22332
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.219
                
Iteration (31900/50001) took 0.068 seconds.
                Mean final reward:        27.5000
                Mean return:              24.4688
                Policy entropy:           0.1658
                Pseudo loss:              1.84261
                Total gradient norm:      0.21099
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.031
                
Iteration (32000/50001) took 0.084 seconds.
                Mean final reward:        26.8750
                Mean return:              23.1875
                Policy entropy:           0.1176
                Pseudo loss:              0.71751
                Total gradient norm:      0.11090
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.688
                
Iteration (32100/50001) took 0.088 seconds.
                Mean final reward:        26.2500
                Mean return:              22.7188
                Policy entropy:           0.0996
                Pseudo loss:              0.74712
                Total gradient norm:      0.12865
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.531
                
Iteration (32200/50001) took 0.051 seconds.
                Mean final reward:        27.5000
                Mean return:              23.9688
                Policy entropy:           0.1449
                Pseudo loss:              1.00778
                Total gradient norm:      0.16145
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.531
                
Iteration (32300/50001) took 0.091 seconds.
                Mean final reward:        26.2500
                Mean return:              22.4375
                Policy entropy:           0.1363
                Pseudo loss:              0.97152
                Total gradient norm:      0.17662
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.812
                
Iteration (32400/50001) took 0.081 seconds.
                Mean final reward:        26.2500
                Mean return:              23.0625
                Policy entropy:           0.1316
                Pseudo loss:              0.51319
                Total gradient norm:      0.06439
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.188
                
Iteration (32500/50001) took 0.063 seconds.
                Mean final reward:        28.7500
                Mean return:              25.3438
                Policy entropy:           0.1596
                Pseudo loss:              1.48251
                Total gradient norm:      0.16505
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.406
                
Iteration (32600/50001) took 0.065 seconds.
                Mean final reward:        26.2500
                Mean return:              23.0312
                Policy entropy:           0.1982
                Pseudo loss:              2.17329
                Total gradient norm:      0.23563
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.219
                
Iteration (32700/50001) took 0.050 seconds.
                Mean final reward:        26.8750
                Mean return:              23.2188
                Policy entropy:           0.1679
                Pseudo loss:              1.93414
                Total gradient norm:      0.26881
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.656
                
Iteration (32800/50001) took 0.069 seconds.
                Mean final reward:        26.2500
                Mean return:              22.6250
                Policy entropy:           0.1742
                Pseudo loss:              1.71049
                Total gradient norm:      0.23896
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.625
                
Iteration (32900/50001) took 0.101 seconds.
                Mean final reward:        28.1250
                Mean return:              24.1875
                Policy entropy:           0.1197
                Pseudo loss:              2.20588
                Total gradient norm:      0.24948
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.938
                
Iteration (33000/50001) took 0.074 seconds.
                Mean final reward:        24.3750
                Mean return:              21.5625
                Policy entropy:           0.1323
                Pseudo loss:              0.71443
                Total gradient norm:      0.12841
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.812
                
Iteration (33100/50001) took 0.083 seconds.
                Mean final reward:        26.8750
                Mean return:              23.2500
                Policy entropy:           0.1711
                Pseudo loss:              1.99960
                Total gradient norm:      0.33153
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.625
                
Iteration (33200/50001) took 0.067 seconds.
                Mean final reward:        26.2500
                Mean return:              23.0938
                Policy entropy:           0.1235
                Pseudo loss:              0.75612
                Total gradient norm:      0.15853
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.156
                
Iteration (33300/50001) took 0.107 seconds.
                Mean final reward:        28.1250
                Mean return:              24.4062
                Policy entropy:           0.1259
                Pseudo loss:              0.96512
                Total gradient norm:      0.17529
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.719
                
Iteration (33400/50001) took 0.060 seconds.
                Mean final reward:        25.6250
                Mean return:              22.0938
                Policy entropy:           0.1127
                Pseudo loss:              1.39681
                Total gradient norm:      0.22157
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.290
                
Iteration (33500/50001) took 0.079 seconds.
                Mean final reward:        26.2500
                Mean return:              22.4688
                Policy entropy:           0.0953
                Pseudo loss:              1.55657
                Total gradient norm:      0.19638
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.781
                
Iteration (33600/50001) took 0.059 seconds.
                Mean final reward:        26.2500
                Mean return:              23.5938
                Policy entropy:           0.1828
                Pseudo loss:              1.29286
                Total gradient norm:      0.21348
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.656
                
Iteration (33700/50001) took 0.086 seconds.
                Mean final reward:        28.7500
                Mean return:              24.3125
                Policy entropy:           0.1246
                Pseudo loss:              0.70935
                Total gradient norm:      0.17893
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.438
                
Iteration (33800/50001) took 0.070 seconds.
                Mean final reward:        25.0000
                Mean return:              21.1250
                Policy entropy:           0.1147
                Pseudo loss:              1.02155
                Total gradient norm:      0.14164
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.875
                
Iteration (33900/50001) took 0.065 seconds.
                Mean final reward:        25.6250
                Mean return:              23.1562
                Policy entropy:           0.1374
                Pseudo loss:              0.69736
                Total gradient norm:      0.11241
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.469
                
Iteration (34000/50001) took 0.071 seconds.
                Mean final reward:        25.6250
                Mean return:              23.1875
                Policy entropy:           0.1332
                Pseudo loss:              1.69268
                Total gradient norm:      0.23712
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.438
                
Iteration (34100/50001) took 0.079 seconds.
                Mean final reward:        25.0000
                Mean return:              21.6875
                Policy entropy:           0.1516
                Pseudo loss:              1.22703
                Total gradient norm:      0.22123
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.312
                
Iteration (34200/50001) took 0.055 seconds.
                Mean final reward:        26.2500
                Mean return:              23.0938
                Policy entropy:           0.1150
                Pseudo loss:              0.74735
                Total gradient norm:      0.16398
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.156
                
Iteration (34300/50001) took 0.186 seconds.
                Mean final reward:        28.1250
                Mean return:              25.3750
                Policy entropy:           0.1769
                Pseudo loss:              1.54990
                Total gradient norm:      0.18391
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.750
                
Iteration (34400/50001) took 0.103 seconds.
                Mean final reward:        26.8750
                Mean return:              23.4688
                Policy entropy:           0.1069
                Pseudo loss:              0.41269
                Total gradient norm:      0.17000
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.406
                
Iteration (34500/50001) took 0.088 seconds.
                Mean final reward:        28.1250
                Mean return:              24.2812
                Policy entropy:           0.1435
                Pseudo loss:              1.75596
                Total gradient norm:      0.22262
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.844
                
Iteration (34600/50001) took 0.067 seconds.
                Mean final reward:        26.8750
                Mean return:              24.1562
                Policy entropy:           0.1380
                Pseudo loss:              1.98256
                Total gradient norm:      0.24097
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.719
                
Iteration (34700/50001) took 0.080 seconds.
                Mean final reward:        25.0000
                Mean return:              21.2500
                Policy entropy:           0.1047
                Pseudo loss:              1.34324
                Total gradient norm:      0.32238
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.750
                
Iteration (34800/50001) took 0.077 seconds.
                Mean final reward:        25.6250
                Mean return:              22.1250
                Policy entropy:           0.1357
                Pseudo loss:              1.09517
                Total gradient norm:      0.13880
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.500
                
Iteration (34900/50001) took 0.069 seconds.
                Mean final reward:        26.8750
                Mean return:              23.5938
                Policy entropy:           0.1438
                Pseudo loss:              1.07953
                Total gradient norm:      0.13174
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.281
                
Iteration (35000/50001) took 0.059 seconds.
                Mean final reward:        28.1250
                Mean return:              23.8125
                Policy entropy:           0.1538
                Pseudo loss:              1.71834
                Total gradient norm:      0.25953
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.312
                
Iteration (35100/50001) took 0.050 seconds.
                Mean final reward:        28.7500
                Mean return:              24.8750
                Policy entropy:           0.1383
                Pseudo loss:              1.92195
                Total gradient norm:      0.21710
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.875
                
Iteration (35200/50001) took 0.062 seconds.
                Mean final reward:        26.8750
                Mean return:              24.1562
                Policy entropy:           0.1071
                Pseudo loss:              1.37787
                Total gradient norm:      0.19497
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.719
                
Iteration (35300/50001) took 0.051 seconds.
                Mean final reward:        26.2500
                Mean return:              22.9062
                Policy entropy:           0.1003
                Pseudo loss:              0.91008
                Total gradient norm:      0.14714
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.344
                
Iteration (35400/50001) took 0.065 seconds.
                Mean final reward:        28.1250
                Mean return:              25.4688
                Policy entropy:           0.1266
                Pseudo loss:              1.06442
                Total gradient norm:      0.22515
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.656
                
Iteration (35500/50001) took 0.046 seconds.
                Mean final reward:        26.8750
                Mean return:              23.6562
                Policy entropy:           0.1900
                Pseudo loss:              1.92927
                Total gradient norm:      0.33338
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.219
                
Iteration (35600/50001) took 0.056 seconds.
                Mean final reward:        25.0000
                Mean return:              21.7188
                Policy entropy:           0.1172
                Pseudo loss:              0.67714
                Total gradient norm:      0.11430
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.281
                
Iteration (35700/50001) took 0.056 seconds.
                Mean final reward:        28.1250
                Mean return:              24.8125
                Policy entropy:           0.1209
                Pseudo loss:              1.07429
                Total gradient norm:      0.15779
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.312
                
Iteration (35800/50001) took 0.045 seconds.
                Mean final reward:        26.8750
                Mean return:              22.8125
                Policy entropy:           0.1126
                Pseudo loss:              0.84552
                Total gradient norm:      0.13679
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.062
                
Iteration (35900/50001) took 0.051 seconds.
                Mean final reward:        27.5000
                Mean return:              23.4062
                Policy entropy:           0.1132
                Pseudo loss:              0.58637
                Total gradient norm:      0.16234
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.094
                
Iteration (36000/50001) took 0.046 seconds.
                Mean final reward:        27.5000
                Mean return:              24.6875
                Policy entropy:           0.1326
                Pseudo loss:              0.88373
                Total gradient norm:      0.16767
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.812
                
Iteration (36100/50001) took 0.055 seconds.
                Mean final reward:        27.5000
                Mean return:              23.2812
                Policy entropy:           0.1329
                Pseudo loss:              2.16966
                Total gradient norm:      0.22868
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.219
                
Iteration (36200/50001) took 0.055 seconds.
                Mean final reward:        26.8750
                Mean return:              23.8438
                Policy entropy:           0.1565
                Pseudo loss:              0.97220
                Total gradient norm:      0.15215
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.031
                
Iteration (36300/50001) took 0.046 seconds.
                Mean final reward:        26.2500
                Mean return:              22.9062
                Policy entropy:           0.1253
                Pseudo loss:              1.75239
                Total gradient norm:      0.17945
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.344
                
Iteration (36400/50001) took 0.061 seconds.
                Mean final reward:        26.8750
                Mean return:              24.1875
                Policy entropy:           0.1761
                Pseudo loss:              0.95955
                Total gradient norm:      0.18107
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.688
                
Iteration (36500/50001) took 0.049 seconds.
                Mean final reward:        25.6250
                Mean return:              22.1875
                Policy entropy:           0.0945
                Pseudo loss:              0.52727
                Total gradient norm:      0.27145
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.438
                
Iteration (36600/50001) took 0.046 seconds.
                Mean final reward:        26.2500
                Mean return:              22.9062
                Policy entropy:           0.1075
                Pseudo loss:              1.41521
                Total gradient norm:      0.24244
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.344
                
Iteration (36700/50001) took 0.063 seconds.
                Mean final reward:        28.7500
                Mean return:              24.7812
                Policy entropy:           0.1264
                Pseudo loss:              1.91048
                Total gradient norm:      0.19507
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.969
                
Iteration (36800/50001) took 0.073 seconds.
                Mean final reward:        25.6250
                Mean return:              20.7812
                Policy entropy:           0.1286
                Pseudo loss:              2.06368
                Total gradient norm:      0.23428
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.645
                
Iteration (36900/50001) took 0.062 seconds.
                Mean final reward:        27.5000
                Mean return:              23.5312
                Policy entropy:           0.1113
                Pseudo loss:              0.83989
                Total gradient norm:      0.18745
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.969
                
Iteration (37000/50001) took 0.061 seconds.
                Mean final reward:        27.5000
                Mean return:              23.9688
                Policy entropy:           0.1219
                Pseudo loss:              1.40958
                Total gradient norm:      0.21524
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.531
                
Iteration (37100/50001) took 0.055 seconds.
                Mean final reward:        25.6250
                Mean return:              23.1562
                Policy entropy:           0.1487
                Pseudo loss:              1.11748
                Total gradient norm:      0.18781
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.469
                
Iteration (37200/50001) took 0.053 seconds.
                Mean final reward:        26.8750
                Mean return:              23.9062
                Policy entropy:           0.0970
                Pseudo loss:              0.55946
                Total gradient norm:      0.16922
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.969
                
Iteration (37300/50001) took 0.046 seconds.
                Mean final reward:        24.3750
                Mean return:              21.5938
                Policy entropy:           0.1171
                Pseudo loss:              0.85821
                Total gradient norm:      0.11805
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.781
                
Iteration (37400/50001) took 0.049 seconds.
                Mean final reward:        26.8750
                Mean return:              23.0625
                Policy entropy:           0.1184
                Pseudo loss:              1.61239
                Total gradient norm:      0.15631
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.812
                
Iteration (37500/50001) took 0.068 seconds.
                Mean final reward:        26.8750
                Mean return:              23.5938
                Policy entropy:           0.1261
                Pseudo loss:              1.48159
                Total gradient norm:      0.23816
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.281
                
Iteration (37600/50001) took 0.045 seconds.
                Mean final reward:        26.2500
                Mean return:              23.4688
                Policy entropy:           0.1233
                Pseudo loss:              0.96224
                Total gradient norm:      0.19909
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.781
                
Iteration (37700/50001) took 0.041 seconds.
                Mean final reward:        26.8750
                Mean return:              23.4375
                Policy entropy:           0.1267
                Pseudo loss:              1.00647
                Total gradient norm:      0.25905
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.438
                
Iteration (37800/50001) took 0.054 seconds.
                Mean final reward:        27.5000
                Mean return:              24.0000
                Policy entropy:           0.1144
                Pseudo loss:              0.96611
                Total gradient norm:      0.18114
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.500
                
Iteration (37900/50001) took 0.036 seconds.
                Mean final reward:        25.6250
                Mean return:              22.5312
                Policy entropy:           0.1208
                Pseudo loss:              0.87199
                Total gradient norm:      0.17932
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.094
                
Iteration (38000/50001) took 0.049 seconds.
                Mean final reward:        28.1250
                Mean return:              24.4062
                Policy entropy:           0.1254
                Pseudo loss:              1.59148
                Total gradient norm:      0.22502
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.719
                
Iteration (38100/50001) took 0.049 seconds.
                Mean final reward:        28.1250
                Mean return:              24.5625
                Policy entropy:           0.1115
                Pseudo loss:              1.25716
                Total gradient norm:      0.20971
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.562
                
Iteration (38200/50001) took 0.044 seconds.
                Mean final reward:        26.8750
                Mean return:              23.7500
                Policy entropy:           0.1494
                Pseudo loss:              1.33487
                Total gradient norm:      0.19992
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.125
                
Iteration (38300/50001) took 0.047 seconds.
                Mean final reward:        27.5000
                Mean return:              23.8750
                Policy entropy:           0.1428
                Pseudo loss:              1.54399
                Total gradient norm:      0.17230
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.625
                
Iteration (38400/50001) took 0.038 seconds.
                Mean final reward:        26.2500
                Mean return:              22.4688
                Policy entropy:           0.1027
                Pseudo loss:              0.55503
                Total gradient norm:      0.11484
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.781
                
Iteration (38500/50001) took 0.034 seconds.
                Mean final reward:        25.6250
                Mean return:              22.1250
                Policy entropy:           0.0930
                Pseudo loss:              0.48096
                Total gradient norm:      0.10897
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.500
                
Iteration (38600/50001) took 0.046 seconds.
                Mean final reward:        27.5000
                Mean return:              23.9062
                Policy entropy:           0.1154
                Pseudo loss:              1.28364
                Total gradient norm:      0.14219
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.594
                
Iteration (38700/50001) took 0.048 seconds.
                Mean final reward:        28.1250
                Mean return:              24.2812
                Policy entropy:           0.1271
                Pseudo loss:              1.48752
                Total gradient norm:      0.21730
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.844
                
Iteration (38800/50001) took 0.045 seconds.
                Mean final reward:        25.6250
                Mean return:              22.8125
                Policy entropy:           0.0907
                Pseudo loss:              0.45890
                Total gradient norm:      0.13266
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.812
                
Iteration (38900/50001) took 0.047 seconds.
                Mean final reward:        25.6250
                Mean return:              22.5000
                Policy entropy:           0.0962
                Pseudo loss:              0.82917
                Total gradient norm:      0.14884
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.125
                
Iteration (39000/50001) took 0.043 seconds.
                Mean final reward:        26.8750
                Mean return:              23.0000
                Policy entropy:           0.0999
                Pseudo loss:              1.41021
                Total gradient norm:      0.18710
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.875
                
Iteration (39100/50001) took 0.049 seconds.
                Mean final reward:        27.5000
                Mean return:              23.7812
                Policy entropy:           0.0941
                Pseudo loss:              0.91250
                Total gradient norm:      0.14503
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.719
                
Iteration (39200/50001) took 0.034 seconds.
                Mean final reward:        26.2500
                Mean return:              23.1875
                Policy entropy:           0.1122
                Pseudo loss:              0.87223
                Total gradient norm:      0.11797
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.062
                
Iteration (39300/50001) took 0.048 seconds.
                Mean final reward:        26.8750
                Mean return:              23.3750
                Policy entropy:           0.1276
                Pseudo loss:              1.44352
                Total gradient norm:      0.17744
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.500
                
Iteration (39400/50001) took 0.049 seconds.
                Mean final reward:        25.6250
                Mean return:              22.0938
                Policy entropy:           0.1054
                Pseudo loss:              0.73153
                Total gradient norm:      0.11624
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.531
                
Iteration (39500/50001) took 0.046 seconds.
                Mean final reward:        26.8750
                Mean return:              23.5312
                Policy entropy:           0.0902
                Pseudo loss:              0.41749
                Total gradient norm:      0.10484
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.344
                
Iteration (39600/50001) took 0.047 seconds.
                Mean final reward:        28.7500
                Mean return:              25.2188
                Policy entropy:           0.1531
                Pseudo loss:              1.28539
                Total gradient norm:      0.24124
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.531
                
Iteration (39700/50001) took 0.031 seconds.
                Mean final reward:        25.0000
                Mean return:              21.5625
                Policy entropy:           0.0846
                Pseudo loss:              0.58296
                Total gradient norm:      0.08642
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.438
                
Iteration (39800/50001) took 0.052 seconds.
                Mean final reward:        27.5000
                Mean return:              23.4062
                Policy entropy:           0.1010
                Pseudo loss:              0.84638
                Total gradient norm:      0.15166
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.094
                
Iteration (39900/50001) took 0.036 seconds.
                Mean final reward:        26.8750
                Mean return:              22.9688
                Policy entropy:           0.0828
                Pseudo loss:              1.11292
                Total gradient norm:      0.14870
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.906
                
Iteration (40000/50001) took 0.032 seconds.
                Mean final reward:        26.2500
                Mean return:              23.8438
                Policy entropy:           0.1186
                Pseudo loss:              0.99271
                Total gradient norm:      0.13244
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.406
                
Iteration (40100/50001) took 0.053 seconds.
                Mean final reward:        25.0000
                Mean return:              21.9688
                Policy entropy:           0.1345
                Pseudo loss:              0.94976
                Total gradient norm:      0.13207
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.031
                
Iteration (40200/50001) took 0.037 seconds.
                Mean final reward:        26.2500
                Mean return:              24.0625
                Policy entropy:           0.1200
                Pseudo loss:              1.32470
                Total gradient norm:      0.19630
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.188
                
Iteration (40300/50001) took 0.052 seconds.
                Mean final reward:        28.1250
                Mean return:              24.2188
                Policy entropy:           0.1476
                Pseudo loss:              1.59141
                Total gradient norm:      0.18848
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.906
                
Iteration (40400/50001) took 0.035 seconds.
                Mean final reward:        26.8750
                Mean return:              23.5312
                Policy entropy:           0.1292
                Pseudo loss:              1.01254
                Total gradient norm:      0.27825
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.344
                
Iteration (40500/50001) took 0.047 seconds.
                Mean final reward:        23.7500
                Mean return:              21.0625
                Policy entropy:           0.1377
                Pseudo loss:              0.62192
                Total gradient norm:      0.14040
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.688
                
Iteration (40600/50001) took 0.041 seconds.
                Mean final reward:        26.2500
                Mean return:              23.2500
                Policy entropy:           0.1289
                Pseudo loss:              0.52904
                Total gradient norm:      0.09857
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.000
                
Iteration (40700/50001) took 0.040 seconds.
                Mean final reward:        27.5000
                Mean return:              24.0625
                Policy entropy:           0.1206
                Pseudo loss:              0.77783
                Total gradient norm:      0.12217
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.438
                
Iteration (40800/50001) took 0.067 seconds.
                Mean final reward:        27.5000
                Mean return:              22.9062
                Policy entropy:           0.0990
                Pseudo loss:              0.61524
                Total gradient norm:      0.14118
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.594
                
Iteration (40900/50001) took 0.037 seconds.
                Mean final reward:        26.2500
                Mean return:              22.5625
                Policy entropy:           0.0878
                Pseudo loss:              1.17148
                Total gradient norm:      0.17891
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.688
                
Iteration (41000/50001) took 0.038 seconds.
                Mean final reward:        28.1250
                Mean return:              24.7188
                Policy entropy:           0.1016
                Pseudo loss:              0.46332
                Total gradient norm:      0.07135
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.406
                
Iteration (41100/50001) took 0.040 seconds.
                Mean final reward:        26.2500
                Mean return:              23.2188
                Policy entropy:           0.1765
                Pseudo loss:              1.20638
                Total gradient norm:      0.20203
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.031
                
Iteration (41200/50001) took 0.043 seconds.
                Mean final reward:        26.8750
                Mean return:              23.9688
                Policy entropy:           0.1309
                Pseudo loss:              1.17035
                Total gradient norm:      0.26320
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.906
                
Iteration (41300/50001) took 0.039 seconds.
                Mean final reward:        26.8750
                Mean return:              24.1875
                Policy entropy:           0.1172
                Pseudo loss:              0.55728
                Total gradient norm:      0.14210
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.688
                
Iteration (41400/50001) took 0.041 seconds.
                Mean final reward:        25.6250
                Mean return:              22.6875
                Policy entropy:           0.0913
                Pseudo loss:              0.59315
                Total gradient norm:      0.10977
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.938
                
Iteration (41500/50001) took 0.042 seconds.
                Mean final reward:        28.7500
                Mean return:              24.9688
                Policy entropy:           0.1335
                Pseudo loss:              1.72291
                Total gradient norm:      0.23653
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.781
                
Iteration (41600/50001) took 0.039 seconds.
                Mean final reward:        25.6250
                Mean return:              22.4375
                Policy entropy:           0.1201
                Pseudo loss:              1.46252
                Total gradient norm:      0.21138
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.188
                
Iteration (41700/50001) took 0.039 seconds.
                Mean final reward:        27.5000
                Mean return:              24.0000
                Policy entropy:           0.1580
                Pseudo loss:              1.37491
                Total gradient norm:      0.25488
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.500
                
Iteration (41800/50001) took 0.047 seconds.
                Mean final reward:        26.8750
                Mean return:              23.9062
                Policy entropy:           0.0857
                Pseudo loss:              0.84497
                Total gradient norm:      0.20459
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.969
                
Iteration (41900/50001) took 0.049 seconds.
                Mean final reward:        25.6250
                Mean return:              22.1250
                Policy entropy:           0.1220
                Pseudo loss:              0.82298
                Total gradient norm:      0.11538
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.500
                
Iteration (42000/50001) took 0.048 seconds.
                Mean final reward:        26.2500
                Mean return:              22.5000
                Policy entropy:           0.0973
                Pseudo loss:              0.83528
                Total gradient norm:      0.14639
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.750
                
Iteration (42100/50001) took 0.080 seconds.
                Mean final reward:        26.8750
                Mean return:              23.3125
                Policy entropy:           0.0854
                Pseudo loss:              0.87258
                Total gradient norm:      0.17795
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.562
                
Iteration (42200/50001) took 0.049 seconds.
                Mean final reward:        25.0000
                Mean return:              21.8125
                Policy entropy:           0.1041
                Pseudo loss:              1.62437
                Total gradient norm:      0.25288
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.188
                
Iteration (42300/50001) took 0.127 seconds.
                Mean final reward:        27.5000
                Mean return:              23.3750
                Policy entropy:           0.1061
                Pseudo loss:              1.45882
                Total gradient norm:      0.17033
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.125
                
Iteration (42400/50001) took 0.079 seconds.
                Mean final reward:        28.1250
                Mean return:              24.6250
                Policy entropy:           0.1026
                Pseudo loss:              1.16462
                Total gradient norm:      0.21642
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.500
                
Iteration (42500/50001) took 0.067 seconds.
                Mean final reward:        26.8750
                Mean return:              23.6562
                Policy entropy:           0.1252
                Pseudo loss:              0.89316
                Total gradient norm:      0.13634
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.219
                
Iteration (42600/50001) took 0.063 seconds.
                Mean final reward:        25.0000
                Mean return:              21.8750
                Policy entropy:           0.1167
                Pseudo loss:              1.40505
                Total gradient norm:      0.18046
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.125
                
Iteration (42700/50001) took 0.070 seconds.
                Mean final reward:        26.8750
                Mean return:              23.0312
                Policy entropy:           0.1082
                Pseudo loss:              1.25985
                Total gradient norm:      0.14647
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.844
                
Iteration (42800/50001) took 0.064 seconds.
                Mean final reward:        26.2500
                Mean return:              23.1875
                Policy entropy:           0.1181
                Pseudo loss:              0.79400
                Total gradient norm:      0.16443
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.062
                
Iteration (42900/50001) took 0.077 seconds.
                Mean final reward:        26.8750
                Mean return:              23.7188
                Policy entropy:           0.1341
                Pseudo loss:              0.86531
                Total gradient norm:      0.12204
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.156
                
Iteration (43000/50001) took 0.072 seconds.
                Mean final reward:        26.8750
                Mean return:              23.0938
                Policy entropy:           0.1354
                Pseudo loss:              2.41881
                Total gradient norm:      0.25908
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.548
                
Iteration (43100/50001) took 0.051 seconds.
                Mean final reward:        26.2500
                Mean return:              23.6562
                Policy entropy:           0.0894
                Pseudo loss:              0.27957
                Total gradient norm:      0.05058
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.594
                
Iteration (43200/50001) took 0.059 seconds.
                Mean final reward:        25.0000
                Mean return:              21.7188
                Policy entropy:           0.0971
                Pseudo loss:              1.75044
                Total gradient norm:      0.27410
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.281
                
Iteration (43300/50001) took 0.072 seconds.
                Mean final reward:        26.2500
                Mean return:              22.5938
                Policy entropy:           0.1231
                Pseudo loss:              0.64959
                Total gradient norm:      0.15217
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.656
                
Iteration (43400/50001) took 0.058 seconds.
                Mean final reward:        25.0000
                Mean return:              22.5625
                Policy entropy:           0.1360
                Pseudo loss:              1.67093
                Total gradient norm:      0.26706
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.438
                
Iteration (43500/50001) took 0.063 seconds.
                Mean final reward:        26.8750
                Mean return:              23.6250
                Policy entropy:           0.0786
                Pseudo loss:              0.66304
                Total gradient norm:      0.12841
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.250
                
Iteration (43600/50001) took 0.067 seconds.
                Mean final reward:        28.1250
                Mean return:              24.6250
                Policy entropy:           0.0803
                Pseudo loss:              0.74035
                Total gradient norm:      0.19427
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.500
                
Iteration (43700/50001) took 0.061 seconds.
                Mean final reward:        28.1250
                Mean return:              25.3438
                Policy entropy:           0.1531
                Pseudo loss:              0.67045
                Total gradient norm:      0.14420
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.781
                
Iteration (43800/50001) took 0.089 seconds.
                Mean final reward:        26.8750
                Mean return:              23.2812
                Policy entropy:           0.1047
                Pseudo loss:              0.74164
                Total gradient norm:      0.15073
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.594
                
Iteration (43900/50001) took 0.084 seconds.
                Mean final reward:        26.2500
                Mean return:              23.4688
                Policy entropy:           0.0942
                Pseudo loss:              0.20700
                Total gradient norm:      0.09143
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.781
                
Iteration (44000/50001) took 0.062 seconds.
                Mean final reward:        28.7500
                Mean return:              25.4688
                Policy entropy:           0.1230
                Pseudo loss:              0.58957
                Total gradient norm:      0.11115
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.281
                
Iteration (44100/50001) took 0.058 seconds.
                Mean final reward:        28.7500
                Mean return:              25.6875
                Policy entropy:           0.1492
                Pseudo loss:              0.95310
                Total gradient norm:      0.28733
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.062
                
Iteration (44200/50001) took 0.041 seconds.
                Mean final reward:        27.5000
                Mean return:              24.5938
                Policy entropy:           0.1579
                Pseudo loss:              1.19667
                Total gradient norm:      0.22044
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.906
                
Iteration (44300/50001) took 0.068 seconds.
                Mean final reward:        28.1250
                Mean return:              24.7812
                Policy entropy:           0.0940
                Pseudo loss:              0.38820
                Total gradient norm:      0.10527
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.344
                
Iteration (44400/50001) took 0.068 seconds.
                Mean final reward:        26.8750
                Mean return:              23.3438
                Policy entropy:           0.1028
                Pseudo loss:              0.78615
                Total gradient norm:      0.20022
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.531
                
Iteration (44500/50001) took 0.071 seconds.
                Mean final reward:        26.8750
                Mean return:              22.8750
                Policy entropy:           0.0680
                Pseudo loss:              0.46818
                Total gradient norm:      0.09381
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.000
                
Iteration (44600/50001) took 0.067 seconds.
                Mean final reward:        28.1250
                Mean return:              24.1562
                Policy entropy:           0.1052
                Pseudo loss:              0.58745
                Total gradient norm:      0.14721
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.969
                
Iteration (44700/50001) took 0.085 seconds.
                Mean final reward:        24.3750
                Mean return:              21.0938
                Policy entropy:           0.1597
                Pseudo loss:              1.36284
                Total gradient norm:      0.21337
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.281
                
Iteration (44800/50001) took 0.066 seconds.
                Mean final reward:        27.5000
                Mean return:              23.7188
                Policy entropy:           0.1178
                Pseudo loss:              1.58193
                Total gradient norm:      0.20652
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.781
                
Iteration (44900/50001) took 0.073 seconds.
                Mean final reward:        25.6250
                Mean return:              21.7500
                Policy entropy:           0.1091
                Pseudo loss:              0.94773
                Total gradient norm:      0.21503
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.875
                
Iteration (45000/50001) took 0.069 seconds.
                Mean final reward:        25.0000
                Mean return:              21.1875
                Policy entropy:           0.0773
                Pseudo loss:              0.77607
                Total gradient norm:      0.14257
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.812
                
Iteration (45100/50001) took 0.057 seconds.
                Mean final reward:        25.6250
                Mean return:              23.0625
                Policy entropy:           0.0795
                Pseudo loss:              0.35709
                Total gradient norm:      0.12060
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.562
                
Iteration (45200/50001) took 0.068 seconds.
                Mean final reward:        28.1250
                Mean return:              24.4688
                Policy entropy:           0.0965
                Pseudo loss:              0.73394
                Total gradient norm:      0.16407
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.656
                
Iteration (45300/50001) took 0.072 seconds.
                Mean final reward:        29.3750
                Mean return:              25.8438
                Policy entropy:           0.1228
                Pseudo loss:              1.52660
                Total gradient norm:      0.20351
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.531
                
Iteration (45400/50001) took 0.065 seconds.
                Mean final reward:        28.1250
                Mean return:              24.4375
                Policy entropy:           0.1161
                Pseudo loss:              1.01296
                Total gradient norm:      0.21239
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.688
                
Iteration (45500/50001) took 0.099 seconds.
                Mean final reward:        24.3750
                Mean return:              20.9375
                Policy entropy:           0.0857
                Pseudo loss:              0.35262
                Total gradient norm:      0.13084
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.438
                
Iteration (45600/50001) took 0.091 seconds.
                Mean final reward:        28.1250
                Mean return:              23.4688
                Policy entropy:           0.0945
                Pseudo loss:              2.57209
                Total gradient norm:      0.22695
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.656
                
Iteration (45700/50001) took 0.087 seconds.
                Mean final reward:        25.6250
                Mean return:              22.3125
                Policy entropy:           0.0978
                Pseudo loss:              0.38262
                Total gradient norm:      0.14074
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.312
                
Iteration (45800/50001) took 0.058 seconds.
                Mean final reward:        25.0000
                Mean return:              22.3438
                Policy entropy:           0.0931
                Pseudo loss:              0.76900
                Total gradient norm:      0.16517
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.656
                
Iteration (45900/50001) took 0.066 seconds.
                Mean final reward:        26.8750
                Mean return:              23.6250
                Policy entropy:           0.0928
                Pseudo loss:              0.47282
                Total gradient norm:      0.14672
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.250
                
Iteration (46000/50001) took 0.057 seconds.
                Mean final reward:        26.2500
                Mean return:              23.0625
                Policy entropy:           0.0691
                Pseudo loss:              0.51603
                Total gradient norm:      0.11419
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.188
                
Iteration (46100/50001) took 0.079 seconds.
                Mean final reward:        26.8750
                Mean return:              23.4375
                Policy entropy:           0.0849
                Pseudo loss:              0.21237
                Total gradient norm:      0.10339
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.438
                
Iteration (46200/50001) took 0.092 seconds.
                Mean final reward:        25.0000
                Mean return:              21.7812
                Policy entropy:           0.1133
                Pseudo loss:              0.87280
                Total gradient norm:      0.23557
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.219
                
Iteration (46300/50001) took 0.064 seconds.
                Mean final reward:        25.6250
                Mean return:              22.2500
                Policy entropy:           0.0818
                Pseudo loss:              1.11438
                Total gradient norm:      0.20512
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.375
                
Iteration (46400/50001) took 0.081 seconds.
                Mean final reward:        27.5000
                Mean return:              24.0938
                Policy entropy:           0.1228
                Pseudo loss:              0.79015
                Total gradient norm:      0.14552
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.406
                
Iteration (46500/50001) took 0.094 seconds.
                Mean final reward:        26.8750
                Mean return:              23.3750
                Policy entropy:           0.1079
                Pseudo loss:              0.45339
                Total gradient norm:      0.13277
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.500
                
Iteration (46600/50001) took 0.067 seconds.
                Mean final reward:        25.6250
                Mean return:              22.1250
                Policy entropy:           0.1100
                Pseudo loss:              1.77155
                Total gradient norm:      0.20548
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.500
                
Iteration (46700/50001) took 0.064 seconds.
                Mean final reward:        28.1250
                Mean return:              24.5000
                Policy entropy:           0.0891
                Pseudo loss:              1.07620
                Total gradient norm:      0.16507
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.625
                
Iteration (46800/50001) took 0.071 seconds.
                Mean final reward:        26.2500
                Mean return:              22.5938
                Policy entropy:           0.1098
                Pseudo loss:              0.61144
                Total gradient norm:      0.15936
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.656
                
Iteration (46900/50001) took 0.066 seconds.
                Mean final reward:        27.5000
                Mean return:              24.0000
                Policy entropy:           0.1112
                Pseudo loss:              0.77354
                Total gradient norm:      0.15086
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.500
                
Iteration (47000/50001) took 0.102 seconds.
                Mean final reward:        28.1250
                Mean return:              24.1562
                Policy entropy:           0.1310
                Pseudo loss:              2.59568
                Total gradient norm:      0.23649
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.969
                
Iteration (47100/50001) took 0.074 seconds.
                Mean final reward:        26.8750
                Mean return:              24.0938
                Policy entropy:           0.0711
                Pseudo loss:              0.13153
                Total gradient norm:      0.05698
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.781
                
Iteration (47200/50001) took 0.086 seconds.
                Mean final reward:        23.7500
                Mean return:              21.2812
                Policy entropy:           0.0898
                Pseudo loss:              0.18266
                Total gradient norm:      0.08753
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.469
                
Iteration (47300/50001) took 0.044 seconds.
                Mean final reward:        28.1250
                Mean return:              24.8125
                Policy entropy:           0.1321
                Pseudo loss:              1.34842
                Total gradient norm:      0.19144
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.312
                
Iteration (47400/50001) took 0.067 seconds.
                Mean final reward:        26.8750
                Mean return:              23.6250
                Policy entropy:           0.1078
                Pseudo loss:              0.80229
                Total gradient norm:      0.17332
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.250
                
Iteration (47500/50001) took 0.058 seconds.
                Mean final reward:        25.6250
                Mean return:              22.7500
                Policy entropy:           0.0910
                Pseudo loss:              0.91256
                Total gradient norm:      0.29398
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.875
                
Iteration (47600/50001) took 0.118 seconds.
                Mean final reward:        27.5000
                Mean return:              23.9375
                Policy entropy:           0.0668
                Pseudo loss:              0.22000
                Total gradient norm:      0.09251
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.562
                
Iteration (47700/50001) took 0.067 seconds.
                Mean final reward:        25.0000
                Mean return:              21.0000
                Policy entropy:           0.0677
                Pseudo loss:              0.59958
                Total gradient norm:      0.14081
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.000
                
Iteration (47800/50001) took 0.070 seconds.
                Mean final reward:        27.5000
                Mean return:              24.6562
                Policy entropy:           0.1300
                Pseudo loss:              0.63437
                Total gradient norm:      0.06589
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.844
                
Iteration (47900/50001) took 0.120 seconds.
                Mean final reward:        28.1250
                Mean return:              24.2812
                Policy entropy:           0.0809
                Pseudo loss:              0.96939
                Total gradient norm:      0.14859
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.844
                
Iteration (48000/50001) took 0.059 seconds.
                Mean final reward:        28.7500
                Mean return:              25.5938
                Policy entropy:           0.0917
                Pseudo loss:              1.90956
                Total gradient norm:      0.27919
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.156
                
Iteration (48100/50001) took 0.061 seconds.
                Mean final reward:        28.1250
                Mean return:              24.6562
                Policy entropy:           0.0772
                Pseudo loss:              0.17926
                Total gradient norm:      0.07280
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.469
                
Iteration (48200/50001) took 0.077 seconds.
                Mean final reward:        26.8750
                Mean return:              23.4688
                Policy entropy:           0.0911
                Pseudo loss:              1.72494
                Total gradient norm:      0.21863
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.406
                
Iteration (48300/50001) took 0.062 seconds.
                Mean final reward:        25.0000
                Mean return:              22.1562
                Policy entropy:           0.0732
                Pseudo loss:              0.26838
                Total gradient norm:      0.10517
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.844
                
Iteration (48400/50001) took 0.077 seconds.
                Mean final reward:        27.5000
                Mean return:              23.5312
                Policy entropy:           0.0792
                Pseudo loss:              1.22057
                Total gradient norm:      0.25698
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.969
                
Iteration (48500/50001) took 0.063 seconds.
                Mean final reward:        27.5000
                Mean return:              23.8750
                Policy entropy:           0.0653
                Pseudo loss:              0.52814
                Total gradient norm:      0.10322
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.625
                
Iteration (48600/50001) took 0.060 seconds.
                Mean final reward:        28.1250
                Mean return:              24.7188
                Policy entropy:           0.1243
                Pseudo loss:              1.26842
                Total gradient norm:      0.17692
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.406
                
Iteration (48700/50001) took 0.077 seconds.
                Mean final reward:        27.5000
                Mean return:              24.1250
                Policy entropy:           0.0892
                Pseudo loss:              0.21281
                Total gradient norm:      0.09775
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.375
                
Iteration (48800/50001) took 0.074 seconds.
                Mean final reward:        25.0000
                Mean return:              21.9062
                Policy entropy:           0.0690
                Pseudo loss:              0.65852
                Total gradient norm:      0.13283
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.094
                
Iteration (48900/50001) took 0.057 seconds.
                Mean final reward:        25.6250
                Mean return:              23.0000
                Policy entropy:           0.1198
                Pseudo loss:              0.59013
                Total gradient norm:      0.10560
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.625
                
Iteration (49000/50001) took 0.088 seconds.
                Mean final reward:        28.1250
                Mean return:              24.0938
                Policy entropy:           0.0864
                Pseudo loss:              0.95957
                Total gradient norm:      0.12909
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.031
                
Iteration (49100/50001) took 0.094 seconds.
                Mean final reward:        26.2500
                Mean return:              22.9062
                Policy entropy:           0.0943
                Pseudo loss:              1.06163
                Total gradient norm:      0.22716
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.344
                
Iteration (49200/50001) took 0.096 seconds.
                Mean final reward:        25.0000
                Mean return:              21.2500
                Policy entropy:           0.0944
                Pseudo loss:              0.52683
                Total gradient norm:      0.13929
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.750
                
Iteration (49300/50001) took 0.067 seconds.
                Mean final reward:        27.5000
                Mean return:              24.4062
                Policy entropy:           0.0895
                Pseudo loss:              1.15081
                Total gradient norm:      0.17729
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.094
                
Iteration (49400/50001) took 0.070 seconds.
                Mean final reward:        28.7500
                Mean return:              24.1875
                Policy entropy:           0.0875
                Pseudo loss:              0.25177
                Total gradient norm:      0.10969
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.562
                
Iteration (49500/50001) took 0.058 seconds.
                Mean final reward:        27.5000
                Mean return:              24.3750
                Policy entropy:           0.1040
                Pseudo loss:              0.66682
                Total gradient norm:      0.16304
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.125
                
Iteration (49600/50001) took 0.095 seconds.
                Mean final reward:        26.2500
                Mean return:              23.4688
                Policy entropy:           0.1148
                Pseudo loss:              0.48535
                Total gradient norm:      0.05623
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.781
                
Iteration (49700/50001) took 0.065 seconds.
                Mean final reward:        26.8750
                Mean return:              23.4062
                Policy entropy:           0.0671
                Pseudo loss:              0.53290
                Total gradient norm:      0.11304
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.469
                
Iteration (49800/50001) took 0.096 seconds.
                Mean final reward:        24.3750
                Mean return:              21.0938
                Policy entropy:           0.0870
                Pseudo loss:              1.31572
                Total gradient norm:      0.15375
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.281
                
Iteration (49900/50001) took 0.042 seconds.
                Mean final reward:        26.8750
                Mean return:              23.2500
                Policy entropy:           0.0985
                Pseudo loss:              0.99385
                Total gradient norm:      0.11584
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.625
                
Iteration (50000/50001) took 0.075 seconds.
                Mean final reward:        23.7500
                Mean return:              21.0000
                Policy entropy:           0.1200
                Pseudo loss:              1.41100
                Total gradient norm:      0.19670
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.750
                
Training took 4075.248 seconds.
