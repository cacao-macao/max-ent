Using device: cpu

Using optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)

Iteration (0/50001) took 0.090 seconds.
                Mean final reward:        0.3750
                Mean return:              -7.9062
                Policy entropy:           1.5803
                Pseudo loss:              -6.95110
                Total gradient norm:      0.41120
                Solved trajectories:      3 / 32
                Avg steps to solve:       2.333
                
Iteration (100/50001) took 0.136 seconds.
                Mean final reward:        2.0938
                Mean return:              -4.9375
                Policy entropy:           1.5783
                Pseudo loss:              -5.11724
                Total gradient norm:      0.39032
                Solved trajectories:      9 / 32
                Avg steps to solve:       3.000
                
Iteration (200/50001) took 0.152 seconds.
                Mean final reward:        2.7812
                Mean return:              -4.6250
                Policy entropy:           1.5751
                Pseudo loss:              -4.04700
                Total gradient norm:      0.42057
                Solved trajectories:      10 / 32
                Avg steps to solve:       4.900
                
Iteration (300/50001) took 0.153 seconds.
                Mean final reward:        4.5000
                Mean return:              -2.0938
                Policy entropy:           1.5710
                Pseudo loss:              -1.88355
                Total gradient norm:      0.48308
                Solved trajectories:      15 / 32
                Avg steps to solve:       4.867
                
Iteration (400/50001) took 0.127 seconds.
                Mean final reward:        5.1875
                Mean return:              -1.0625
                Policy entropy:           1.5571
                Pseudo loss:              -1.33079
                Total gradient norm:      0.46262
                Solved trajectories:      17 / 32
                Avg steps to solve:       4.824
                
Iteration (500/50001) took 0.080 seconds.
                Mean final reward:        4.1562
                Mean return:              -2.0312
                Policy entropy:           1.5483
                Pseudo loss:              -3.26928
                Total gradient norm:      0.44920
                Solved trajectories:      15 / 32
                Avg steps to solve:       4.000
                
Iteration (600/50001) took 0.106 seconds.
                Mean final reward:        6.2188
                Mean return:              1.0312
                Policy entropy:           1.5186
                Pseudo loss:              -0.72683
                Total gradient norm:      0.42301
                Solved trajectories:      20 / 32
                Avg steps to solve:       3.900
                
Iteration (700/50001) took 0.086 seconds.
                Mean final reward:        7.9375
                Mean return:              3.8750
                Policy entropy:           1.4937
                Pseudo loss:              0.95729
                Total gradient norm:      0.45185
                Solved trajectories:      26 / 32
                Avg steps to solve:       3.923
                
Iteration (800/50001) took 0.141 seconds.
                Mean final reward:        5.1875
                Mean return:              -0.5000
                Policy entropy:           1.4829
                Pseudo loss:              -2.52755
                Total gradient norm:      0.39434
                Solved trajectories:      17 / 32
                Avg steps to solve:       3.765
                
Iteration (900/50001) took 0.110 seconds.
                Mean final reward:        6.5625
                Mean return:              0.4062
                Policy entropy:           1.4442
                Pseudo loss:              -0.01985
                Total gradient norm:      0.40760
                Solved trajectories:      20 / 32
                Avg steps to solve:       5.450
                
Iteration (1000/50001) took 0.168 seconds.
                Mean final reward:        6.5625
                Mean return:              0.9062
                Policy entropy:           1.4234
                Pseudo loss:              -0.64459
                Total gradient norm:      0.39927
                Solved trajectories:      20 / 32
                Avg steps to solve:       4.650
                
Iteration (1100/50001) took 0.093 seconds.
                Mean final reward:        8.2812
                Mean return:              4.2188
                Policy entropy:           1.3623
                Pseudo loss:              0.76480
                Total gradient norm:      0.35955
                Solved trajectories:      27 / 32
                Avg steps to solve:       4.148
                
Iteration (1200/50001) took 0.147 seconds.
                Mean final reward:        7.5938
                Mean return:              2.1562
                Policy entropy:           1.3465
                Pseudo loss:              0.54867
                Total gradient norm:      0.35064
                Solved trajectories:      21 / 32
                Avg steps to solve:       4.571
                
Iteration (1300/50001) took 0.152 seconds.
                Mean final reward:        7.9375
                Mean return:              3.5312
                Policy entropy:           1.2818
                Pseudo loss:              0.29391
                Total gradient norm:      0.31478
                Solved trajectories:      26 / 32
                Avg steps to solve:       4.346
                
Iteration (1400/50001) took 0.096 seconds.
                Mean final reward:        7.9375
                Mean return:              3.4062
                Policy entropy:           1.2866
                Pseudo loss:              0.43357
                Total gradient norm:      0.30454
                Solved trajectories:      24 / 32
                Avg steps to solve:       4.042
                
Iteration (1500/50001) took 0.125 seconds.
                Mean final reward:        9.6562
                Mean return:              5.8750
                Policy entropy:           1.2043
                Pseudo loss:              2.45984
                Total gradient norm:      0.27848
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.433
                
Iteration (1600/50001) took 0.047 seconds.
                Mean final reward:        8.2812
                Mean return:              4.6250
                Policy entropy:           1.2111
                Pseudo loss:              -0.03782
                Total gradient norm:      0.28417
                Solved trajectories:      27 / 32
                Avg steps to solve:       3.667
                
Iteration (1700/50001) took 0.062 seconds.
                Mean final reward:        9.3125
                Mean return:              5.4375
                Policy entropy:           1.1508
                Pseudo loss:              1.90933
                Total gradient norm:      0.23598
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.533
                
Iteration (1800/50001) took 0.067 seconds.
                Mean final reward:        8.6250
                Mean return:              4.5312
                Policy entropy:           1.1367
                Pseudo loss:              0.82306
                Total gradient norm:      0.26777
                Solved trajectories:      28 / 32
                Avg steps to solve:       4.393
                
Iteration (1900/50001) took 0.081 seconds.
                Mean final reward:        9.3125
                Mean return:              5.2812
                Policy entropy:           1.1010
                Pseudo loss:              1.90235
                Total gradient norm:      0.23666
                Solved trajectories:      29 / 32
                Avg steps to solve:       4.517
                
Iteration (2000/50001) took 0.150 seconds.
                Mean final reward:        8.6250
                Mean return:              3.8750
                Policy entropy:           1.1250
                Pseudo loss:              1.34590
                Total gradient norm:      0.26262
                Solved trajectories:      28 / 32
                Avg steps to solve:       5.143
                
Iteration (2100/50001) took 0.103 seconds.
                Mean final reward:        9.6562
                Mean return:              6.1562
                Policy entropy:           1.0120
                Pseudo loss:              1.80578
                Total gradient norm:      0.20699
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.323
                
Iteration (2200/50001) took 0.122 seconds.
                Mean final reward:        10.0000
                Mean return:              6.5625
                Policy entropy:           0.9537
                Pseudo loss:              2.29607
                Total gradient norm:      0.18216
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.438
                
Iteration (2300/50001) took 0.104 seconds.
                Mean final reward:        9.6562
                Mean return:              5.1250
                Policy entropy:           1.0048
                Pseudo loss:              2.57894
                Total gradient norm:      0.21060
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.233
                
Iteration (2400/50001) took 0.076 seconds.
                Mean final reward:        9.6562
                Mean return:              5.4688
                Policy entropy:           1.0033
                Pseudo loss:              2.25728
                Total gradient norm:      0.18790
                Solved trajectories:      29 / 32
                Avg steps to solve:       4.690
                
Iteration (2500/50001) took 0.136 seconds.
                Mean final reward:        9.6562
                Mean return:              5.9062
                Policy entropy:           0.9124
                Pseudo loss:              1.73492
                Total gradient norm:      0.21241
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.581
                
Iteration (2600/50001) took 0.078 seconds.
                Mean final reward:        9.6562
                Mean return:              6.2812
                Policy entropy:           0.8840
                Pseudo loss:              1.43028
                Total gradient norm:      0.17174
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.000
                
Iteration (2700/50001) took 0.088 seconds.
                Mean final reward:        10.0000
                Mean return:              6.5312
                Policy entropy:           0.8563
                Pseudo loss:              2.01108
                Total gradient norm:      0.15997
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.469
                
Iteration (2800/50001) took 0.059 seconds.
                Mean final reward:        9.6562
                Mean return:              6.3125
                Policy entropy:           0.8571
                Pseudo loss:              1.54429
                Total gradient norm:      0.16604
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.161
                
Iteration (2900/50001) took 0.056 seconds.
                Mean final reward:        10.0000
                Mean return:              6.5938
                Policy entropy:           0.8348
                Pseudo loss:              2.24824
                Total gradient norm:      0.16001
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.226
                
Iteration (3000/50001) took 0.062 seconds.
                Mean final reward:        9.6562
                Mean return:              6.5938
                Policy entropy:           0.7946
                Pseudo loss:              0.93858
                Total gradient norm:      0.16711
                Solved trajectories:      31 / 32
                Avg steps to solve:       3.871
                
Iteration (3100/50001) took 0.114 seconds.
                Mean final reward:        10.0000
                Mean return:              7.1875
                Policy entropy:           0.7607
                Pseudo loss:              1.57725
                Total gradient norm:      0.11402
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.812
                
Iteration (3200/50001) took 0.104 seconds.
                Mean final reward:        10.0000
                Mean return:              6.9688
                Policy entropy:           0.7382
                Pseudo loss:              1.54721
                Total gradient norm:      0.13478
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.031
                
Iteration (3300/50001) took 0.119 seconds.
                Mean final reward:        10.0000
                Mean return:              6.6875
                Policy entropy:           0.7337
                Pseudo loss:              1.78934
                Total gradient norm:      0.14922
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.312
                
Iteration (3400/50001) took 0.055 seconds.
                Mean final reward:        10.0000
                Mean return:              7.4062
                Policy entropy:           0.6795
                Pseudo loss:              1.28139
                Total gradient norm:      0.10799
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.594
                
Iteration (3500/50001) took 0.106 seconds.
                Mean final reward:        10.0000
                Mean return:              7.4375
                Policy entropy:           0.6782
                Pseudo loss:              1.18422
                Total gradient norm:      0.09350
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.562
                
Iteration (3600/50001) took 0.070 seconds.
                Mean final reward:        10.0000
                Mean return:              6.5625
                Policy entropy:           0.6968
                Pseudo loss:              1.86356
                Total gradient norm:      0.13867
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.438
                
Iteration (3700/50001) took 0.095 seconds.
                Mean final reward:        10.0000
                Mean return:              6.7500
                Policy entropy:           0.6367
                Pseudo loss:              1.97427
                Total gradient norm:      0.16300
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.250
                
Iteration (3800/50001) took 0.127 seconds.
                Mean final reward:        10.0000
                Mean return:              6.5625
                Policy entropy:           0.7191
                Pseudo loss:              1.88610
                Total gradient norm:      0.14445
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.258
                
Iteration (3900/50001) took 0.096 seconds.
                Mean final reward:        10.0000
                Mean return:              7.0938
                Policy entropy:           0.6220
                Pseudo loss:              1.36387
                Total gradient norm:      0.10428
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.906
                
Iteration (4000/50001) took 0.122 seconds.
                Mean final reward:        10.0000
                Mean return:              6.7812
                Policy entropy:           0.6544
                Pseudo loss:              1.40505
                Total gradient norm:      0.12514
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.219
                
Iteration (4100/50001) took 0.101 seconds.
                Mean final reward:        10.0000
                Mean return:              7.3750
                Policy entropy:           0.6217
                Pseudo loss:              1.04732
                Total gradient norm:      0.12336
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.625
                
Iteration (4200/50001) took 0.076 seconds.
                Mean final reward:        10.0000
                Mean return:              7.7812
                Policy entropy:           0.5370
                Pseudo loss:              0.83380
                Total gradient norm:      0.09591
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.219
                
Iteration (4300/50001) took 0.093 seconds.
                Mean final reward:        10.0000
                Mean return:              7.1875
                Policy entropy:           0.5627
                Pseudo loss:              1.30617
                Total gradient norm:      0.09504
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.812
                
Iteration (4400/50001) took 0.095 seconds.
                Mean final reward:        10.0000
                Mean return:              7.2500
                Policy entropy:           0.5882
                Pseudo loss:              1.32062
                Total gradient norm:      0.11338
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.750
                
Iteration (4500/50001) took 0.089 seconds.
                Mean final reward:        10.0000
                Mean return:              7.2812
                Policy entropy:           0.5308
                Pseudo loss:              1.28990
                Total gradient norm:      0.16378
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.719
                
Iteration (4600/50001) took 0.062 seconds.
                Mean final reward:        10.0000
                Mean return:              7.2188
                Policy entropy:           0.5806
                Pseudo loss:              1.10856
                Total gradient norm:      0.17301
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.781
                
Iteration (4700/50001) took 0.128 seconds.
                Mean final reward:        10.0000
                Mean return:              6.9688
                Policy entropy:           0.5356
                Pseudo loss:              1.28223
                Total gradient norm:      0.10757
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.031
                
Iteration (4800/50001) took 0.061 seconds.
                Mean final reward:        10.0000
                Mean return:              7.7188
                Policy entropy:           0.4475
                Pseudo loss:              0.75566
                Total gradient norm:      0.08363
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.281
                
Iteration (4900/50001) took 0.128 seconds.
                Mean final reward:        10.0000
                Mean return:              7.3750
                Policy entropy:           0.5189
                Pseudo loss:              0.83118
                Total gradient norm:      0.10364
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.625
                
Iteration (5000/50001) took 0.069 seconds.
                Mean final reward:        10.0000
                Mean return:              7.3125
                Policy entropy:           0.5293
                Pseudo loss:              1.28468
                Total gradient norm:      0.11839
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.688
                
Iteration (5100/50001) took 0.129 seconds.
                Mean final reward:        10.0000
                Mean return:              7.5312
                Policy entropy:           0.4645
                Pseudo loss:              0.95962
                Total gradient norm:      0.10822
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.469
                
Iteration (5200/50001) took 0.058 seconds.
                Mean final reward:        10.0000
                Mean return:              7.0000
                Policy entropy:           0.5200
                Pseudo loss:              0.96825
                Total gradient norm:      0.12288
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.000
                
Iteration (5300/50001) took 0.118 seconds.
                Mean final reward:        10.0000
                Mean return:              7.4688
                Policy entropy:           0.4943
                Pseudo loss:              0.76035
                Total gradient norm:      0.08038
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.531
                
Iteration (5400/50001) took 0.061 seconds.
                Mean final reward:        10.0000
                Mean return:              7.6250
                Policy entropy:           0.4033
                Pseudo loss:              0.55704
                Total gradient norm:      0.07072
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.375
                
Iteration (5500/50001) took 0.080 seconds.
                Mean final reward:        10.0000
                Mean return:              7.3750
                Policy entropy:           0.4651
                Pseudo loss:              0.84105
                Total gradient norm:      0.08164
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.625
                
Iteration (5600/50001) took 0.111 seconds.
                Mean final reward:        10.0000
                Mean return:              6.9375
                Policy entropy:           0.4464
                Pseudo loss:              1.42453
                Total gradient norm:      0.13426
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.062
                
Iteration (5700/50001) took 0.089 seconds.
                Mean final reward:        10.0000
                Mean return:              7.0938
                Policy entropy:           0.4362
                Pseudo loss:              0.98496
                Total gradient norm:      0.07170
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.906
                
Iteration (5800/50001) took 0.095 seconds.
                Mean final reward:        10.0000
                Mean return:              7.2812
                Policy entropy:           0.4233
                Pseudo loss:              0.85328
                Total gradient norm:      0.08578
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.719
                
Iteration (5900/50001) took 0.054 seconds.
                Mean final reward:        10.0000
                Mean return:              7.5938
                Policy entropy:           0.4148
                Pseudo loss:              0.83474
                Total gradient norm:      0.09030
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.406
                
Iteration (6000/50001) took 0.120 seconds.
                Mean final reward:        10.0000
                Mean return:              7.2500
                Policy entropy:           0.4053
                Pseudo loss:              0.87996
                Total gradient norm:      0.07707
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.750
                
Iteration (6100/50001) took 0.068 seconds.
                Mean final reward:        10.0000
                Mean return:              6.8438
                Policy entropy:           0.4127
                Pseudo loss:              1.24986
                Total gradient norm:      0.12141
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.156
                
Iteration (6200/50001) took 0.078 seconds.
                Mean final reward:        10.0000
                Mean return:              7.4688
                Policy entropy:           0.4034
                Pseudo loss:              0.99597
                Total gradient norm:      0.08437
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.531
                
Iteration (6300/50001) took 0.083 seconds.
                Mean final reward:        10.0000
                Mean return:              7.4375
                Policy entropy:           0.4239
                Pseudo loss:              0.62477
                Total gradient norm:      0.06360
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.562
                
Iteration (6400/50001) took 0.098 seconds.
                Mean final reward:        10.0000
                Mean return:              7.0938
                Policy entropy:           0.3979
                Pseudo loss:              1.08163
                Total gradient norm:      0.09778
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.906
                
Iteration (6500/50001) took 0.098 seconds.
                Mean final reward:        10.0000
                Mean return:              7.3125
                Policy entropy:           0.3884
                Pseudo loss:              0.84800
                Total gradient norm:      0.07465
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.688
                
Iteration (6600/50001) took 0.105 seconds.
                Mean final reward:        10.0000
                Mean return:              7.4062
                Policy entropy:           0.3533
                Pseudo loss:              0.67833
                Total gradient norm:      0.07926
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.594
                
Iteration (6700/50001) took 0.092 seconds.
                Mean final reward:        10.0000
                Mean return:              7.4688
                Policy entropy:           0.4422
                Pseudo loss:              0.90280
                Total gradient norm:      0.06952
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.531
                
Iteration (6800/50001) took 0.072 seconds.
                Mean final reward:        10.0000
                Mean return:              7.3750
                Policy entropy:           0.3340
                Pseudo loss:              0.83981
                Total gradient norm:      0.07414
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.625
                
Iteration (6900/50001) took 0.075 seconds.
                Mean final reward:        10.0000
                Mean return:              7.2812
                Policy entropy:           0.3330
                Pseudo loss:              0.89904
                Total gradient norm:      0.06592
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.719
                
Iteration (7000/50001) took 0.079 seconds.
                Mean final reward:        10.0000
                Mean return:              7.4062
                Policy entropy:           0.3057
                Pseudo loss:              0.86752
                Total gradient norm:      0.07877
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.594
                
Iteration (7100/50001) took 0.113 seconds.
                Mean final reward:        10.0000
                Mean return:              7.1250
                Policy entropy:           0.3445
                Pseudo loss:              0.63679
                Total gradient norm:      0.08003
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.875
                
Iteration (7200/50001) took 0.058 seconds.
                Mean final reward:        10.0000
                Mean return:              7.5938
                Policy entropy:           0.3354
                Pseudo loss:              0.64193
                Total gradient norm:      0.06604
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.406
                
Iteration (7300/50001) took 0.057 seconds.
                Mean final reward:        10.0000
                Mean return:              7.6875
                Policy entropy:           0.2843
                Pseudo loss:              0.52119
                Total gradient norm:      0.07248
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.312
                
Iteration (7400/50001) took 0.065 seconds.
                Mean final reward:        10.0000
                Mean return:              7.6562
                Policy entropy:           0.3082
                Pseudo loss:              0.49251
                Total gradient norm:      0.06391
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.344
                
Iteration (7500/50001) took 0.068 seconds.
                Mean final reward:        10.0000
                Mean return:              7.0000
                Policy entropy:           0.3877
                Pseudo loss:              1.04771
                Total gradient norm:      0.07828
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.000
                
Iteration (7600/50001) took 0.064 seconds.
                Mean final reward:        10.0000
                Mean return:              7.6250
                Policy entropy:           0.3220
                Pseudo loss:              0.55753
                Total gradient norm:      0.07124
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.375
                
Iteration (7700/50001) took 0.066 seconds.
                Mean final reward:        10.0000
                Mean return:              7.1250
                Policy entropy:           0.3598
                Pseudo loss:              0.82347
                Total gradient norm:      0.12804
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.875
                
Iteration (7800/50001) took 0.098 seconds.
                Mean final reward:        10.0000
                Mean return:              7.6875
                Policy entropy:           0.3070
                Pseudo loss:              0.44225
                Total gradient norm:      0.06932
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.312
                
Iteration (7900/50001) took 0.068 seconds.
                Mean final reward:        10.0000
                Mean return:              7.5000
                Policy entropy:           0.3134
                Pseudo loss:              0.50648
                Total gradient norm:      0.06653
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.500
                
Iteration (8000/50001) took 0.116 seconds.
                Mean final reward:        10.0000
                Mean return:              7.4688
                Policy entropy:           0.3460
                Pseudo loss:              0.71998
                Total gradient norm:      0.08147
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.531
                
Iteration (8100/50001) took 0.067 seconds.
                Mean final reward:        10.0000
                Mean return:              7.2188
                Policy entropy:           0.3087
                Pseudo loss:              0.94679
                Total gradient norm:      0.07751
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.781
                
Iteration (8200/50001) took 0.100 seconds.
                Mean final reward:        10.0000
                Mean return:              7.5312
                Policy entropy:           0.3240
                Pseudo loss:              0.62613
                Total gradient norm:      0.10132
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.469
                
Iteration (8300/50001) took 0.110 seconds.
                Mean final reward:        10.0000
                Mean return:              7.2188
                Policy entropy:           0.3485
                Pseudo loss:              0.85655
                Total gradient norm:      0.08242
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.781
                
Iteration (8400/50001) took 0.069 seconds.
                Mean final reward:        10.0000
                Mean return:              7.4375
                Policy entropy:           0.3088
                Pseudo loss:              0.66570
                Total gradient norm:      0.08459
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.562
                
Iteration (8500/50001) took 0.067 seconds.
                Mean final reward:        10.0000
                Mean return:              7.8438
                Policy entropy:           0.2517
                Pseudo loss:              0.49673
                Total gradient norm:      0.05390
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.156
                
Iteration (8600/50001) took 0.065 seconds.
                Mean final reward:        10.0000
                Mean return:              7.2188
                Policy entropy:           0.3059
                Pseudo loss:              0.64708
                Total gradient norm:      0.07138
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.781
                
Iteration (8700/50001) took 0.062 seconds.
                Mean final reward:        10.0000
                Mean return:              7.3438
                Policy entropy:           0.2899
                Pseudo loss:              0.60850
                Total gradient norm:      0.06504
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.656
                
Iteration (8800/50001) took 0.041 seconds.
                Mean final reward:        10.0000
                Mean return:              7.2812
                Policy entropy:           0.2790
                Pseudo loss:              0.57525
                Total gradient norm:      0.06989
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.719
                
Iteration (8900/50001) took 0.045 seconds.
                Mean final reward:        10.0000
                Mean return:              7.4062
                Policy entropy:           0.3078
                Pseudo loss:              0.65477
                Total gradient norm:      0.07429
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.594
                
Iteration (9000/50001) took 0.096 seconds.
                Mean final reward:        10.0000
                Mean return:              7.6250
                Policy entropy:           0.2686
                Pseudo loss:              0.59268
                Total gradient norm:      0.07014
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.375
                
Iteration (9100/50001) took 0.108 seconds.
                Mean final reward:        10.0000
                Mean return:              7.6562
                Policy entropy:           0.2623
                Pseudo loss:              0.54729
                Total gradient norm:      0.07954
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.344
                
Iteration (9200/50001) took 0.126 seconds.
                Mean final reward:        10.0000
                Mean return:              7.1875
                Policy entropy:           0.2624
                Pseudo loss:              0.42550
                Total gradient norm:      0.05052
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.812
                
Iteration (9300/50001) took 0.081 seconds.
                Mean final reward:        10.0000
                Mean return:              7.5938
                Policy entropy:           0.2649
                Pseudo loss:              0.69061
                Total gradient norm:      0.08036
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.406
                
Iteration (9400/50001) took 0.062 seconds.
                Mean final reward:        10.0000
                Mean return:              7.4375
                Policy entropy:           0.3067
                Pseudo loss:              0.62735
                Total gradient norm:      0.09065
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.562
                
Iteration (9500/50001) took 0.072 seconds.
                Mean final reward:        10.0000
                Mean return:              7.3125
                Policy entropy:           0.2685
                Pseudo loss:              0.73891
                Total gradient norm:      0.07318
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.688
                
Iteration (9600/50001) took 0.114 seconds.
                Mean final reward:        10.0000
                Mean return:              8.0625
                Policy entropy:           0.2383
                Pseudo loss:              0.26839
                Total gradient norm:      0.04521
                Solved trajectories:      32 / 32
                Avg steps to solve:       2.938
                
Iteration (9700/50001) took 0.121 seconds.
                Mean final reward:        10.0000
                Mean return:              7.5000
                Policy entropy:           0.2613
                Pseudo loss:              0.44143
                Total gradient norm:      0.05380
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.500
                
Iteration (9800/50001) took 0.132 seconds.
                Mean final reward:        10.0000
                Mean return:              7.4688
                Policy entropy:           0.2451
                Pseudo loss:              0.50008
                Total gradient norm:      0.06080
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.531
                
Iteration (9900/50001) took 0.103 seconds.
                Mean final reward:        10.0000
                Mean return:              7.3438
                Policy entropy:           0.2589
                Pseudo loss:              0.39756
                Total gradient norm:      0.04894
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.656
                
Iteration (10000/50001) took 0.088 seconds.
                Mean final reward:        10.0000
                Mean return:              7.5938
                Policy entropy:           0.2488
                Pseudo loss:              0.59375
                Total gradient norm:      0.07300
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.406
                
Iteration (10100/50001) took 0.065 seconds.
                Mean final reward:        10.0000
                Mean return:              7.4062
                Policy entropy:           0.2411
                Pseudo loss:              0.77870
                Total gradient norm:      0.10435
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.594
                
Iteration (10200/50001) took 0.040 seconds.
                Mean final reward:        10.0000
                Mean return:              7.5000
                Policy entropy:           0.2277
                Pseudo loss:              0.75438
                Total gradient norm:      0.09885
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.500
                
Iteration (10300/50001) took 0.086 seconds.
                Mean final reward:        10.0000
                Mean return:              7.6562
                Policy entropy:           0.2322
                Pseudo loss:              0.50915
                Total gradient norm:      0.07772
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.344
                
Iteration (10400/50001) took 0.068 seconds.
                Mean final reward:        10.0000
                Mean return:              7.1875
                Policy entropy:           0.2769
                Pseudo loss:              0.86857
                Total gradient norm:      0.11735
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.812
                
Iteration (10500/50001) took 0.078 seconds.
                Mean final reward:        10.0000
                Mean return:              7.7812
                Policy entropy:           0.2249
                Pseudo loss:              0.68808
                Total gradient norm:      0.06858
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.219
                
Iteration (10600/50001) took 0.134 seconds.
                Mean final reward:        10.0000
                Mean return:              7.6875
                Policy entropy:           0.2232
                Pseudo loss:              0.28004
                Total gradient norm:      0.04149
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.312
                
Iteration (10700/50001) took 0.103 seconds.
                Mean final reward:        10.0000
                Mean return:              7.5625
                Policy entropy:           0.2356
                Pseudo loss:              0.60757
                Total gradient norm:      0.06100
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.438
                
Iteration (10800/50001) took 0.081 seconds.
                Mean final reward:        10.0000
                Mean return:              7.3750
                Policy entropy:           0.2577
                Pseudo loss:              0.51981
                Total gradient norm:      0.10041
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.625
                
Iteration (10900/50001) took 0.083 seconds.
                Mean final reward:        10.0000
                Mean return:              7.6250
                Policy entropy:           0.2416
                Pseudo loss:              0.47124
                Total gradient norm:      0.05353
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.375
                
Iteration (11000/50001) took 0.130 seconds.
                Mean final reward:        10.0000
                Mean return:              7.4062
                Policy entropy:           0.2445
                Pseudo loss:              0.63209
                Total gradient norm:      0.06047
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.594
                
Iteration (11100/50001) took 0.116 seconds.
                Mean final reward:        10.0000
                Mean return:              7.2500
                Policy entropy:           0.2903
                Pseudo loss:              0.64817
                Total gradient norm:      0.08041
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.750
                
Iteration (11200/50001) took 0.112 seconds.
                Mean final reward:        10.0000
                Mean return:              7.6875
                Policy entropy:           0.1791
                Pseudo loss:              0.20556
                Total gradient norm:      0.04502
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.312
                
Iteration (11300/50001) took 0.090 seconds.
                Mean final reward:        10.0000
                Mean return:              7.6875
                Policy entropy:           0.2084
                Pseudo loss:              0.57507
                Total gradient norm:      0.08096
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.312
                
Iteration (11400/50001) took 0.062 seconds.
                Mean final reward:        10.0000
                Mean return:              7.4688
                Policy entropy:           0.2449
                Pseudo loss:              0.55206
                Total gradient norm:      0.06970
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.531
                
Iteration (11500/50001) took 0.070 seconds.
                Mean final reward:        10.0000
                Mean return:              7.5938
                Policy entropy:           0.1978
                Pseudo loss:              0.46681
                Total gradient norm:      0.06124
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.406
                
Iteration (11600/50001) took 0.106 seconds.
                Mean final reward:        10.0000
                Mean return:              7.2812
                Policy entropy:           0.2446
                Pseudo loss:              0.55210
                Total gradient norm:      0.07176
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.719
                
Iteration (11700/50001) took 0.086 seconds.
                Mean final reward:        10.0000
                Mean return:              7.5000
                Policy entropy:           0.2178
                Pseudo loss:              0.76872
                Total gradient norm:      0.09299
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.500
                
Iteration (11800/50001) took 0.056 seconds.
                Mean final reward:        10.0000
                Mean return:              7.4688
                Policy entropy:           0.2131
                Pseudo loss:              0.23935
                Total gradient norm:      0.06195
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.531
                
Iteration (11900/50001) took 0.085 seconds.
                Mean final reward:        10.0000
                Mean return:              7.3125
                Policy entropy:           0.2252
                Pseudo loss:              0.51427
                Total gradient norm:      0.08674
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.688
                
Iteration (12000/50001) took 0.111 seconds.
                Mean final reward:        10.0000
                Mean return:              7.7188
                Policy entropy:           0.2382
                Pseudo loss:              0.48957
                Total gradient norm:      0.07425
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.281
                
Iteration (12100/50001) took 0.108 seconds.
                Mean final reward:        10.0000
                Mean return:              7.2188
                Policy entropy:           0.2000
                Pseudo loss:              0.53953
                Total gradient norm:      0.07373
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.781
                
Iteration (12200/50001) took 0.056 seconds.
                Mean final reward:        10.0000
                Mean return:              7.6250
                Policy entropy:           0.1605
                Pseudo loss:              0.32267
                Total gradient norm:      0.05857
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.375
                
Iteration (12300/50001) took 0.098 seconds.
                Mean final reward:        10.0000
                Mean return:              7.0938
                Policy entropy:           0.2959
                Pseudo loss:              0.71816
                Total gradient norm:      0.06681
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.906
                
Iteration (12400/50001) took 0.045 seconds.
                Mean final reward:        10.0000
                Mean return:              7.5312
                Policy entropy:           0.1604
                Pseudo loss:              0.29479
                Total gradient norm:      0.04972
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.469
                
Iteration (12500/50001) took 0.058 seconds.
                Mean final reward:        10.0000
                Mean return:              7.4062
                Policy entropy:           0.2022
                Pseudo loss:              0.50569
                Total gradient norm:      0.05400
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.594
                
Iteration (12600/50001) took 0.081 seconds.
                Mean final reward:        10.0000
                Mean return:              7.1250
                Policy entropy:           0.2277
                Pseudo loss:              0.80904
                Total gradient norm:      0.09362
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.875
                
Iteration (12700/50001) took 0.105 seconds.
                Mean final reward:        10.0000
                Mean return:              7.5625
                Policy entropy:           0.2091
                Pseudo loss:              0.65780
                Total gradient norm:      0.09138
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.438
                
Iteration (12800/50001) took 0.111 seconds.
                Mean final reward:        10.0000
                Mean return:              6.8125
                Policy entropy:           0.2333
                Pseudo loss:              0.80447
                Total gradient norm:      0.08120
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.188
                
Iteration (12900/50001) took 0.092 seconds.
                Mean final reward:        10.0000
                Mean return:              7.7500
                Policy entropy:           0.1909
                Pseudo loss:              0.33281
                Total gradient norm:      0.05113
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.250
                
Iteration (13000/50001) took 0.057 seconds.
                Mean final reward:        10.0000
                Mean return:              7.4062
                Policy entropy:           0.2187
                Pseudo loss:              0.57017
                Total gradient norm:      0.07714
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.594
                
Iteration (13100/50001) took 0.100 seconds.
                Mean final reward:        10.0000
                Mean return:              7.1562
                Policy entropy:           0.2450
                Pseudo loss:              0.65920
                Total gradient norm:      0.06551
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.844
                
Iteration (13200/50001) took 0.073 seconds.
                Mean final reward:        10.0000
                Mean return:              7.2812
                Policy entropy:           0.2077
                Pseudo loss:              0.60450
                Total gradient norm:      0.08045
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.719
                
Iteration (13300/50001) took 0.084 seconds.
                Mean final reward:        10.0000
                Mean return:              7.4688
                Policy entropy:           0.1991
                Pseudo loss:              0.46150
                Total gradient norm:      0.07712
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.531
                
Iteration (13400/50001) took 0.063 seconds.
                Mean final reward:        10.0000
                Mean return:              7.5625
                Policy entropy:           0.2638
                Pseudo loss:              0.77807
                Total gradient norm:      0.08747
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.438
                
Iteration (13500/50001) took 0.123 seconds.
                Mean final reward:        10.0000
                Mean return:              7.0938
                Policy entropy:           0.2641
                Pseudo loss:              0.63968
                Total gradient norm:      0.06788
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.906
                
Iteration (13600/50001) took 0.073 seconds.
                Mean final reward:        10.0000
                Mean return:              7.4375
                Policy entropy:           0.2152
                Pseudo loss:              0.44239
                Total gradient norm:      0.05118
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.562
                
Iteration (13700/50001) took 0.107 seconds.
                Mean final reward:        10.0000
                Mean return:              7.5625
                Policy entropy:           0.2285
                Pseudo loss:              0.53725
                Total gradient norm:      0.06822
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.438
                
Iteration (13800/50001) took 0.068 seconds.
                Mean final reward:        10.0000
                Mean return:              7.6250
                Policy entropy:           0.1864
                Pseudo loss:              0.44650
                Total gradient norm:      0.05193
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.375
                
Iteration (13900/50001) took 0.074 seconds.
                Mean final reward:        10.0000
                Mean return:              7.3125
                Policy entropy:           0.1971
                Pseudo loss:              0.42824
                Total gradient norm:      0.05796
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.688
                
Iteration (14000/50001) took 0.097 seconds.
                Mean final reward:        10.0000
                Mean return:              7.2812
                Policy entropy:           0.2495
                Pseudo loss:              0.71684
                Total gradient norm:      0.10129
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.719
                
Iteration (14100/50001) took 0.110 seconds.
                Mean final reward:        10.0000
                Mean return:              7.7500
                Policy entropy:           0.1673
                Pseudo loss:              0.37428
                Total gradient norm:      0.04255
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.250
                
Iteration (14200/50001) took 0.067 seconds.
                Mean final reward:        10.0000
                Mean return:              7.3125
                Policy entropy:           0.2291
                Pseudo loss:              0.69601
                Total gradient norm:      0.07220
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.688
                
Iteration (14300/50001) took 0.081 seconds.
                Mean final reward:        10.0000
                Mean return:              7.8125
                Policy entropy:           0.1652
                Pseudo loss:              0.39493
                Total gradient norm:      0.06999
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.188
                
Iteration (14400/50001) took 0.047 seconds.
                Mean final reward:        10.0000
                Mean return:              7.3438
                Policy entropy:           0.1796
                Pseudo loss:              0.60470
                Total gradient norm:      0.08550
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.656
                
Iteration (14500/50001) took 0.062 seconds.
                Mean final reward:        10.0000
                Mean return:              7.6250
                Policy entropy:           0.1988
                Pseudo loss:              0.40109
                Total gradient norm:      0.07885
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.375
                
Iteration (14600/50001) took 0.068 seconds.
                Mean final reward:        10.0000
                Mean return:              7.3125
                Policy entropy:           0.2110
                Pseudo loss:              0.59635
                Total gradient norm:      0.08717
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.688
                
Iteration (14700/50001) took 0.036 seconds.
                Mean final reward:        10.0000
                Mean return:              7.3438
                Policy entropy:           0.1747
                Pseudo loss:              0.55463
                Total gradient norm:      0.08770
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.656
                
Iteration (14800/50001) took 0.041 seconds.
                Mean final reward:        10.0000
                Mean return:              7.5312
                Policy entropy:           0.2169
                Pseudo loss:              0.53714
                Total gradient norm:      0.05861
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.469
                
Iteration (14900/50001) took 0.106 seconds.
                Mean final reward:        10.0000
                Mean return:              7.6875
                Policy entropy:           0.1679
                Pseudo loss:              0.71514
                Total gradient norm:      0.09355
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.312
                
Iteration (15000/50001) took 0.100 seconds.
                Mean final reward:        10.0000
                Mean return:              7.6875
                Policy entropy:           0.2046
                Pseudo loss:              0.47178
                Total gradient norm:      0.08672
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.312
                
Iteration (15100/50001) took 0.125 seconds.
                Mean final reward:        10.0000
                Mean return:              7.5312
                Policy entropy:           0.1791
                Pseudo loss:              0.53828
                Total gradient norm:      0.06689
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.469
                
Iteration (15200/50001) took 0.095 seconds.
                Mean final reward:        10.0000
                Mean return:              7.3750
                Policy entropy:           0.1756
                Pseudo loss:              0.40189
                Total gradient norm:      0.07656
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.625
                
Iteration (15300/50001) took 0.152 seconds.
                Mean final reward:        10.0000
                Mean return:              7.5000
                Policy entropy:           0.1764
                Pseudo loss:              0.33114
                Total gradient norm:      0.06262
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.500
                
Iteration (15400/50001) took 0.180 seconds.
                Mean final reward:        10.0000
                Mean return:              7.4062
                Policy entropy:           0.2109
                Pseudo loss:              0.30024
                Total gradient norm:      0.06902
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.594
                
Iteration (15500/50001) took 0.165 seconds.
                Mean final reward:        10.0000
                Mean return:              7.5938
                Policy entropy:           0.1968
                Pseudo loss:              0.55569
                Total gradient norm:      0.09076
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.406
                
Iteration (15600/50001) took 0.113 seconds.
                Mean final reward:        10.0000
                Mean return:              7.6875
                Policy entropy:           0.2044
                Pseudo loss:              0.36579
                Total gradient norm:      0.04389
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.312
                
Iteration (15700/50001) took 0.094 seconds.
                Mean final reward:        10.0000
                Mean return:              7.5312
                Policy entropy:           0.2223
                Pseudo loss:              0.48470
                Total gradient norm:      0.06123
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.469
                
Iteration (15800/50001) took 0.124 seconds.
                Mean final reward:        10.0000
                Mean return:              7.2500
                Policy entropy:           0.2266
                Pseudo loss:              0.53702
                Total gradient norm:      0.07058
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.750
                
Iteration (15900/50001) took 0.101 seconds.
                Mean final reward:        10.0000
                Mean return:              7.3438
                Policy entropy:           0.1952
                Pseudo loss:              0.54586
                Total gradient norm:      0.08708
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.656
                
Iteration (16000/50001) took 0.121 seconds.
                Mean final reward:        10.0000
                Mean return:              7.5938
                Policy entropy:           0.2027
                Pseudo loss:              0.34586
                Total gradient norm:      0.04472
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.406
                
Iteration (16100/50001) took 0.110 seconds.
                Mean final reward:        10.0000
                Mean return:              7.7812
                Policy entropy:           0.1719
                Pseudo loss:              0.40174
                Total gradient norm:      0.06427
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.219
                
Iteration (16200/50001) took 0.053 seconds.
                Mean final reward:        10.0000
                Mean return:              7.7812
                Policy entropy:           0.1643
                Pseudo loss:              0.25248
                Total gradient norm:      0.04309
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.219
                
Iteration (16300/50001) took 0.075 seconds.
                Mean final reward:        10.0000
                Mean return:              7.5312
                Policy entropy:           0.1507
                Pseudo loss:              0.23329
                Total gradient norm:      0.05172
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.469
                
Iteration (16400/50001) took 0.033 seconds.
                Mean final reward:        10.0000
                Mean return:              8.3750
                Policy entropy:           0.1330
                Pseudo loss:              0.11162
                Total gradient norm:      0.02944
                Solved trajectories:      32 / 32
                Avg steps to solve:       2.625
                
Iteration (16500/50001) took 0.086 seconds.
                Mean final reward:        10.0000
                Mean return:              7.9688
                Policy entropy:           0.1585
                Pseudo loss:              0.37386
                Total gradient norm:      0.07906
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.031
                
Iteration (16600/50001) took 0.130 seconds.
                Mean final reward:        10.0000
                Mean return:              7.5938
                Policy entropy:           0.2141
                Pseudo loss:              0.36302
                Total gradient norm:      0.04553
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.406
                
Iteration (16700/50001) took 0.077 seconds.
                Mean final reward:        10.0000
                Mean return:              7.5312
                Policy entropy:           0.1762
                Pseudo loss:              0.32631
                Total gradient norm:      0.04242
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.469
                
Iteration (16800/50001) took 0.069 seconds.
                Mean final reward:        10.0000
                Mean return:              7.7812
                Policy entropy:           0.1633
                Pseudo loss:              0.28748
                Total gradient norm:      0.03961
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.219
                
Iteration (16900/50001) took 0.098 seconds.
                Mean final reward:        10.0000
                Mean return:              7.3438
                Policy entropy:           0.1539
                Pseudo loss:              0.48727
                Total gradient norm:      0.06111
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.656
                
Iteration (17000/50001) took 0.060 seconds.
                Mean final reward:        10.0000
                Mean return:              7.3438
                Policy entropy:           0.1753
                Pseudo loss:              0.44692
                Total gradient norm:      0.05810
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.656
                
Iteration (17100/50001) took 0.100 seconds.
                Mean final reward:        10.0000
                Mean return:              7.6562
                Policy entropy:           0.1485
                Pseudo loss:              0.16321
                Total gradient norm:      0.05570
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.344
                
Iteration (17200/50001) took 0.087 seconds.
                Mean final reward:        10.0000
                Mean return:              7.0000
                Policy entropy:           0.2180
                Pseudo loss:              0.66142
                Total gradient norm:      0.07681
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.000
                
Iteration (17300/50001) took 0.076 seconds.
                Mean final reward:        10.0000
                Mean return:              7.1875
                Policy entropy:           0.1985
                Pseudo loss:              0.50346
                Total gradient norm:      0.05912
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.812
                
Iteration (17400/50001) took 0.057 seconds.
                Mean final reward:        10.0000
                Mean return:              7.2500
                Policy entropy:           0.1710
                Pseudo loss:              0.43815
                Total gradient norm:      0.05306
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.750
                
Iteration (17500/50001) took 0.076 seconds.
                Mean final reward:        10.0000
                Mean return:              7.6875
                Policy entropy:           0.1602
                Pseudo loss:              0.32803
                Total gradient norm:      0.05663
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.312
                
Iteration (17600/50001) took 0.095 seconds.
                Mean final reward:        10.0000
                Mean return:              7.2500
                Policy entropy:           0.1741
                Pseudo loss:              0.36070
                Total gradient norm:      0.08718
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.750
                
Iteration (17700/50001) took 0.095 seconds.
                Mean final reward:        10.0000
                Mean return:              7.4375
                Policy entropy:           0.1594
                Pseudo loss:              0.39631
                Total gradient norm:      0.04905
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.562
                
Iteration (17800/50001) took 0.061 seconds.
                Mean final reward:        10.0000
                Mean return:              7.6250
                Policy entropy:           0.1605
                Pseudo loss:              0.23722
                Total gradient norm:      0.03831
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.375
                
Iteration (17900/50001) took 0.092 seconds.
                Mean final reward:        10.0000
                Mean return:              7.6562
                Policy entropy:           0.2038
                Pseudo loss:              0.33545
                Total gradient norm:      0.05116
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.344
                
Iteration (18000/50001) took 0.069 seconds.
                Mean final reward:        10.0000
                Mean return:              7.7500
                Policy entropy:           0.1598
                Pseudo loss:              0.35459
                Total gradient norm:      0.04302
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.250
                
Iteration (18100/50001) took 0.103 seconds.
                Mean final reward:        10.0000
                Mean return:              7.5000
                Policy entropy:           0.1726
                Pseudo loss:              0.38066
                Total gradient norm:      0.05855
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.500
                
Iteration (18200/50001) took 0.067 seconds.
                Mean final reward:        10.0000
                Mean return:              7.4062
                Policy entropy:           0.1612
                Pseudo loss:              0.38368
                Total gradient norm:      0.04488
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.594
                
Iteration (18300/50001) took 0.105 seconds.
                Mean final reward:        10.0000
                Mean return:              7.3125
                Policy entropy:           0.1942
                Pseudo loss:              0.68891
                Total gradient norm:      0.08268
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.688
                
Iteration (18400/50001) took 0.158 seconds.
                Mean final reward:        10.0000
                Mean return:              7.9688
                Policy entropy:           0.1304
                Pseudo loss:              0.43926
                Total gradient norm:      0.07522
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.031
                
Iteration (18500/50001) took 0.101 seconds.
                Mean final reward:        10.0000
                Mean return:              7.6875
                Policy entropy:           0.1729
                Pseudo loss:              0.38620
                Total gradient norm:      0.04405
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.312
                
Iteration (18600/50001) took 0.064 seconds.
                Mean final reward:        10.0000
                Mean return:              7.6875
                Policy entropy:           0.1633
                Pseudo loss:              0.34282
                Total gradient norm:      0.06108
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.312
                
Iteration (18700/50001) took 0.096 seconds.
                Mean final reward:        10.0000
                Mean return:              7.6875
                Policy entropy:           0.1680
                Pseudo loss:              0.27740
                Total gradient norm:      0.09483
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.312
                
Iteration (18800/50001) took 0.088 seconds.
                Mean final reward:        10.0000
                Mean return:              7.9062
                Policy entropy:           0.1369
                Pseudo loss:              0.20776
                Total gradient norm:      0.03572
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.094
                
Iteration (18900/50001) took 0.101 seconds.
                Mean final reward:        10.0000
                Mean return:              7.5625
                Policy entropy:           0.1865
                Pseudo loss:              0.35193
                Total gradient norm:      0.05240
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.438
                
Iteration (19000/50001) took 0.065 seconds.
                Mean final reward:        10.0000
                Mean return:              7.8750
                Policy entropy:           0.1321
                Pseudo loss:              0.23545
                Total gradient norm:      0.06316
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.125
                
Iteration (19100/50001) took 0.101 seconds.
                Mean final reward:        10.0000
                Mean return:              7.6562
                Policy entropy:           0.1359
                Pseudo loss:              0.35210
                Total gradient norm:      0.04595
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.344
                
Iteration (19200/50001) took 0.075 seconds.
                Mean final reward:        10.0000
                Mean return:              7.6250
                Policy entropy:           0.1603
                Pseudo loss:              0.29530
                Total gradient norm:      0.06155
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.375
                
Iteration (19300/50001) took 0.090 seconds.
                Mean final reward:        10.0000
                Mean return:              7.7500
                Policy entropy:           0.1426
                Pseudo loss:              0.49029
                Total gradient norm:      0.06743
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.250
                
Iteration (19400/50001) took 0.108 seconds.
                Mean final reward:        10.0000
                Mean return:              7.5312
                Policy entropy:           0.1728
                Pseudo loss:              0.32584
                Total gradient norm:      0.06403
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.469
                
Iteration (19500/50001) took 0.105 seconds.
                Mean final reward:        10.0000
                Mean return:              7.9062
                Policy entropy:           0.1196
                Pseudo loss:              0.13127
                Total gradient norm:      0.05208
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.094
                
Iteration (19600/50001) took 0.126 seconds.
                Mean final reward:        10.0000
                Mean return:              7.2500
                Policy entropy:           0.2128
                Pseudo loss:              0.42716
                Total gradient norm:      0.08368
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.750
                
Iteration (19700/50001) took 0.069 seconds.
                Mean final reward:        10.0000
                Mean return:              7.8125
                Policy entropy:           0.1275
                Pseudo loss:              0.12382
                Total gradient norm:      0.02875
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.188
                
Iteration (19800/50001) took 0.045 seconds.
                Mean final reward:        10.0000
                Mean return:              7.5938
                Policy entropy:           0.1535
                Pseudo loss:              0.38430
                Total gradient norm:      0.04579
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.406
                
Iteration (19900/50001) took 0.105 seconds.
                Mean final reward:        10.0000
                Mean return:              7.7500
                Policy entropy:           0.1424
                Pseudo loss:              0.37242
                Total gradient norm:      0.04412
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.250
                
Iteration (20000/50001) took 0.090 seconds.
                Mean final reward:        10.0000
                Mean return:              7.5625
                Policy entropy:           0.1671
                Pseudo loss:              0.37486
                Total gradient norm:      0.05529
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.438
                
Iteration (20100/50001) took 0.078 seconds.
                Mean final reward:        10.0000
                Mean return:              7.9375
                Policy entropy:           0.1647
                Pseudo loss:              0.32882
                Total gradient norm:      0.04749
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.062
                
Iteration (20200/50001) took 0.109 seconds.
                Mean final reward:        10.0000
                Mean return:              8.1562
                Policy entropy:           0.1459
                Pseudo loss:              0.28156
                Total gradient norm:      0.04093
                Solved trajectories:      32 / 32
                Avg steps to solve:       2.844
                
Iteration (20300/50001) took 0.110 seconds.
                Mean final reward:        10.0000
                Mean return:              7.4375
                Policy entropy:           0.1978
                Pseudo loss:              0.28062
                Total gradient norm:      0.04864
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.562
                
Iteration (20400/50001) took 0.088 seconds.
                Mean final reward:        10.0000
                Mean return:              8.0000
                Policy entropy:           0.1137
                Pseudo loss:              0.41698
                Total gradient norm:      0.09189
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.000
                
Iteration (20500/50001) took 0.098 seconds.
                Mean final reward:        10.0000
                Mean return:              7.8125
                Policy entropy:           0.1604
                Pseudo loss:              0.42205
                Total gradient norm:      0.06448
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.188
                
Iteration (20600/50001) took 0.106 seconds.
                Mean final reward:        10.0000
                Mean return:              7.5625
                Policy entropy:           0.1848
                Pseudo loss:              0.28390
                Total gradient norm:      0.06676
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.438
                
Iteration (20700/50001) took 0.067 seconds.
                Mean final reward:        10.0000
                Mean return:              7.4062
                Policy entropy:           0.1698
                Pseudo loss:              0.57124
                Total gradient norm:      0.08866
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.594
                
Iteration (20800/50001) took 0.094 seconds.
                Mean final reward:        10.0000
                Mean return:              7.3750
                Policy entropy:           0.1695
                Pseudo loss:              0.28925
                Total gradient norm:      0.04961
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.625
                
Iteration (20900/50001) took 0.081 seconds.
                Mean final reward:        10.0000
                Mean return:              7.4062
                Policy entropy:           0.1395
                Pseudo loss:              0.31392
                Total gradient norm:      0.04842
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.594
                
Iteration (21000/50001) took 0.059 seconds.
                Mean final reward:        10.0000
                Mean return:              7.4062
                Policy entropy:           0.1837
                Pseudo loss:              0.35788
                Total gradient norm:      0.05133
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.594
                
Iteration (21100/50001) took 0.063 seconds.
                Mean final reward:        10.0000
                Mean return:              7.5000
                Policy entropy:           0.1561
                Pseudo loss:              0.29722
                Total gradient norm:      0.04480
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.500
                
Iteration (21200/50001) took 0.053 seconds.
                Mean final reward:        10.0000
                Mean return:              7.4688
                Policy entropy:           0.1465
                Pseudo loss:              0.32266
                Total gradient norm:      0.05526
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.531
                
Iteration (21300/50001) took 0.128 seconds.
                Mean final reward:        10.0000
                Mean return:              7.1562
                Policy entropy:           0.1462
                Pseudo loss:              0.44640
                Total gradient norm:      0.07503
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.844
                
Iteration (21400/50001) took 0.122 seconds.
                Mean final reward:        10.0000
                Mean return:              7.3750
                Policy entropy:           0.1387
                Pseudo loss:              0.31515
                Total gradient norm:      0.04971
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.625
                
Iteration (21500/50001) took 0.068 seconds.
                Mean final reward:        10.0000
                Mean return:              7.3438
                Policy entropy:           0.1980
                Pseudo loss:              0.42870
                Total gradient norm:      0.07562
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.656
                
Iteration (21600/50001) took 0.114 seconds.
                Mean final reward:        10.0000
                Mean return:              7.4375
                Policy entropy:           0.1473
                Pseudo loss:              0.33604
                Total gradient norm:      0.05450
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.562
                
Iteration (21700/50001) took 0.127 seconds.
                Mean final reward:        10.0000
                Mean return:              7.5625
                Policy entropy:           0.1727
                Pseudo loss:              0.20487
                Total gradient norm:      0.04850
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.438
                
Iteration (21800/50001) took 0.077 seconds.
                Mean final reward:        10.0000
                Mean return:              7.4062
                Policy entropy:           0.1748
                Pseudo loss:              0.34963
                Total gradient norm:      0.05819
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.594
                
Iteration (21900/50001) took 0.140 seconds.
                Mean final reward:        10.0000
                Mean return:              7.5938
                Policy entropy:           0.1387
                Pseudo loss:              0.21969
                Total gradient norm:      0.04141
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.406
                
Iteration (22000/50001) took 0.113 seconds.
                Mean final reward:        10.0000
                Mean return:              7.8125
                Policy entropy:           0.1718
                Pseudo loss:              0.29579
                Total gradient norm:      0.06737
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.188
                
Iteration (22100/50001) took 0.089 seconds.
                Mean final reward:        10.0000
                Mean return:              7.3438
                Policy entropy:           0.1726
                Pseudo loss:              0.50547
                Total gradient norm:      0.11165
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.656
                
Iteration (22200/50001) took 0.117 seconds.
                Mean final reward:        10.0000
                Mean return:              7.7812
                Policy entropy:           0.1457
                Pseudo loss:              0.37678
                Total gradient norm:      0.07039
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.219
                
Iteration (22300/50001) took 0.162 seconds.
                Mean final reward:        10.0000
                Mean return:              7.5938
                Policy entropy:           0.1616
                Pseudo loss:              0.43870
                Total gradient norm:      0.08420
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.406
                
Iteration (22400/50001) took 0.087 seconds.
                Mean final reward:        10.0000
                Mean return:              7.6250
                Policy entropy:           0.1480
                Pseudo loss:              0.17447
                Total gradient norm:      0.05902
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.375
                
Iteration (22500/50001) took 0.118 seconds.
                Mean final reward:        10.0000
                Mean return:              7.6250
                Policy entropy:           0.1713
                Pseudo loss:              0.40846
                Total gradient norm:      0.05444
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.375
                
Iteration (22600/50001) took 0.096 seconds.
                Mean final reward:        10.0000
                Mean return:              7.7812
                Policy entropy:           0.1855
                Pseudo loss:              0.25809
                Total gradient norm:      0.03932
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.219
                
Iteration (22700/50001) took 0.067 seconds.
                Mean final reward:        10.0000
                Mean return:              7.7812
                Policy entropy:           0.1203
                Pseudo loss:              0.18676
                Total gradient norm:      0.04989
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.219
                
Iteration (22800/50001) took 0.108 seconds.
                Mean final reward:        10.0000
                Mean return:              7.5312
                Policy entropy:           0.1913
                Pseudo loss:              0.56209
                Total gradient norm:      0.05849
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.469
                
Iteration (22900/50001) took 0.107 seconds.
                Mean final reward:        10.0000
                Mean return:              7.6250
                Policy entropy:           0.1182
                Pseudo loss:              0.44514
                Total gradient norm:      0.05371
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.375
                
Iteration (23000/50001) took 0.093 seconds.
                Mean final reward:        10.0000
                Mean return:              8.0312
                Policy entropy:           0.1198
                Pseudo loss:              0.38500
                Total gradient norm:      0.07313
                Solved trajectories:      32 / 32
                Avg steps to solve:       2.969
                
Iteration (23100/50001) took 0.104 seconds.
                Mean final reward:        10.0000
                Mean return:              8.1562
                Policy entropy:           0.0875
                Pseudo loss:              0.07647
                Total gradient norm:      0.03084
                Solved trajectories:      32 / 32
                Avg steps to solve:       2.844
                
Iteration (23200/50001) took 0.130 seconds.
                Mean final reward:        10.0000
                Mean return:              7.0625
                Policy entropy:           0.2059
                Pseudo loss:              0.38516
                Total gradient norm:      0.04841
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.938
                
Iteration (23300/50001) took 0.077 seconds.
                Mean final reward:        10.0000
                Mean return:              7.9688
                Policy entropy:           0.1351
                Pseudo loss:              0.17535
                Total gradient norm:      0.05131
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.031
                
Iteration (23400/50001) took 0.068 seconds.
                Mean final reward:        10.0000
                Mean return:              7.4375
                Policy entropy:           0.1250
                Pseudo loss:              0.38005
                Total gradient norm:      0.04722
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.562
                
Iteration (23500/50001) took 0.128 seconds.
                Mean final reward:        10.0000
                Mean return:              7.5312
                Policy entropy:           0.1450
                Pseudo loss:              0.39827
                Total gradient norm:      0.05407
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.469
                
Iteration (23600/50001) took 0.082 seconds.
                Mean final reward:        10.0000
                Mean return:              7.5000
                Policy entropy:           0.1195
                Pseudo loss:              0.22463
                Total gradient norm:      0.03429
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.500
                
Iteration (23700/50001) took 0.103 seconds.
                Mean final reward:        10.0000
                Mean return:              7.5625
                Policy entropy:           0.2064
                Pseudo loss:              0.42581
                Total gradient norm:      0.05864
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.438
                
Iteration (23800/50001) took 0.068 seconds.
                Mean final reward:        10.0000
                Mean return:              7.6250
                Policy entropy:           0.1767
                Pseudo loss:              0.23938
                Total gradient norm:      0.04204
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.375
                
Iteration (23900/50001) took 0.081 seconds.
                Mean final reward:        10.0000
                Mean return:              7.6562
                Policy entropy:           0.1250
                Pseudo loss:              0.33037
                Total gradient norm:      0.05342
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.344
                
Iteration (24000/50001) took 0.063 seconds.
                Mean final reward:        10.0000
                Mean return:              7.8750
                Policy entropy:           0.1459
                Pseudo loss:              0.43075
                Total gradient norm:      0.07538
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.125
                
Iteration (24100/50001) took 0.067 seconds.
                Mean final reward:        10.0000
                Mean return:              7.7188
                Policy entropy:           0.1553
                Pseudo loss:              0.20910
                Total gradient norm:      0.04916
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.281
                
Iteration (24200/50001) took 0.047 seconds.
                Mean final reward:        10.0000
                Mean return:              7.9375
                Policy entropy:           0.1482
                Pseudo loss:              0.30101
                Total gradient norm:      0.07590
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.062
                
Iteration (24300/50001) took 0.068 seconds.
                Mean final reward:        10.0000
                Mean return:              7.3438
                Policy entropy:           0.1743
                Pseudo loss:              0.61912
                Total gradient norm:      0.10075
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.656
                
Iteration (24400/50001) took 0.106 seconds.
                Mean final reward:        10.0000
                Mean return:              7.7188
                Policy entropy:           0.1155
                Pseudo loss:              0.32229
                Total gradient norm:      0.05636
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.281
                
Iteration (24500/50001) took 0.043 seconds.
                Mean final reward:        10.0000
                Mean return:              7.7188
                Policy entropy:           0.1485
                Pseudo loss:              0.40847
                Total gradient norm:      0.05208
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.281
                
Iteration (24600/50001) took 0.073 seconds.
                Mean final reward:        10.0000
                Mean return:              7.6250
                Policy entropy:           0.1565
                Pseudo loss:              0.28394
                Total gradient norm:      0.05097
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.375
                
Iteration (24700/50001) took 0.108 seconds.
                Mean final reward:        10.0000
                Mean return:              7.6250
                Policy entropy:           0.1598
                Pseudo loss:              0.41618
                Total gradient norm:      0.04572
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.375
                
Iteration (24800/50001) took 0.100 seconds.
                Mean final reward:        10.0000
                Mean return:              7.9688
                Policy entropy:           0.1130
                Pseudo loss:              0.36521
                Total gradient norm:      0.05331
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.031
                
Iteration (24900/50001) took 0.069 seconds.
                Mean final reward:        10.0000
                Mean return:              7.5312
                Policy entropy:           0.1889
                Pseudo loss:              0.27739
                Total gradient norm:      0.05491
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.469
                
Iteration (25000/50001) took 0.066 seconds.
                Mean final reward:        10.0000
                Mean return:              7.3750
                Policy entropy:           0.0975
                Pseudo loss:              0.18687
                Total gradient norm:      0.03943
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.625
                
Iteration (25100/50001) took 0.085 seconds.
                Mean final reward:        10.0000
                Mean return:              7.4375
                Policy entropy:           0.1366
                Pseudo loss:              0.43019
                Total gradient norm:      0.06128
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.562
                
Iteration (25200/50001) took 0.064 seconds.
                Mean final reward:        10.0000
                Mean return:              7.3750
                Policy entropy:           0.1483
                Pseudo loss:              0.46832
                Total gradient norm:      0.06339
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.625
                
Iteration (25300/50001) took 0.074 seconds.
                Mean final reward:        10.0000
                Mean return:              7.4062
                Policy entropy:           0.2091
                Pseudo loss:              0.45319
                Total gradient norm:      0.07669
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.594
                
Iteration (25400/50001) took 0.111 seconds.
                Mean final reward:        10.0000
                Mean return:              7.5938
                Policy entropy:           0.1417
                Pseudo loss:              0.14408
                Total gradient norm:      0.03565
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.406
                
Iteration (25500/50001) took 0.080 seconds.
                Mean final reward:        10.0000
                Mean return:              7.6562
                Policy entropy:           0.1319
                Pseudo loss:              0.19253
                Total gradient norm:      0.03004
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.344
                
Iteration (25600/50001) took 0.065 seconds.
                Mean final reward:        10.0000
                Mean return:              7.4688
                Policy entropy:           0.1363
                Pseudo loss:              0.37186
                Total gradient norm:      0.05869
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.531
                
Iteration (25700/50001) took 0.099 seconds.
                Mean final reward:        10.0000
                Mean return:              7.6562
                Policy entropy:           0.1170
                Pseudo loss:              0.14069
                Total gradient norm:      0.03243
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.344
                
Iteration (25800/50001) took 0.070 seconds.
                Mean final reward:        10.0000
                Mean return:              7.9062
                Policy entropy:           0.1199
                Pseudo loss:              0.14622
                Total gradient norm:      0.02946
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.094
                
Iteration (25900/50001) took 0.067 seconds.
                Mean final reward:        10.0000
                Mean return:              7.3438
                Policy entropy:           0.1308
                Pseudo loss:              0.27465
                Total gradient norm:      0.05839
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.656
                
Iteration (26000/50001) took 0.113 seconds.
                Mean final reward:        10.0000
                Mean return:              7.5000
                Policy entropy:           0.1502
                Pseudo loss:              0.49140
                Total gradient norm:      0.05986
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.500
                
Iteration (26100/50001) took 0.056 seconds.
                Mean final reward:        10.0000
                Mean return:              7.6562
                Policy entropy:           0.1435
                Pseudo loss:              0.38960
                Total gradient norm:      0.05889
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.344
                
Iteration (26200/50001) took 0.047 seconds.
                Mean final reward:        10.0000
                Mean return:              7.8438
                Policy entropy:           0.1330
                Pseudo loss:              0.20075
                Total gradient norm:      0.04369
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.156
                
Iteration (26300/50001) took 0.043 seconds.
                Mean final reward:        10.0000
                Mean return:              7.3125
                Policy entropy:           0.1418
                Pseudo loss:              0.22472
                Total gradient norm:      0.03943
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.688
                
Iteration (26400/50001) took 0.088 seconds.
                Mean final reward:        10.0000
                Mean return:              7.4375
                Policy entropy:           0.1324
                Pseudo loss:              0.29729
                Total gradient norm:      0.05585
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.562
                
Iteration (26500/50001) took 0.069 seconds.
                Mean final reward:        10.0000
                Mean return:              7.5625
                Policy entropy:           0.1140
                Pseudo loss:              0.21969
                Total gradient norm:      0.05619
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.438
                
Iteration (26600/50001) took 0.060 seconds.
                Mean final reward:        10.0000
                Mean return:              7.9375
                Policy entropy:           0.1692
                Pseudo loss:              0.42598
                Total gradient norm:      0.08611
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.062
                
Iteration (26700/50001) took 0.070 seconds.
                Mean final reward:        10.0000
                Mean return:              7.4062
                Policy entropy:           0.1533
                Pseudo loss:              0.42547
                Total gradient norm:      0.06546
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.594
                
Iteration (26800/50001) took 0.063 seconds.
                Mean final reward:        10.0000
                Mean return:              7.6250
                Policy entropy:           0.1193
                Pseudo loss:              0.25458
                Total gradient norm:      0.03783
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.375
                
Iteration (26900/50001) took 0.073 seconds.
                Mean final reward:        10.0000
                Mean return:              7.9375
                Policy entropy:           0.1341
                Pseudo loss:              0.20298
                Total gradient norm:      0.04882
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.062
                
Iteration (27000/50001) took 0.049 seconds.
                Mean final reward:        10.0000
                Mean return:              7.5625
                Policy entropy:           0.1594
                Pseudo loss:              0.42559
                Total gradient norm:      0.04393
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.438
                
Iteration (27100/50001) took 0.085 seconds.
                Mean final reward:        10.0000
                Mean return:              7.3750
                Policy entropy:           0.1496
                Pseudo loss:              0.42682
                Total gradient norm:      0.05792
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.625
                
Iteration (27200/50001) took 0.083 seconds.
                Mean final reward:        10.0000
                Mean return:              7.7188
                Policy entropy:           0.1285
                Pseudo loss:              0.30958
                Total gradient norm:      0.04288
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.281
                
Iteration (27300/50001) took 0.064 seconds.
                Mean final reward:        10.0000
                Mean return:              7.3125
                Policy entropy:           0.1543
                Pseudo loss:              0.30593
                Total gradient norm:      0.05746
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.688
                
Iteration (27400/50001) took 0.071 seconds.
                Mean final reward:        10.0000
                Mean return:              7.0938
                Policy entropy:           0.1810
                Pseudo loss:              0.48518
                Total gradient norm:      0.05023
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.906
                
Iteration (27500/50001) took 0.136 seconds.
                Mean final reward:        10.0000
                Mean return:              7.4375
                Policy entropy:           0.1939
                Pseudo loss:              0.40841
                Total gradient norm:      0.07810
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.562
                
Iteration (27600/50001) took 0.095 seconds.
                Mean final reward:        10.0000
                Mean return:              7.3438
                Policy entropy:           0.0884
                Pseudo loss:              0.12079
                Total gradient norm:      0.04230
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.656
                
Iteration (27700/50001) took 0.097 seconds.
                Mean final reward:        10.0000
                Mean return:              7.8125
                Policy entropy:           0.1227
                Pseudo loss:              0.22471
                Total gradient norm:      0.04522
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.188
                
Iteration (27800/50001) took 0.126 seconds.
                Mean final reward:        10.0000
                Mean return:              7.4688
                Policy entropy:           0.1392
                Pseudo loss:              0.18240
                Total gradient norm:      0.03586
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.531
                
Iteration (27900/50001) took 0.065 seconds.
                Mean final reward:        10.0000
                Mean return:              7.7812
                Policy entropy:           0.1774
                Pseudo loss:              0.38941
                Total gradient norm:      0.05081
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.219
                
Iteration (28000/50001) took 0.040 seconds.
                Mean final reward:        10.0000
                Mean return:              7.5000
                Policy entropy:           0.1472
                Pseudo loss:              0.21167
                Total gradient norm:      0.04070
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.500
                
Iteration (28100/50001) took 0.109 seconds.
                Mean final reward:        10.0000
                Mean return:              7.8750
                Policy entropy:           0.1052
                Pseudo loss:              0.23439
                Total gradient norm:      0.07710
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.125
                
Iteration (28200/50001) took 0.065 seconds.
                Mean final reward:        10.0000
                Mean return:              7.7500
                Policy entropy:           0.1151
                Pseudo loss:              0.18568
                Total gradient norm:      0.03468
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.250
                
Iteration (28300/50001) took 0.088 seconds.
                Mean final reward:        10.0000
                Mean return:              7.8438
                Policy entropy:           0.1056
                Pseudo loss:              0.28415
                Total gradient norm:      0.05801
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.156
                
Iteration (28400/50001) took 0.088 seconds.
                Mean final reward:        10.0000
                Mean return:              7.4688
                Policy entropy:           0.1204
                Pseudo loss:              0.17157
                Total gradient norm:      0.06035
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.531
                
Iteration (28500/50001) took 0.113 seconds.
                Mean final reward:        10.0000
                Mean return:              7.7188
                Policy entropy:           0.1273
                Pseudo loss:              0.29882
                Total gradient norm:      0.07546
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.281
                
Iteration (28600/50001) took 0.106 seconds.
                Mean final reward:        10.0000
                Mean return:              7.1875
                Policy entropy:           0.1535
                Pseudo loss:              0.29237
                Total gradient norm:      0.06304
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.812
                
Iteration (28700/50001) took 0.130 seconds.
                Mean final reward:        10.0000
                Mean return:              7.4688
                Policy entropy:           0.1510
                Pseudo loss:              0.40014
                Total gradient norm:      0.06595
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.531
                
Iteration (28800/50001) took 0.065 seconds.
                Mean final reward:        10.0000
                Mean return:              7.5938
                Policy entropy:           0.1661
                Pseudo loss:              0.28497
                Total gradient norm:      0.05205
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.406
                
Iteration (28900/50001) took 0.104 seconds.
                Mean final reward:        10.0000
                Mean return:              6.9375
                Policy entropy:           0.1859
                Pseudo loss:              0.72518
                Total gradient norm:      0.09120
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.062
                
Iteration (29000/50001) took 0.104 seconds.
                Mean final reward:        10.0000
                Mean return:              7.4375
                Policy entropy:           0.1451
                Pseudo loss:              0.40118
                Total gradient norm:      0.05562
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.562
                
Iteration (29100/50001) took 0.091 seconds.
                Mean final reward:        10.0000
                Mean return:              7.7500
                Policy entropy:           0.1024
                Pseudo loss:              0.15475
                Total gradient norm:      0.04440
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.250
                
Iteration (29200/50001) took 0.061 seconds.
                Mean final reward:        10.0000
                Mean return:              7.8125
                Policy entropy:           0.1276
                Pseudo loss:              0.35338
                Total gradient norm:      0.05632
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.188
                
Iteration (29300/50001) took 0.060 seconds.
                Mean final reward:        10.0000
                Mean return:              7.0312
                Policy entropy:           0.1097
                Pseudo loss:              0.16669
                Total gradient norm:      0.04271
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.969
                
Iteration (29400/50001) took 0.056 seconds.
                Mean final reward:        10.0000
                Mean return:              7.6875
                Policy entropy:           0.1138
                Pseudo loss:              0.23818
                Total gradient norm:      0.04828
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.312
                
Iteration (29500/50001) took 0.131 seconds.
                Mean final reward:        10.0000
                Mean return:              7.2812
                Policy entropy:           0.1594
                Pseudo loss:              0.47486
                Total gradient norm:      0.05475
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.719
                
Iteration (29600/50001) took 0.133 seconds.
                Mean final reward:        10.0000
                Mean return:              7.5312
                Policy entropy:           0.1649
                Pseudo loss:              0.25848
                Total gradient norm:      0.07775
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.469
                
Iteration (29700/50001) took 0.058 seconds.
                Mean final reward:        10.0000
                Mean return:              7.6562
                Policy entropy:           0.1443
                Pseudo loss:              0.17069
                Total gradient norm:      0.05524
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.344
                
Iteration (29800/50001) took 0.090 seconds.
                Mean final reward:        10.0000
                Mean return:              7.6250
                Policy entropy:           0.1276
                Pseudo loss:              0.26878
                Total gradient norm:      0.05180
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.375
                
Iteration (29900/50001) took 0.065 seconds.
                Mean final reward:        10.0000
                Mean return:              7.3125
                Policy entropy:           0.1775
                Pseudo loss:              0.42169
                Total gradient norm:      0.04635
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.688
                
Iteration (30000/50001) took 0.099 seconds.
                Mean final reward:        10.0000
                Mean return:              7.6250
                Policy entropy:           0.1219
                Pseudo loss:              0.13354
                Total gradient norm:      0.06285
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.375
                
Iteration (30100/50001) took 0.107 seconds.
                Mean final reward:        10.0000
                Mean return:              7.2812
                Policy entropy:           0.1635
                Pseudo loss:              0.63147
                Total gradient norm:      0.06537
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.719
                
Iteration (30200/50001) took 0.077 seconds.
                Mean final reward:        10.0000
                Mean return:              7.3750
                Policy entropy:           0.1664
                Pseudo loss:              0.26535
                Total gradient norm:      0.06446
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.625
                
Iteration (30300/50001) took 0.056 seconds.
                Mean final reward:        10.0000
                Mean return:              7.4062
                Policy entropy:           0.1454
                Pseudo loss:              0.44402
                Total gradient norm:      0.07200
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.594
                
Iteration (30400/50001) took 0.081 seconds.
                Mean final reward:        10.0000
                Mean return:              7.7188
                Policy entropy:           0.1160
                Pseudo loss:              0.18348
                Total gradient norm:      0.05967
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.281
                
Iteration (30500/50001) took 0.123 seconds.
                Mean final reward:        10.0000
                Mean return:              7.7500
                Policy entropy:           0.1165
                Pseudo loss:              0.29340
                Total gradient norm:      0.06643
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.250
                
Iteration (30600/50001) took 0.064 seconds.
                Mean final reward:        10.0000
                Mean return:              7.6875
                Policy entropy:           0.1097
                Pseudo loss:              0.20156
                Total gradient norm:      0.04540
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.312
                
Iteration (30700/50001) took 0.125 seconds.
                Mean final reward:        10.0000
                Mean return:              7.9062
                Policy entropy:           0.0884
                Pseudo loss:              0.26858
                Total gradient norm:      0.04284
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.094
                
Iteration (30800/50001) took 0.061 seconds.
                Mean final reward:        10.0000
                Mean return:              7.5938
                Policy entropy:           0.1063
                Pseudo loss:              0.17866
                Total gradient norm:      0.05050
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.406
                
Iteration (30900/50001) took 0.068 seconds.
                Mean final reward:        10.0000
                Mean return:              7.7500
                Policy entropy:           0.1282
                Pseudo loss:              0.17956
                Total gradient norm:      0.05410
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.250
                
Iteration (31000/50001) took 0.082 seconds.
                Mean final reward:        10.0000
                Mean return:              7.4375
                Policy entropy:           0.1345
                Pseudo loss:              0.21383
                Total gradient norm:      0.03452
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.562
                
Iteration (31100/50001) took 0.094 seconds.
                Mean final reward:        10.0000
                Mean return:              7.4688
                Policy entropy:           0.1245
                Pseudo loss:              0.22701
                Total gradient norm:      0.03964
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.531
                
Iteration (31200/50001) took 0.085 seconds.
                Mean final reward:        10.0000
                Mean return:              7.4688
                Policy entropy:           0.1170
                Pseudo loss:              0.20178
                Total gradient norm:      0.03319
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.531
                
Iteration (31300/50001) took 0.069 seconds.
                Mean final reward:        10.0000
                Mean return:              7.8438
                Policy entropy:           0.1198
                Pseudo loss:              0.22267
                Total gradient norm:      0.08774
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.156
                
Iteration (31400/50001) took 0.056 seconds.
                Mean final reward:        10.0000
                Mean return:              7.5625
                Policy entropy:           0.1212
                Pseudo loss:              0.22710
                Total gradient norm:      0.06020
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.438
                
Iteration (31500/50001) took 0.095 seconds.
                Mean final reward:        10.0000
                Mean return:              7.6250
                Policy entropy:           0.1620
                Pseudo loss:              0.30396
                Total gradient norm:      0.05355
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.375
                
Iteration (31600/50001) took 0.038 seconds.
                Mean final reward:        10.0000
                Mean return:              7.4688
                Policy entropy:           0.1213
                Pseudo loss:              0.14266
                Total gradient norm:      0.03594
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.531
                
Iteration (31700/50001) took 0.108 seconds.
                Mean final reward:        10.0000
                Mean return:              7.5000
                Policy entropy:           0.1305
                Pseudo loss:              0.26436
                Total gradient norm:      0.06329
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.500
                
Iteration (31800/50001) took 0.089 seconds.
                Mean final reward:        10.0000
                Mean return:              7.8125
                Policy entropy:           0.1059
                Pseudo loss:              0.20991
                Total gradient norm:      0.05482
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.188
                
Iteration (31900/50001) took 0.070 seconds.
                Mean final reward:        10.0000
                Mean return:              7.1875
                Policy entropy:           0.1389
                Pseudo loss:              0.43929
                Total gradient norm:      0.07140
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.812
                
Iteration (32000/50001) took 0.111 seconds.
                Mean final reward:        10.0000
                Mean return:              7.3750
                Policy entropy:           0.1591
                Pseudo loss:              0.25452
                Total gradient norm:      0.03894
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.625
                
Iteration (32100/50001) took 0.090 seconds.
                Mean final reward:        10.0000
                Mean return:              7.9062
                Policy entropy:           0.0675
                Pseudo loss:              0.22047
                Total gradient norm:      0.06445
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.094
                
Iteration (32200/50001) took 0.062 seconds.
                Mean final reward:        10.0000
                Mean return:              7.4375
                Policy entropy:           0.1264
                Pseudo loss:              0.29644
                Total gradient norm:      0.03942
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.562
                
Iteration (32300/50001) took 0.116 seconds.
                Mean final reward:        10.0000
                Mean return:              7.8750
                Policy entropy:           0.1299
                Pseudo loss:              0.29946
                Total gradient norm:      0.06427
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.125
                
Iteration (32400/50001) took 0.098 seconds.
                Mean final reward:        10.0000
                Mean return:              7.8125
                Policy entropy:           0.1316
                Pseudo loss:              0.28546
                Total gradient norm:      0.08303
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.188
                
Iteration (32500/50001) took 0.062 seconds.
                Mean final reward:        10.0000
                Mean return:              7.5312
                Policy entropy:           0.1409
                Pseudo loss:              0.24823
                Total gradient norm:      0.03750
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.469
                
Iteration (32600/50001) took 0.058 seconds.
                Mean final reward:        10.0000
                Mean return:              7.2812
                Policy entropy:           0.1208
                Pseudo loss:              0.40325
                Total gradient norm:      0.08187
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.719
                
Iteration (32700/50001) took 0.150 seconds.
                Mean final reward:        10.0000
                Mean return:              7.4375
                Policy entropy:           0.1306
                Pseudo loss:              0.36398
                Total gradient norm:      0.08271
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.562
                
Iteration (32800/50001) took 0.074 seconds.
                Mean final reward:        10.0000
                Mean return:              7.3750
                Policy entropy:           0.1278
                Pseudo loss:              0.26270
                Total gradient norm:      0.03830
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.625
                
Iteration (32900/50001) took 0.088 seconds.
                Mean final reward:        10.0000
                Mean return:              7.7188
                Policy entropy:           0.1278
                Pseudo loss:              0.23441
                Total gradient norm:      0.03659
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.281
                
Iteration (33000/50001) took 0.108 seconds.
                Mean final reward:        10.0000
                Mean return:              7.2188
                Policy entropy:           0.1422
                Pseudo loss:              0.50788
                Total gradient norm:      0.07923
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.781
                
Iteration (33100/50001) took 0.107 seconds.
                Mean final reward:        10.0000
                Mean return:              8.1250
                Policy entropy:           0.0749
                Pseudo loss:              0.07933
                Total gradient norm:      0.01655
                Solved trajectories:      32 / 32
                Avg steps to solve:       2.875
                
Iteration (33200/50001) took 0.079 seconds.
                Mean final reward:        10.0000
                Mean return:              7.5000
                Policy entropy:           0.1735
                Pseudo loss:              0.30059
                Total gradient norm:      0.06111
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.500
                
Iteration (33300/50001) took 0.062 seconds.
                Mean final reward:        10.0000
                Mean return:              7.8125
                Policy entropy:           0.0854
                Pseudo loss:              0.17077
                Total gradient norm:      0.04431
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.188
                
Iteration (33400/50001) took 0.104 seconds.
                Mean final reward:        10.0000
                Mean return:              8.0312
                Policy entropy:           0.1396
                Pseudo loss:              0.19157
                Total gradient norm:      0.07092
                Solved trajectories:      32 / 32
                Avg steps to solve:       2.969
                
Iteration (33500/50001) took 0.102 seconds.
                Mean final reward:        10.0000
                Mean return:              7.1250
                Policy entropy:           0.1660
                Pseudo loss:              0.42678
                Total gradient norm:      0.07203
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.875
                
Iteration (33600/50001) took 0.106 seconds.
                Mean final reward:        10.0000
                Mean return:              7.4688
                Policy entropy:           0.1303
                Pseudo loss:              0.36518
                Total gradient norm:      0.07716
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.531
                
Iteration (33700/50001) took 0.121 seconds.
                Mean final reward:        10.0000
                Mean return:              8.0000
                Policy entropy:           0.1360
                Pseudo loss:              0.43566
                Total gradient norm:      0.06856
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.000
                
Iteration (33800/50001) took 0.105 seconds.
                Mean final reward:        10.0000
                Mean return:              7.4375
                Policy entropy:           0.1260
                Pseudo loss:              0.19762
                Total gradient norm:      0.06507
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.562
                
Iteration (33900/50001) took 0.059 seconds.
                Mean final reward:        10.0000
                Mean return:              7.3750
                Policy entropy:           0.1398
                Pseudo loss:              0.28993
                Total gradient norm:      0.06109
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.625
                
Iteration (34000/50001) took 0.063 seconds.
                Mean final reward:        10.0000
                Mean return:              7.7500
                Policy entropy:           0.1225
                Pseudo loss:              0.19401
                Total gradient norm:      0.03114
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.250
                
Iteration (34100/50001) took 0.067 seconds.
                Mean final reward:        10.0000
                Mean return:              7.1875
                Policy entropy:           0.1864
                Pseudo loss:              0.69520
                Total gradient norm:      0.06833
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.812
                
Iteration (34200/50001) took 0.067 seconds.
                Mean final reward:        10.0000
                Mean return:              7.2500
                Policy entropy:           0.1424
                Pseudo loss:              0.24952
                Total gradient norm:      0.04275
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.750
                
Iteration (34300/50001) took 0.066 seconds.
                Mean final reward:        10.0000
                Mean return:              7.3750
                Policy entropy:           0.1155
                Pseudo loss:              0.22294
                Total gradient norm:      0.06277
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.625
                
Iteration (34400/50001) took 0.086 seconds.
                Mean final reward:        10.0000
                Mean return:              7.1562
                Policy entropy:           0.1877
                Pseudo loss:              0.37537
                Total gradient norm:      0.05351
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.844
                
Iteration (34500/50001) took 0.118 seconds.
                Mean final reward:        10.0000
                Mean return:              7.1875
                Policy entropy:           0.1268
                Pseudo loss:              0.32114
                Total gradient norm:      0.05605
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.812
                
Iteration (34600/50001) took 0.064 seconds.
                Mean final reward:        10.0000
                Mean return:              7.8750
                Policy entropy:           0.1258
                Pseudo loss:              0.23024
                Total gradient norm:      0.05245
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.125
                
Iteration (34700/50001) took 0.132 seconds.
                Mean final reward:        10.0000
                Mean return:              7.3125
                Policy entropy:           0.1172
                Pseudo loss:              0.47145
                Total gradient norm:      0.07208
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.688
                
Iteration (34800/50001) took 0.082 seconds.
                Mean final reward:        10.0000
                Mean return:              7.4062
                Policy entropy:           0.1340
                Pseudo loss:              0.33323
                Total gradient norm:      0.07044
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.594
                
Iteration (34900/50001) took 0.086 seconds.
                Mean final reward:        10.0000
                Mean return:              7.3750
                Policy entropy:           0.1549
                Pseudo loss:              0.27714
                Total gradient norm:      0.03703
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.625
                
Iteration (35000/50001) took 0.093 seconds.
                Mean final reward:        10.0000
                Mean return:              7.8125
                Policy entropy:           0.1255
                Pseudo loss:              0.20615
                Total gradient norm:      0.02391
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.188
                
Iteration (35100/50001) took 0.062 seconds.
                Mean final reward:        10.0000
                Mean return:              7.4375
                Policy entropy:           0.1092
                Pseudo loss:              0.26461
                Total gradient norm:      0.03724
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.562
                
Iteration (35200/50001) took 0.082 seconds.
                Mean final reward:        10.0000
                Mean return:              7.6875
                Policy entropy:           0.1195
                Pseudo loss:              0.25901
                Total gradient norm:      0.04948
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.312
                
Iteration (35300/50001) took 0.083 seconds.
                Mean final reward:        10.0000
                Mean return:              7.7812
                Policy entropy:           0.1211
                Pseudo loss:              0.18795
                Total gradient norm:      0.04232
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.219
                
Iteration (35400/50001) took 0.118 seconds.
                Mean final reward:        10.0000
                Mean return:              7.3438
                Policy entropy:           0.1166
                Pseudo loss:              0.29239
                Total gradient norm:      0.05650
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.656
                
Iteration (35500/50001) took 0.074 seconds.
                Mean final reward:        10.0000
                Mean return:              7.7812
                Policy entropy:           0.1296
                Pseudo loss:              0.27052
                Total gradient norm:      0.06216
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.219
                
Iteration (35600/50001) took 0.097 seconds.
                Mean final reward:        10.0000
                Mean return:              7.7812
                Policy entropy:           0.0942
                Pseudo loss:              0.44088
                Total gradient norm:      0.05106
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.219
                
Iteration (35700/50001) took 0.070 seconds.
                Mean final reward:        10.0000
                Mean return:              7.6562
                Policy entropy:           0.0747
                Pseudo loss:              0.19561
                Total gradient norm:      0.03916
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.344
                
Iteration (35800/50001) took 0.057 seconds.
                Mean final reward:        10.0000
                Mean return:              7.8750
                Policy entropy:           0.1056
                Pseudo loss:              0.21440
                Total gradient norm:      0.03586
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.125
                
Iteration (35900/50001) took 0.082 seconds.
                Mean final reward:        10.0000
                Mean return:              7.5938
                Policy entropy:           0.1625
                Pseudo loss:              0.39609
                Total gradient norm:      0.08135
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.406
                
Iteration (36000/50001) took 0.050 seconds.
                Mean final reward:        10.0000
                Mean return:              7.7188
                Policy entropy:           0.1257
                Pseudo loss:              0.15348
                Total gradient norm:      0.06099
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.281
                
Iteration (36100/50001) took 0.112 seconds.
                Mean final reward:        10.0000
                Mean return:              7.6562
                Policy entropy:           0.1243
                Pseudo loss:              0.19635
                Total gradient norm:      0.03791
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.344
                
Iteration (36200/50001) took 0.085 seconds.
                Mean final reward:        10.0000
                Mean return:              7.6562
                Policy entropy:           0.0878
                Pseudo loss:              0.20824
                Total gradient norm:      0.03868
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.344
                
Iteration (36300/50001) took 0.063 seconds.
                Mean final reward:        10.0000
                Mean return:              7.7188
                Policy entropy:           0.1493
                Pseudo loss:              0.47947
                Total gradient norm:      0.05427
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.281
                
Iteration (36400/50001) took 0.070 seconds.
                Mean final reward:        10.0000
                Mean return:              7.7812
                Policy entropy:           0.1452
                Pseudo loss:              0.48108
                Total gradient norm:      0.06676
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.219
                
Iteration (36500/50001) took 0.090 seconds.
                Mean final reward:        10.0000
                Mean return:              7.4375
                Policy entropy:           0.1064
                Pseudo loss:              0.28069
                Total gradient norm:      0.04419
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.562
                
Iteration (36600/50001) took 0.131 seconds.
                Mean final reward:        10.0000
                Mean return:              7.9688
                Policy entropy:           0.0698
                Pseudo loss:              0.19931
                Total gradient norm:      0.03583
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.031
                
Iteration (36700/50001) took 0.101 seconds.
                Mean final reward:        10.0000
                Mean return:              7.2812
                Policy entropy:           0.1184
                Pseudo loss:              0.24704
                Total gradient norm:      0.04245
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.719
                
Iteration (36800/50001) took 0.136 seconds.
                Mean final reward:        10.0000
                Mean return:              7.4375
                Policy entropy:           0.1385
                Pseudo loss:              0.26891
                Total gradient norm:      0.07192
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.562
                
Iteration (36900/50001) took 0.065 seconds.
                Mean final reward:        10.0000
                Mean return:              7.7812
                Policy entropy:           0.1714
                Pseudo loss:              0.20105
                Total gradient norm:      0.04854
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.219
                
Iteration (37000/50001) took 0.091 seconds.
                Mean final reward:        10.0000
                Mean return:              7.4062
                Policy entropy:           0.1537
                Pseudo loss:              0.35209
                Total gradient norm:      0.05404
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.594
                
Iteration (37100/50001) took 0.086 seconds.
                Mean final reward:        10.0000
                Mean return:              7.5312
                Policy entropy:           0.1050
                Pseudo loss:              0.30891
                Total gradient norm:      0.07783
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.469
                
Iteration (37200/50001) took 0.105 seconds.
                Mean final reward:        10.0000
                Mean return:              7.6250
                Policy entropy:           0.1122
                Pseudo loss:              0.29969
                Total gradient norm:      0.06259
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.375
                
Iteration (37300/50001) took 0.117 seconds.
                Mean final reward:        10.0000
                Mean return:              7.6562
                Policy entropy:           0.1083
                Pseudo loss:              0.12150
                Total gradient norm:      0.05412
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.344
                
Iteration (37400/50001) took 0.062 seconds.
                Mean final reward:        10.0000
                Mean return:              8.1562
                Policy entropy:           0.1027
                Pseudo loss:              0.15552
                Total gradient norm:      0.04273
                Solved trajectories:      32 / 32
                Avg steps to solve:       2.844
                
Iteration (37500/50001) took 0.103 seconds.
                Mean final reward:        10.0000
                Mean return:              7.7500
                Policy entropy:           0.1189
                Pseudo loss:              0.27931
                Total gradient norm:      0.08944
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.250
                
Iteration (37600/50001) took 0.074 seconds.
                Mean final reward:        10.0000
                Mean return:              7.4062
                Policy entropy:           0.1339
                Pseudo loss:              0.34811
                Total gradient norm:      0.06121
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.594
                
Iteration (37700/50001) took 0.103 seconds.
                Mean final reward:        10.0000
                Mean return:              7.6562
                Policy entropy:           0.1015
                Pseudo loss:              0.11626
                Total gradient norm:      0.02500
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.344
                
Iteration (37800/50001) took 0.062 seconds.
                Mean final reward:        10.0000
                Mean return:              7.5000
                Policy entropy:           0.1302
                Pseudo loss:              0.15553
                Total gradient norm:      0.04593
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.500
                
Iteration (37900/50001) took 0.108 seconds.
                Mean final reward:        10.0000
                Mean return:              7.7188
                Policy entropy:           0.1381
                Pseudo loss:              0.29528
                Total gradient norm:      0.06246
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.281
                
Iteration (38000/50001) took 0.082 seconds.
                Mean final reward:        10.0000
                Mean return:              7.5938
                Policy entropy:           0.1381
                Pseudo loss:              0.34144
                Total gradient norm:      0.06623
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.406
                
Iteration (38100/50001) took 0.137 seconds.
                Mean final reward:        10.0000
                Mean return:              7.6250
                Policy entropy:           0.1094
                Pseudo loss:              0.13664
                Total gradient norm:      0.04660
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.375
                
Iteration (38200/50001) took 0.120 seconds.
                Mean final reward:        10.0000
                Mean return:              8.0938
                Policy entropy:           0.1327
                Pseudo loss:              0.28189
                Total gradient norm:      0.04384
                Solved trajectories:      32 / 32
                Avg steps to solve:       2.906
                
Iteration (38300/50001) took 0.060 seconds.
                Mean final reward:        10.0000
                Mean return:              7.5625
                Policy entropy:           0.1246
                Pseudo loss:              0.21557
                Total gradient norm:      0.04227
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.438
                
Iteration (38400/50001) took 0.123 seconds.
                Mean final reward:        10.0000
                Mean return:              7.6562
                Policy entropy:           0.0860
                Pseudo loss:              0.18274
                Total gradient norm:      0.04235
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.344
                
Iteration (38500/50001) took 0.052 seconds.
                Mean final reward:        10.0000
                Mean return:              7.5000
                Policy entropy:           0.1106
                Pseudo loss:              0.24311
                Total gradient norm:      0.06918
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.500
                
Iteration (38600/50001) took 0.117 seconds.
                Mean final reward:        10.0000
                Mean return:              7.4688
                Policy entropy:           0.1437
                Pseudo loss:              0.28041
                Total gradient norm:      0.06079
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.531
                
Iteration (38700/50001) took 0.073 seconds.
                Mean final reward:        10.0000
                Mean return:              7.3438
                Policy entropy:           0.1124
                Pseudo loss:              0.15915
                Total gradient norm:      0.03027
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.656
                
Iteration (38800/50001) took 0.121 seconds.
                Mean final reward:        10.0000
                Mean return:              7.9062
                Policy entropy:           0.0895
                Pseudo loss:              0.37664
                Total gradient norm:      0.05501
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.094
                
Iteration (38900/50001) took 0.080 seconds.
                Mean final reward:        10.0000
                Mean return:              7.8438
                Policy entropy:           0.1142
                Pseudo loss:              0.26402
                Total gradient norm:      0.04406
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.156
                
Iteration (39000/50001) took 0.126 seconds.
                Mean final reward:        10.0000
                Mean return:              7.4062
                Policy entropy:           0.1354
                Pseudo loss:              0.29172
                Total gradient norm:      0.07224
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.594
                
Iteration (39100/50001) took 0.089 seconds.
                Mean final reward:        10.0000
                Mean return:              7.8438
                Policy entropy:           0.0802
                Pseudo loss:              0.14142
                Total gradient norm:      0.03669
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.156
                
Iteration (39200/50001) took 0.088 seconds.
                Mean final reward:        10.0000
                Mean return:              7.9688
                Policy entropy:           0.0676
                Pseudo loss:              0.16643
                Total gradient norm:      0.04078
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.031
                
Iteration (39300/50001) took 0.088 seconds.
                Mean final reward:        10.0000
                Mean return:              7.5625
                Policy entropy:           0.1651
                Pseudo loss:              0.29740
                Total gradient norm:      0.05957
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.438
                
Iteration (39400/50001) took 0.055 seconds.
                Mean final reward:        10.0000
                Mean return:              7.7812
                Policy entropy:           0.0956
                Pseudo loss:              0.46086
                Total gradient norm:      0.05477
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.219
                
Iteration (39500/50001) took 0.029 seconds.
                Mean final reward:        10.0000
                Mean return:              8.0312
                Policy entropy:           0.1318
                Pseudo loss:              0.11207
                Total gradient norm:      0.07036
                Solved trajectories:      32 / 32
                Avg steps to solve:       2.969
                
Iteration (39600/50001) took 0.063 seconds.
                Mean final reward:        10.0000
                Mean return:              7.6562
                Policy entropy:           0.1311
                Pseudo loss:              0.21290
                Total gradient norm:      0.03758
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.344
                
Iteration (39700/50001) took 0.054 seconds.
                Mean final reward:        10.0000
                Mean return:              7.5938
                Policy entropy:           0.1149
                Pseudo loss:              0.19686
                Total gradient norm:      0.07487
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.406
                
Iteration (39800/50001) took 0.055 seconds.
                Mean final reward:        10.0000
                Mean return:              7.6250
                Policy entropy:           0.1072
                Pseudo loss:              0.17833
                Total gradient norm:      0.02868
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.375
                
Iteration (39900/50001) took 0.061 seconds.
                Mean final reward:        10.0000
                Mean return:              7.6875
                Policy entropy:           0.1255
                Pseudo loss:              0.48198
                Total gradient norm:      0.12530
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.312
                
Iteration (40000/50001) took 0.110 seconds.
                Mean final reward:        10.0000
                Mean return:              7.7500
                Policy entropy:           0.1428
                Pseudo loss:              0.38252
                Total gradient norm:      0.05035
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.250
                
Iteration (40100/50001) took 0.091 seconds.
                Mean final reward:        10.0000
                Mean return:              7.6562
                Policy entropy:           0.1286
                Pseudo loss:              0.32425
                Total gradient norm:      0.04617
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.344
                
Iteration (40200/50001) took 0.063 seconds.
                Mean final reward:        10.0000
                Mean return:              7.4688
                Policy entropy:           0.0874
                Pseudo loss:              0.18538
                Total gradient norm:      0.04626
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.531
                
Iteration (40300/50001) took 0.062 seconds.
                Mean final reward:        10.0000
                Mean return:              7.5625
                Policy entropy:           0.1364
                Pseudo loss:              0.22957
                Total gradient norm:      0.02980
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.438
                
Iteration (40400/50001) took 0.076 seconds.
                Mean final reward:        10.0000
                Mean return:              7.5000
                Policy entropy:           0.1484
                Pseudo loss:              0.35065
                Total gradient norm:      0.07510
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.500
                
Iteration (40500/50001) took 0.095 seconds.
                Mean final reward:        10.0000
                Mean return:              7.3125
                Policy entropy:           0.1462
                Pseudo loss:              0.28379
                Total gradient norm:      0.05270
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.688
                
Iteration (40600/50001) took 0.099 seconds.
                Mean final reward:        10.0000
                Mean return:              7.3438
                Policy entropy:           0.1366
                Pseudo loss:              0.29470
                Total gradient norm:      0.06724
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.656
                
Iteration (40700/50001) took 0.079 seconds.
                Mean final reward:        10.0000
                Mean return:              7.4688
                Policy entropy:           0.1262
                Pseudo loss:              0.32088
                Total gradient norm:      0.08426
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.531
                
Iteration (40800/50001) took 0.060 seconds.
                Mean final reward:        10.0000
                Mean return:              7.6562
                Policy entropy:           0.0715
                Pseudo loss:              0.10659
                Total gradient norm:      0.03655
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.344
                
Iteration (40900/50001) took 0.065 seconds.
                Mean final reward:        10.0000
                Mean return:              7.4375
                Policy entropy:           0.1553
                Pseudo loss:              0.39983
                Total gradient norm:      0.06836
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.562
                
Iteration (41000/50001) took 0.057 seconds.
                Mean final reward:        10.0000
                Mean return:              7.7812
                Policy entropy:           0.0986
                Pseudo loss:              0.15678
                Total gradient norm:      0.03087
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.219
                
Iteration (41100/50001) took 0.061 seconds.
                Mean final reward:        10.0000
                Mean return:              7.6875
                Policy entropy:           0.0909
                Pseudo loss:              0.17084
                Total gradient norm:      0.03636
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.312
                
Iteration (41200/50001) took 0.058 seconds.
                Mean final reward:        10.0000
                Mean return:              7.7812
                Policy entropy:           0.0931
                Pseudo loss:              0.25458
                Total gradient norm:      0.06015
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.219
                
Iteration (41300/50001) took 0.051 seconds.
                Mean final reward:        10.0000
                Mean return:              7.7188
                Policy entropy:           0.0854
                Pseudo loss:              0.12744
                Total gradient norm:      0.04913
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.281
                
Iteration (41400/50001) took 0.089 seconds.
                Mean final reward:        10.0000
                Mean return:              7.4375
                Policy entropy:           0.0806
                Pseudo loss:              0.19935
                Total gradient norm:      0.04269
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.562
                
Iteration (41500/50001) took 0.067 seconds.
                Mean final reward:        10.0000
                Mean return:              7.7500
                Policy entropy:           0.1094
                Pseudo loss:              0.13233
                Total gradient norm:      0.03957
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.250
                
Iteration (41600/50001) took 0.060 seconds.
                Mean final reward:        10.0000
                Mean return:              7.2500
                Policy entropy:           0.1100
                Pseudo loss:              0.19899
                Total gradient norm:      0.04684
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.750
                
Iteration (41700/50001) took 0.118 seconds.
                Mean final reward:        10.0000
                Mean return:              7.5000
                Policy entropy:           0.1108
                Pseudo loss:              0.24052
                Total gradient norm:      0.06548
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.500
                
Iteration (41800/50001) took 0.084 seconds.
                Mean final reward:        10.0000
                Mean return:              7.5938
                Policy entropy:           0.1285
                Pseudo loss:              0.30578
                Total gradient norm:      0.04177
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.406
                
Iteration (41900/50001) took 0.068 seconds.
                Mean final reward:        10.0000
                Mean return:              7.4688
                Policy entropy:           0.1153
                Pseudo loss:              0.23503
                Total gradient norm:      0.03372
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.531
                
Iteration (42000/50001) took 0.063 seconds.
                Mean final reward:        10.0000
                Mean return:              7.5625
                Policy entropy:           0.1147
                Pseudo loss:              0.13567
                Total gradient norm:      0.02812
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.438
                
Iteration (42100/50001) took 0.056 seconds.
                Mean final reward:        10.0000
                Mean return:              8.0000
                Policy entropy:           0.0770
                Pseudo loss:              0.09138
                Total gradient norm:      0.01839
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.000
                
Iteration (42200/50001) took 0.083 seconds.
                Mean final reward:        10.0000
                Mean return:              7.7188
                Policy entropy:           0.1004
                Pseudo loss:              0.14233
                Total gradient norm:      0.04511
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.281
                
Iteration (42300/50001) took 0.070 seconds.
                Mean final reward:        10.0000
                Mean return:              7.6875
                Policy entropy:           0.1176
                Pseudo loss:              0.18688
                Total gradient norm:      0.03195
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.312
                
Iteration (42400/50001) took 0.037 seconds.
                Mean final reward:        10.0000
                Mean return:              7.8438
                Policy entropy:           0.0988
                Pseudo loss:              0.25846
                Total gradient norm:      0.05473
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.156
                
Iteration (42500/50001) took 0.062 seconds.
                Mean final reward:        10.0000
                Mean return:              7.8750
                Policy entropy:           0.0736
                Pseudo loss:              0.08305
                Total gradient norm:      0.01775
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.125
                
Iteration (42600/50001) took 0.064 seconds.
                Mean final reward:        10.0000
                Mean return:              7.1562
                Policy entropy:           0.1933
                Pseudo loss:              0.41574
                Total gradient norm:      0.05478
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.844
                
Iteration (42700/50001) took 0.065 seconds.
                Mean final reward:        10.0000
                Mean return:              7.5625
                Policy entropy:           0.1114
                Pseudo loss:              0.32042
                Total gradient norm:      0.07427
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.438
                
Iteration (42800/50001) took 0.065 seconds.
                Mean final reward:        10.0000
                Mean return:              7.2812
                Policy entropy:           0.1033
                Pseudo loss:              0.44818
                Total gradient norm:      0.09200
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.719
                
Iteration (42900/50001) took 0.033 seconds.
                Mean final reward:        10.0000
                Mean return:              7.5000
                Policy entropy:           0.0906
                Pseudo loss:              0.15940
                Total gradient norm:      0.02990
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.500
                
Iteration (43000/50001) took 0.060 seconds.
                Mean final reward:        10.0000
                Mean return:              7.8125
                Policy entropy:           0.0964
                Pseudo loss:              0.35777
                Total gradient norm:      0.07390
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.188
                
Iteration (43100/50001) took 0.031 seconds.
                Mean final reward:        10.0000
                Mean return:              7.9062
                Policy entropy:           0.0948
                Pseudo loss:              0.35025
                Total gradient norm:      0.04646
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.094
                
Iteration (43200/50001) took 0.094 seconds.
                Mean final reward:        10.0000
                Mean return:              7.6875
                Policy entropy:           0.0849
                Pseudo loss:              0.19362
                Total gradient norm:      0.05007
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.312
                
Iteration (43300/50001) took 0.078 seconds.
                Mean final reward:        10.0000
                Mean return:              7.4688
                Policy entropy:           0.1593
                Pseudo loss:              0.28485
                Total gradient norm:      0.05108
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.531
                
Iteration (43400/50001) took 0.098 seconds.
                Mean final reward:        10.0000
                Mean return:              7.5625
                Policy entropy:           0.1494
                Pseudo loss:              0.47168
                Total gradient norm:      0.06613
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.438
                
Iteration (43500/50001) took 0.056 seconds.
                Mean final reward:        10.0000
                Mean return:              7.9375
                Policy entropy:           0.0833
                Pseudo loss:              0.23810
                Total gradient norm:      0.06398
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.062
                
Iteration (43600/50001) took 0.096 seconds.
                Mean final reward:        10.0000
                Mean return:              7.6250
                Policy entropy:           0.1355
                Pseudo loss:              0.36924
                Total gradient norm:      0.09196
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.375
                
Iteration (43700/50001) took 0.112 seconds.
                Mean final reward:        10.0000
                Mean return:              7.7188
                Policy entropy:           0.0887
                Pseudo loss:              0.24943
                Total gradient norm:      0.05513
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.281
                
Iteration (43800/50001) took 0.041 seconds.
                Mean final reward:        10.0000
                Mean return:              7.4062
                Policy entropy:           0.1621
                Pseudo loss:              0.42008
                Total gradient norm:      0.07691
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.594
                
Iteration (43900/50001) took 0.047 seconds.
                Mean final reward:        10.0000
                Mean return:              7.9375
                Policy entropy:           0.1175
                Pseudo loss:              0.15333
                Total gradient norm:      0.03533
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.062
                
Iteration (44000/50001) took 0.060 seconds.
                Mean final reward:        10.0000
                Mean return:              7.6250
                Policy entropy:           0.1318
                Pseudo loss:              0.12640
                Total gradient norm:      0.06215
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.375
                
Iteration (44100/50001) took 0.063 seconds.
                Mean final reward:        10.0000
                Mean return:              7.6562
                Policy entropy:           0.1282
                Pseudo loss:              0.24578
                Total gradient norm:      0.04399
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.344
                
Iteration (44200/50001) took 0.037 seconds.
                Mean final reward:        10.0000
                Mean return:              7.4062
                Policy entropy:           0.1472
                Pseudo loss:              0.25596
                Total gradient norm:      0.06763
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.594
                
Iteration (44300/50001) took 0.061 seconds.
                Mean final reward:        10.0000
                Mean return:              7.7188
                Policy entropy:           0.1132
                Pseudo loss:              0.21003
                Total gradient norm:      0.06418
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.281
                
Iteration (44400/50001) took 0.064 seconds.
                Mean final reward:        10.0000
                Mean return:              7.8125
                Policy entropy:           0.1012
                Pseudo loss:              0.16229
                Total gradient norm:      0.04482
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.188
                
Iteration (44500/50001) took 0.060 seconds.
                Mean final reward:        10.0000
                Mean return:              7.5312
                Policy entropy:           0.1477
                Pseudo loss:              0.42155
                Total gradient norm:      0.06424
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.469
                
Iteration (44600/50001) took 0.062 seconds.
                Mean final reward:        10.0000
                Mean return:              8.0938
                Policy entropy:           0.0868
                Pseudo loss:              0.09958
                Total gradient norm:      0.04805
                Solved trajectories:      32 / 32
                Avg steps to solve:       2.906
                
Iteration (44700/50001) took 0.082 seconds.
                Mean final reward:        10.0000
                Mean return:              7.4062
                Policy entropy:           0.1543
                Pseudo loss:              0.29457
                Total gradient norm:      0.06974
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.594
                
Iteration (44800/50001) took 0.111 seconds.
                Mean final reward:        10.0000
                Mean return:              7.6250
                Policy entropy:           0.1165
                Pseudo loss:              0.19142
                Total gradient norm:      0.02960
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.375
                
Iteration (44900/50001) took 0.106 seconds.
                Mean final reward:        10.0000
                Mean return:              7.6250
                Policy entropy:           0.1105
                Pseudo loss:              0.42893
                Total gradient norm:      0.08070
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.375
                
Iteration (45000/50001) took 0.066 seconds.
                Mean final reward:        10.0000
                Mean return:              7.2188
                Policy entropy:           0.1705
                Pseudo loss:              0.39517
                Total gradient norm:      0.06171
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.781
                
Iteration (45100/50001) took 0.058 seconds.
                Mean final reward:        10.0000
                Mean return:              7.7812
                Policy entropy:           0.0945
                Pseudo loss:              0.19337
                Total gradient norm:      0.03829
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.219
                
Iteration (45200/50001) took 0.087 seconds.
                Mean final reward:        10.0000
                Mean return:              7.8750
                Policy entropy:           0.0752
                Pseudo loss:              0.14107
                Total gradient norm:      0.02810
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.125
                
Iteration (45300/50001) took 0.058 seconds.
                Mean final reward:        10.0000
                Mean return:              8.1562
                Policy entropy:           0.0862
                Pseudo loss:              0.12867
                Total gradient norm:      0.03433
                Solved trajectories:      32 / 32
                Avg steps to solve:       2.844
                
Iteration (45400/50001) took 0.067 seconds.
                Mean final reward:        10.0000
                Mean return:              7.5000
                Policy entropy:           0.1169
                Pseudo loss:              0.33972
                Total gradient norm:      0.06061
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.500
                
Iteration (45500/50001) took 0.062 seconds.
                Mean final reward:        10.0000
                Mean return:              7.4062
                Policy entropy:           0.1245
                Pseudo loss:              0.19312
                Total gradient norm:      0.07574
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.594
                
Iteration (45600/50001) took 0.060 seconds.
                Mean final reward:        10.0000
                Mean return:              7.7500
                Policy entropy:           0.1063
                Pseudo loss:              0.24495
                Total gradient norm:      0.05383
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.250
                
Iteration (45700/50001) took 0.058 seconds.
                Mean final reward:        10.0000
                Mean return:              7.7500
                Policy entropy:           0.0748
                Pseudo loss:              0.13376
                Total gradient norm:      0.03536
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.250
                
Iteration (45800/50001) took 0.078 seconds.
                Mean final reward:        10.0000
                Mean return:              7.5625
                Policy entropy:           0.1449
                Pseudo loss:              0.32530
                Total gradient norm:      0.08052
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.438
                
Iteration (45900/50001) took 0.077 seconds.
                Mean final reward:        10.0000
                Mean return:              7.6562
                Policy entropy:           0.0778
                Pseudo loss:              0.14699
                Total gradient norm:      0.04434
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.344
                
Iteration (46000/50001) took 0.115 seconds.
                Mean final reward:        10.0000
                Mean return:              7.8438
                Policy entropy:           0.0717
                Pseudo loss:              0.11985
                Total gradient norm:      0.03452
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.156
                
Iteration (46100/50001) took 0.126 seconds.
                Mean final reward:        10.0000
                Mean return:              7.5000
                Policy entropy:           0.0720
                Pseudo loss:              0.14213
                Total gradient norm:      0.04598
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.500
                
Iteration (46200/50001) took 0.116 seconds.
                Mean final reward:        10.0000
                Mean return:              7.5312
                Policy entropy:           0.1146
                Pseudo loss:              0.16876
                Total gradient norm:      0.04980
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.469
                
Iteration (46300/50001) took 0.069 seconds.
                Mean final reward:        10.0000
                Mean return:              7.7188
                Policy entropy:           0.0934
                Pseudo loss:              0.21754
                Total gradient norm:      0.05244
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.281
                
Iteration (46400/50001) took 0.049 seconds.
                Mean final reward:        10.0000
                Mean return:              7.8750
                Policy entropy:           0.0937
                Pseudo loss:              0.19271
                Total gradient norm:      0.04764
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.125
                
Iteration (46500/50001) took 0.066 seconds.
                Mean final reward:        10.0000
                Mean return:              7.7188
                Policy entropy:           0.0808
                Pseudo loss:              0.20054
                Total gradient norm:      0.03773
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.281
                
Iteration (46600/50001) took 0.069 seconds.
                Mean final reward:        10.0000
                Mean return:              7.5312
                Policy entropy:           0.1390
                Pseudo loss:              0.27499
                Total gradient norm:      0.07366
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.469
                
Iteration (46700/50001) took 0.066 seconds.
                Mean final reward:        10.0000
                Mean return:              7.1875
                Policy entropy:           0.1369
                Pseudo loss:              0.45745
                Total gradient norm:      0.08935
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.812
                
Iteration (46800/50001) took 0.111 seconds.
                Mean final reward:        10.0000
                Mean return:              7.2812
                Policy entropy:           0.1100
                Pseudo loss:              0.21281
                Total gradient norm:      0.03911
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.719
                
Iteration (46900/50001) took 0.120 seconds.
                Mean final reward:        10.0000
                Mean return:              7.7500
                Policy entropy:           0.1251
                Pseudo loss:              0.39611
                Total gradient norm:      0.06423
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.250
                
Iteration (47000/50001) took 0.149 seconds.
                Mean final reward:        10.0000
                Mean return:              7.5625
                Policy entropy:           0.1167
                Pseudo loss:              0.20320
                Total gradient norm:      0.04340
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.438
                
Iteration (47100/50001) took 0.096 seconds.
                Mean final reward:        10.0000
                Mean return:              7.5312
                Policy entropy:           0.1208
                Pseudo loss:              0.20450
                Total gradient norm:      0.04526
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.469
                
Iteration (47200/50001) took 0.143 seconds.
                Mean final reward:        10.0000
                Mean return:              7.6875
                Policy entropy:           0.1333
                Pseudo loss:              0.22824
                Total gradient norm:      0.04710
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.312
                
Iteration (47300/50001) took 0.062 seconds.
                Mean final reward:        10.0000
                Mean return:              7.6562
                Policy entropy:           0.0951
                Pseudo loss:              0.22740
                Total gradient norm:      0.07362
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.344
                
Iteration (47400/50001) took 0.105 seconds.
                Mean final reward:        10.0000
                Mean return:              7.6562
                Policy entropy:           0.1661
                Pseudo loss:              0.25751
                Total gradient norm:      0.05108
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.344
                
Iteration (47500/50001) took 0.136 seconds.
                Mean final reward:        10.0000
                Mean return:              7.5938
                Policy entropy:           0.1211
                Pseudo loss:              0.40556
                Total gradient norm:      0.05529
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.406
                
Iteration (47600/50001) took 0.171 seconds.
                Mean final reward:        10.0000
                Mean return:              7.8438
                Policy entropy:           0.0716
                Pseudo loss:              0.08960
                Total gradient norm:      0.01441
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.156
                
Iteration (47700/50001) took 0.268 seconds.
                Mean final reward:        10.0000
                Mean return:              7.3750
                Policy entropy:           0.1732
                Pseudo loss:              0.49099
                Total gradient norm:      0.06571
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.625
                
Iteration (47800/50001) took 0.111 seconds.
                Mean final reward:        10.0000
                Mean return:              7.4688
                Policy entropy:           0.1198
                Pseudo loss:              0.31691
                Total gradient norm:      0.05041
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.531
                
Iteration (47900/50001) took 0.098 seconds.
                Mean final reward:        10.0000
                Mean return:              7.3125
                Policy entropy:           0.1057
                Pseudo loss:              0.37048
                Total gradient norm:      0.04965
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.688
                
Iteration (48000/50001) took 0.108 seconds.
                Mean final reward:        10.0000
                Mean return:              7.8125
                Policy entropy:           0.0607
                Pseudo loss:              0.17687
                Total gradient norm:      0.03513
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.188
                
Iteration (48100/50001) took 0.093 seconds.
                Mean final reward:        10.0000
                Mean return:              7.4688
                Policy entropy:           0.1258
                Pseudo loss:              0.28304
                Total gradient norm:      0.06152
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.531
                
Iteration (48200/50001) took 0.068 seconds.
                Mean final reward:        10.0000
                Mean return:              7.6250
                Policy entropy:           0.1479
                Pseudo loss:              0.43015
                Total gradient norm:      0.07675
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.375
                
Iteration (48300/50001) took 0.157 seconds.
                Mean final reward:        10.0000
                Mean return:              7.6250
                Policy entropy:           0.0757
                Pseudo loss:              0.12943
                Total gradient norm:      0.04493
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.375
                
Iteration (48400/50001) took 0.094 seconds.
                Mean final reward:        10.0000
                Mean return:              7.4688
                Policy entropy:           0.1179
                Pseudo loss:              0.34979
                Total gradient norm:      0.05921
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.531
                
Iteration (48500/50001) took 0.122 seconds.
                Mean final reward:        10.0000
                Mean return:              7.7500
                Policy entropy:           0.1447
                Pseudo loss:              0.52808
                Total gradient norm:      0.11573
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.250
                
Iteration (48600/50001) took 0.125 seconds.
                Mean final reward:        10.0000
                Mean return:              7.5625
                Policy entropy:           0.0973
                Pseudo loss:              0.21164
                Total gradient norm:      0.04014
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.438
                
Iteration (48700/50001) took 0.091 seconds.
                Mean final reward:        10.0000
                Mean return:              7.4375
                Policy entropy:           0.1381
                Pseudo loss:              0.36181
                Total gradient norm:      0.03891
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.562
                
Iteration (48800/50001) took 0.069 seconds.
                Mean final reward:        10.0000
                Mean return:              7.3125
                Policy entropy:           0.1186
                Pseudo loss:              0.21094
                Total gradient norm:      0.06736
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.688
                
Iteration (48900/50001) took 0.116 seconds.
                Mean final reward:        10.0000
                Mean return:              7.7188
                Policy entropy:           0.0852
                Pseudo loss:              0.13909
                Total gradient norm:      0.01953
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.281
                
Iteration (49000/50001) took 0.053 seconds.
                Mean final reward:        10.0000
                Mean return:              7.3750
                Policy entropy:           0.1188
                Pseudo loss:              0.13187
                Total gradient norm:      0.04487
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.625
                
Iteration (49100/50001) took 0.084 seconds.
                Mean final reward:        10.0000
                Mean return:              7.4062
                Policy entropy:           0.1066
                Pseudo loss:              0.16037
                Total gradient norm:      0.04393
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.594
                
Iteration (49200/50001) took 0.064 seconds.
                Mean final reward:        10.0000
                Mean return:              7.6250
                Policy entropy:           0.1250
                Pseudo loss:              0.30481
                Total gradient norm:      0.04910
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.375
                
Iteration (49300/50001) took 0.192 seconds.
                Mean final reward:        10.0000
                Mean return:              7.5625
                Policy entropy:           0.1072
                Pseudo loss:              0.26701
                Total gradient norm:      0.05745
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.438
                
Iteration (49400/50001) took 0.119 seconds.
                Mean final reward:        10.0000
                Mean return:              7.6250
                Policy entropy:           0.1186
                Pseudo loss:              0.24357
                Total gradient norm:      0.06615
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.375
                
Iteration (49500/50001) took 0.183 seconds.
                Mean final reward:        10.0000
                Mean return:              7.4375
                Policy entropy:           0.1448
                Pseudo loss:              0.41225
                Total gradient norm:      0.05598
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.562
                
Iteration (49600/50001) took 0.135 seconds.
                Mean final reward:        10.0000
                Mean return:              7.6875
                Policy entropy:           0.0912
                Pseudo loss:              0.16547
                Total gradient norm:      0.04352
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.312
                
Iteration (49700/50001) took 0.063 seconds.
                Mean final reward:        10.0000
                Mean return:              7.5938
                Policy entropy:           0.1759
                Pseudo loss:              0.41599
                Total gradient norm:      0.04944
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.406
                
Iteration (49800/50001) took 0.068 seconds.
                Mean final reward:        10.0000
                Mean return:              7.5625
                Policy entropy:           0.1124
                Pseudo loss:              0.14223
                Total gradient norm:      0.05622
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.438
                
Iteration (49900/50001) took 0.100 seconds.
                Mean final reward:        10.0000
                Mean return:              7.8438
                Policy entropy:           0.1068
                Pseudo loss:              0.09214
                Total gradient norm:      0.04591
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.156
                
Iteration (50000/50001) took 0.121 seconds.
                Mean final reward:        10.0000
                Mean return:              7.5625
                Policy entropy:           0.0764
                Pseudo loss:              0.12316
                Total gradient norm:      0.03654
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.438
                
Training took 4438.203 seconds.
