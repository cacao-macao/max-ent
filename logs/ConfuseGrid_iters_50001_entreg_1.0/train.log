Using device: cpu

Using optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)

Iteration (0/50001) took 0.140 seconds.
                Mean final reward:        0.3750
                Mean return:              -7.9062
                Policy entropy:           1.5803
                Pseudo loss:              4.44172
                Total gradient norm:      0.72631
                Solved trajectories:      3 / 32
                Avg steps to solve:       2.333
                
Iteration (100/50001) took 0.220 seconds.
                Mean final reward:        2.0938
                Mean return:              -5.2812
                Policy entropy:           1.5775
                Pseudo loss:              5.36021
                Total gradient norm:      0.77349
                Solved trajectories:      8 / 32
                Avg steps to solve:       3.500
                
Iteration (200/50001) took 0.078 seconds.
                Mean final reward:        3.8125
                Mean return:              -2.3125
                Policy entropy:           1.5760
                Pseudo loss:              5.35032
                Total gradient norm:      0.79573
                Solved trajectories:      14 / 32
                Avg steps to solve:       3.429
                
Iteration (300/50001) took 0.178 seconds.
                Mean final reward:        1.7500
                Mean return:              -5.7188
                Policy entropy:           1.5701
                Pseudo loss:              4.30278
                Total gradient norm:      0.63153
                Solved trajectories:      8 / 32
                Avg steps to solve:       3.875
                
Iteration (400/50001) took 0.144 seconds.
                Mean final reward:        3.4688
                Mean return:              -3.5938
                Policy entropy:           1.5635
                Pseudo loss:              6.31249
                Total gradient norm:      0.78888
                Solved trajectories:      13 / 32
                Avg steps to solve:       5.231
                
Iteration (500/50001) took 0.093 seconds.
                Mean final reward:        2.7812
                Mean return:              -4.2812
                Policy entropy:           1.5547
                Pseudo loss:              4.93730
                Total gradient norm:      0.62009
                Solved trajectories:      10 / 32
                Avg steps to solve:       3.800
                
Iteration (600/50001) took 0.173 seconds.
                Mean final reward:        2.0938
                Mean return:              -5.1250
                Policy entropy:           1.5492
                Pseudo loss:              3.96119
                Total gradient norm:      0.55491
                Solved trajectories:      8 / 32
                Avg steps to solve:       2.875
                
Iteration (700/50001) took 0.092 seconds.
                Mean final reward:        4.5000
                Mean return:              -1.8125
                Policy entropy:           1.5380
                Pseudo loss:              5.43317
                Total gradient norm:      0.67427
                Solved trajectories:      16 / 32
                Avg steps to solve:       4.625
                
Iteration (800/50001) took 0.101 seconds.
                Mean final reward:        5.5312
                Mean return:              -0.1875
                Policy entropy:           1.5271
                Pseudo loss:              5.91326
                Total gradient norm:      0.76117
                Solved trajectories:      19 / 32
                Avg steps to solve:       4.474
                
Iteration (900/50001) took 0.084 seconds.
                Mean final reward:        4.5000
                Mean return:              -1.5625
                Policy entropy:           1.5110
                Pseudo loss:              4.60069
                Total gradient norm:      0.61530
                Solved trajectories:      15 / 32
                Avg steps to solve:       3.733
                
Iteration (1000/50001) took 0.145 seconds.
                Mean final reward:        4.1562
                Mean return:              -2.2188
                Policy entropy:           1.5026
                Pseudo loss:              4.48074
                Total gradient norm:      0.63395
                Solved trajectories:      14 / 32
                Avg steps to solve:       4.000
                
Iteration (1100/50001) took 0.168 seconds.
                Mean final reward:        4.8438
                Mean return:              -1.9688
                Policy entropy:           1.4940
                Pseudo loss:              6.11461
                Total gradient norm:      0.69362
                Solved trajectories:      16 / 32
                Avg steps to solve:       5.625
                
Iteration (1200/50001) took 0.121 seconds.
                Mean final reward:        3.8125
                Mean return:              -2.9375
                Policy entropy:           1.4847
                Pseudo loss:              4.68931
                Total gradient norm:      0.64132
                Solved trajectories:      13 / 32
                Avg steps to solve:       4.462
                
Iteration (1300/50001) took 0.193 seconds.
                Mean final reward:        4.5000
                Mean return:              -2.0625
                Policy entropy:           1.4667
                Pseudo loss:              4.82927
                Total gradient norm:      0.57249
                Solved trajectories:      16 / 32
                Avg steps to solve:       5.125
                
Iteration (1400/50001) took 0.163 seconds.
                Mean final reward:        5.5312
                Mean return:              -0.1875
                Policy entropy:           1.4357
                Pseudo loss:              4.78660
                Total gradient norm:      0.59573
                Solved trajectories:      19 / 32
                Avg steps to solve:       4.474
                
Iteration (1500/50001) took 0.147 seconds.
                Mean final reward:        4.5000
                Mean return:              -2.0938
                Policy entropy:           1.4408
                Pseudo loss:              4.93488
                Total gradient norm:      0.59647
                Solved trajectories:      15 / 32
                Avg steps to solve:       4.867
                
Iteration (1600/50001) took 0.163 seconds.
                Mean final reward:        6.5625
                Mean return:              0.1875
                Policy entropy:           1.4111
                Pseudo loss:              6.28009
                Total gradient norm:      0.65254
                Solved trajectories:      19 / 32
                Avg steps to solve:       5.579
                
Iteration (1700/50001) took 0.181 seconds.
                Mean final reward:        6.5625
                Mean return:              1.0938
                Policy entropy:           1.3687
                Pseudo loss:              4.99882
                Total gradient norm:      0.52040
                Solved trajectories:      22 / 32
                Avg steps to solve:       4.864
                
Iteration (1800/50001) took 0.123 seconds.
                Mean final reward:        5.5312
                Mean return:              0.6562
                Policy entropy:           1.3630
                Pseudo loss:              2.69996
                Total gradient norm:      0.45423
                Solved trajectories:      19 / 32
                Avg steps to solve:       3.053
                
Iteration (1900/50001) took 0.113 seconds.
                Mean final reward:        7.5938
                Mean return:              2.5938
                Policy entropy:           1.3283
                Pseudo loss:              5.09455
                Total gradient norm:      0.50000
                Solved trajectories:      25 / 32
                Avg steps to solve:       4.880
                
Iteration (2000/50001) took 0.087 seconds.
                Mean final reward:        6.9062
                Mean return:              1.2500
                Policy entropy:           1.3243
                Pseudo loss:              5.23056
                Total gradient norm:      0.47105
                Solved trajectories:      23 / 32
                Avg steps to solve:       5.348
                
Iteration (2100/50001) took 0.183 seconds.
                Mean final reward:        6.5625
                Mean return:              1.0625
                Policy entropy:           1.3160
                Pseudo loss:              4.80009
                Total gradient norm:      0.42391
                Solved trajectories:      21 / 32
                Avg steps to solve:       4.667
                
Iteration (2200/50001) took 0.190 seconds.
                Mean final reward:        6.2188
                Mean return:              0.1562
                Policy entropy:           1.3110
                Pseudo loss:              5.52834
                Total gradient norm:      0.58615
                Solved trajectories:      19 / 32
                Avg steps to solve:       5.053
                
Iteration (2300/50001) took 0.123 seconds.
                Mean final reward:        6.9062
                Mean return:              2.4062
                Policy entropy:           1.2666
                Pseudo loss:              3.25124
                Total gradient norm:      0.39553
                Solved trajectories:      23 / 32
                Avg steps to solve:       3.739
                
Iteration (2400/50001) took 0.136 seconds.
                Mean final reward:        7.9375
                Mean return:              2.7500
                Policy entropy:           1.2503
                Pseudo loss:              5.37742
                Total gradient norm:      0.50393
                Solved trajectories:      25 / 32
                Avg steps to solve:       5.120
                
Iteration (2500/50001) took 0.122 seconds.
                Mean final reward:        7.9375
                Mean return:              3.8750
                Policy entropy:           1.1916
                Pseudo loss:              3.98432
                Total gradient norm:      0.38883
                Solved trajectories:      25 / 32
                Avg steps to solve:       3.680
                
Iteration (2600/50001) took 0.125 seconds.
                Mean final reward:        7.9375
                Mean return:              2.8125
                Policy entropy:           1.2004
                Pseudo loss:              4.93037
                Total gradient norm:      0.47539
                Solved trajectories:      25 / 32
                Avg steps to solve:       5.040
                
Iteration (2700/50001) took 0.165 seconds.
                Mean final reward:        8.2812
                Mean return:              4.0938
                Policy entropy:           1.1430
                Pseudo loss:              4.21766
                Total gradient norm:      0.42057
                Solved trajectories:      25 / 32
                Avg steps to solve:       3.840
                
Iteration (2800/50001) took 0.113 seconds.
                Mean final reward:        7.5938
                Mean return:              3.0625
                Policy entropy:           1.1873
                Pseudo loss:              3.73943
                Total gradient norm:      0.39404
                Solved trajectories:      24 / 32
                Avg steps to solve:       4.042
                
Iteration (2900/50001) took 0.155 seconds.
                Mean final reward:        8.6250
                Mean return:              3.7188
                Policy entropy:           1.1475
                Pseudo loss:              5.55593
                Total gradient norm:      0.45642
                Solved trajectories:      28 / 32
                Avg steps to solve:       5.321
                
Iteration (3000/50001) took 0.084 seconds.
                Mean final reward:        7.5938
                Mean return:              2.9375
                Policy entropy:           1.1404
                Pseudo loss:              4.12901
                Total gradient norm:      0.37182
                Solved trajectories:      25 / 32
                Avg steps to solve:       4.440
                
Iteration (3100/50001) took 0.073 seconds.
                Mean final reward:        6.5625
                Mean return:              0.9375
                Policy entropy:           1.1742
                Pseudo loss:              4.55810
                Total gradient norm:      0.39472
                Solved trajectories:      21 / 32
                Avg steps to solve:       4.857
                
Iteration (3200/50001) took 0.058 seconds.
                Mean final reward:        8.9688
                Mean return:              4.7500
                Policy entropy:           1.0746
                Pseudo loss:              4.41548
                Total gradient norm:      0.42558
                Solved trajectories:      29 / 32
                Avg steps to solve:       4.724
                
Iteration (3300/50001) took 0.055 seconds.
                Mean final reward:        8.9688
                Mean return:              4.7188
                Policy entropy:           1.0426
                Pseudo loss:              4.81702
                Total gradient norm:      0.38414
                Solved trajectories:      29 / 32
                Avg steps to solve:       4.759
                
Iteration (3400/50001) took 0.083 seconds.
                Mean final reward:        8.6250
                Mean return:              4.1875
                Policy entropy:           1.0780
                Pseudo loss:              4.34441
                Total gradient norm:      0.40102
                Solved trajectories:      27 / 32
                Avg steps to solve:       4.593
                
Iteration (3500/50001) took 0.062 seconds.
                Mean final reward:        8.6250
                Mean return:              4.0625
                Policy entropy:           1.0684
                Pseudo loss:              4.90057
                Total gradient norm:      0.38517
                Solved trajectories:      25 / 32
                Avg steps to solve:       4.320
                
Iteration (3600/50001) took 0.150 seconds.
                Mean final reward:        8.6250
                Mean return:              3.4688
                Policy entropy:           1.0564
                Pseudo loss:              5.65163
                Total gradient norm:      0.44963
                Solved trajectories:      26 / 32
                Avg steps to solve:       5.269
                
Iteration (3700/50001) took 0.140 seconds.
                Mean final reward:        8.9688
                Mean return:              4.4375
                Policy entropy:           1.0327
                Pseudo loss:              4.82657
                Total gradient norm:      0.38323
                Solved trajectories:      27 / 32
                Avg steps to solve:       4.704
                
Iteration (3800/50001) took 0.111 seconds.
                Mean final reward:        9.3125
                Mean return:              5.4062
                Policy entropy:           0.9863
                Pseudo loss:              4.05435
                Total gradient norm:      0.35612
                Solved trajectories:      28 / 32
                Avg steps to solve:       4.179
                
Iteration (3900/50001) took 0.103 seconds.
                Mean final reward:        7.9375
                Mean return:              2.9688
                Policy entropy:           1.0329
                Pseudo loss:              4.55948
                Total gradient norm:      0.42089
                Solved trajectories:      26 / 32
                Avg steps to solve:       5.038
                
Iteration (4000/50001) took 0.134 seconds.
                Mean final reward:        9.3125
                Mean return:              5.4062
                Policy entropy:           0.9593
                Pseudo loss:              4.49150
                Total gradient norm:      0.35106
                Solved trajectories:      29 / 32
                Avg steps to solve:       4.379
                
Iteration (4100/50001) took 0.163 seconds.
                Mean final reward:        8.9688
                Mean return:              4.2500
                Policy entropy:           1.0304
                Pseudo loss:              4.98989
                Total gradient norm:      0.35382
                Solved trajectories:      28 / 32
                Avg steps to solve:       5.107
                
Iteration (4200/50001) took 0.071 seconds.
                Mean final reward:        10.0000
                Mean return:              7.2500
                Policy entropy:           0.8798
                Pseudo loss:              3.25232
                Total gradient norm:      0.34117
                Solved trajectories:      30 / 32
                Avg steps to solve:       3.333
                
Iteration (4300/50001) took 0.146 seconds.
                Mean final reward:        9.6562
                Mean return:              6.0625
                Policy entropy:           0.9083
                Pseudo loss:              4.01130
                Total gradient norm:      0.32084
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.419
                
Iteration (4400/50001) took 0.072 seconds.
                Mean final reward:        8.6250
                Mean return:              4.9375
                Policy entropy:           0.9373
                Pseudo loss:              3.33867
                Total gradient norm:      0.29463
                Solved trajectories:      28 / 32
                Avg steps to solve:       3.929
                
Iteration (4500/50001) took 0.102 seconds.
                Mean final reward:        9.3125
                Mean return:              5.7812
                Policy entropy:           0.9266
                Pseudo loss:              3.78496
                Total gradient norm:      0.33561
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.167
                
Iteration (4600/50001) took 0.084 seconds.
                Mean final reward:        9.6562
                Mean return:              6.2188
                Policy entropy:           0.8675
                Pseudo loss:              4.03592
                Total gradient norm:      0.34259
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.067
                
Iteration (4700/50001) took 0.196 seconds.
                Mean final reward:        9.3125
                Mean return:              4.7812
                Policy entropy:           0.9163
                Pseudo loss:              4.92017
                Total gradient norm:      0.36409
                Solved trajectories:      27 / 32
                Avg steps to solve:       4.704
                
Iteration (4800/50001) took 0.108 seconds.
                Mean final reward:        9.3125
                Mean return:              5.4062
                Policy entropy:           0.9163
                Pseudo loss:              4.30578
                Total gradient norm:      0.35386
                Solved trajectories:      29 / 32
                Avg steps to solve:       4.379
                
Iteration (4900/50001) took 0.066 seconds.
                Mean final reward:        9.3125
                Mean return:              5.5625
                Policy entropy:           0.9050
                Pseudo loss:              3.98285
                Total gradient norm:      0.34182
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.400
                
Iteration (5000/50001) took 0.132 seconds.
                Mean final reward:        9.3125
                Mean return:              5.2500
                Policy entropy:           0.9115
                Pseudo loss:              4.20385
                Total gradient norm:      0.35502
                Solved trajectories:      28 / 32
                Avg steps to solve:       4.357
                
Iteration (5100/50001) took 0.070 seconds.
                Mean final reward:        9.3125
                Mean return:              6.1562
                Policy entropy:           0.8402
                Pseudo loss:              2.82455
                Total gradient norm:      0.32871
                Solved trajectories:      30 / 32
                Avg steps to solve:       3.767
                
Iteration (5200/50001) took 0.099 seconds.
                Mean final reward:        9.6562
                Mean return:              6.2812
                Policy entropy:           0.8703
                Pseudo loss:              3.50902
                Total gradient norm:      0.33256
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.194
                
Iteration (5300/50001) took 0.095 seconds.
                Mean final reward:        8.9688
                Mean return:              5.0312
                Policy entropy:           0.8888
                Pseudo loss:              4.13047
                Total gradient norm:      0.36171
                Solved trajectories:      28 / 32
                Avg steps to solve:       4.214
                
Iteration (5400/50001) took 0.100 seconds.
                Mean final reward:        9.6562
                Mean return:              5.5938
                Policy entropy:           0.8702
                Pseudo loss:              4.47168
                Total gradient norm:      0.33148
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.903
                
Iteration (5500/50001) took 0.087 seconds.
                Mean final reward:        9.6562
                Mean return:              6.0625
                Policy entropy:           0.8307
                Pseudo loss:              4.03320
                Total gradient norm:      0.37492
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.419
                
Iteration (5600/50001) took 0.127 seconds.
                Mean final reward:        10.0000
                Mean return:              7.2188
                Policy entropy:           0.7412
                Pseudo loss:              2.71007
                Total gradient norm:      0.29037
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.781
                
Iteration (5700/50001) took 0.094 seconds.
                Mean final reward:        9.6562
                Mean return:              5.8125
                Policy entropy:           0.8311
                Pseudo loss:              4.08755
                Total gradient norm:      0.39771
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.500
                
Iteration (5800/50001) took 0.083 seconds.
                Mean final reward:        9.6562
                Mean return:              6.5312
                Policy entropy:           0.8037
                Pseudo loss:              3.05430
                Total gradient norm:      0.33823
                Solved trajectories:      29 / 32
                Avg steps to solve:       3.517
                
Iteration (5900/50001) took 0.089 seconds.
                Mean final reward:        9.3125
                Mean return:              5.9688
                Policy entropy:           0.8336
                Pseudo loss:              2.89133
                Total gradient norm:      0.35031
                Solved trajectories:      30 / 32
                Avg steps to solve:       3.967
                
Iteration (6000/50001) took 0.076 seconds.
                Mean final reward:        9.6562
                Mean return:              5.8125
                Policy entropy:           0.8057
                Pseudo loss:              3.70122
                Total gradient norm:      0.32882
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.677
                
Iteration (6100/50001) took 0.086 seconds.
                Mean final reward:        10.0000
                Mean return:              6.2188
                Policy entropy:           0.8240
                Pseudo loss:              4.27119
                Total gradient norm:      0.34964
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.781
                
Iteration (6200/50001) took 0.108 seconds.
                Mean final reward:        8.9688
                Mean return:              5.3750
                Policy entropy:           0.8142
                Pseudo loss:              3.06303
                Total gradient norm:      0.40608
                Solved trajectories:      28 / 32
                Avg steps to solve:       3.821
                
Iteration (6300/50001) took 0.060 seconds.
                Mean final reward:        9.6562
                Mean return:              5.2500
                Policy entropy:           0.8461
                Pseudo loss:              5.22982
                Total gradient norm:      0.40621
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.100
                
Iteration (6400/50001) took 0.089 seconds.
                Mean final reward:        9.3125
                Mean return:              6.0625
                Policy entropy:           0.7884
                Pseudo loss:              3.25081
                Total gradient norm:      0.32796
                Solved trajectories:      29 / 32
                Avg steps to solve:       3.655
                
Iteration (6500/50001) took 0.137 seconds.
                Mean final reward:        9.3125
                Mean return:              5.5000
                Policy entropy:           0.8145
                Pseudo loss:              4.05478
                Total gradient norm:      0.29182
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.467
                
Iteration (6600/50001) took 0.128 seconds.
                Mean final reward:        8.9688
                Mean return:              4.3750
                Policy entropy:           0.8548
                Pseudo loss:              4.36844
                Total gradient norm:      0.39206
                Solved trajectories:      27 / 32
                Avg steps to solve:       4.778
                
Iteration (6700/50001) took 0.058 seconds.
                Mean final reward:        9.3125
                Mean return:              5.3125
                Policy entropy:           0.8186
                Pseudo loss:              4.20589
                Total gradient norm:      0.41786
                Solved trajectories:      29 / 32
                Avg steps to solve:       4.483
                
Iteration (6800/50001) took 0.079 seconds.
                Mean final reward:        10.0000
                Mean return:              6.5000
                Policy entropy:           0.7528
                Pseudo loss:              3.89635
                Total gradient norm:      0.29597
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.500
                
Iteration (6900/50001) took 0.151 seconds.
                Mean final reward:        10.0000
                Mean return:              5.3750
                Policy entropy:           0.8183
                Pseudo loss:              5.69884
                Total gradient norm:      0.43823
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.484
                
Iteration (7000/50001) took 0.080 seconds.
                Mean final reward:        10.0000
                Mean return:              6.5312
                Policy entropy:           0.7262
                Pseudo loss:              3.85803
                Total gradient norm:      0.31953
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.290
                
Iteration (7100/50001) took 0.077 seconds.
                Mean final reward:        9.6562
                Mean return:              6.3125
                Policy entropy:           0.7579
                Pseudo loss:              2.81580
                Total gradient norm:      0.31003
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.161
                
Iteration (7200/50001) took 0.068 seconds.
                Mean final reward:        10.0000
                Mean return:              6.4688
                Policy entropy:           0.7477
                Pseudo loss:              3.80711
                Total gradient norm:      0.30471
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.531
                
Iteration (7300/50001) took 0.082 seconds.
                Mean final reward:        10.0000
                Mean return:              6.6250
                Policy entropy:           0.7808
                Pseudo loss:              3.82556
                Total gradient norm:      0.33192
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.194
                
Iteration (7400/50001) took 0.116 seconds.
                Mean final reward:        9.6562
                Mean return:              6.1250
                Policy entropy:           0.7837
                Pseudo loss:              3.50198
                Total gradient norm:      0.32187
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.355
                
Iteration (7500/50001) took 0.077 seconds.
                Mean final reward:        10.0000
                Mean return:              6.7188
                Policy entropy:           0.7293
                Pseudo loss:              3.45777
                Total gradient norm:      0.29695
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.281
                
Iteration (7600/50001) took 0.070 seconds.
                Mean final reward:        9.6562
                Mean return:              6.4375
                Policy entropy:           0.7437
                Pseudo loss:              2.67512
                Total gradient norm:      0.27823
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.032
                
Iteration (7700/50001) took 0.102 seconds.
                Mean final reward:        10.0000
                Mean return:              6.3750
                Policy entropy:           0.7509
                Pseudo loss:              4.27544
                Total gradient norm:      0.43491
                Solved trajectories:      29 / 32
                Avg steps to solve:       4.069
                
Iteration (7800/50001) took 0.139 seconds.
                Mean final reward:        9.3125
                Mean return:              5.6562
                Policy entropy:           0.7873
                Pseudo loss:              3.52417
                Total gradient norm:      0.43685
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.300
                
Iteration (7900/50001) took 0.111 seconds.
                Mean final reward:        10.0000
                Mean return:              6.8438
                Policy entropy:           0.7318
                Pseudo loss:              2.88077
                Total gradient norm:      0.35591
                Solved trajectories:      31 / 32
                Avg steps to solve:       3.968
                
Iteration (8000/50001) took 0.136 seconds.
                Mean final reward:        8.9688
                Mean return:              4.9688
                Policy entropy:           0.7987
                Pseudo loss:              3.60297
                Total gradient norm:      0.30221
                Solved trajectories:      28 / 32
                Avg steps to solve:       4.286
                
Iteration (8100/50001) took 0.075 seconds.
                Mean final reward:        9.6562
                Mean return:              6.1562
                Policy entropy:           0.7389
                Pseudo loss:              3.70834
                Total gradient norm:      0.32851
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.133
                
Iteration (8200/50001) took 0.112 seconds.
                Mean final reward:        9.6562
                Mean return:              5.9062
                Policy entropy:           0.7462
                Pseudo loss:              4.23011
                Total gradient norm:      0.49919
                Solved trajectories:      29 / 32
                Avg steps to solve:       4.207
                
Iteration (8300/50001) took 0.143 seconds.
                Mean final reward:        9.6562
                Mean return:              5.9688
                Policy entropy:           0.7580
                Pseudo loss:              3.23298
                Total gradient norm:      0.30679
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.333
                
Iteration (8400/50001) took 0.076 seconds.
                Mean final reward:        10.0000
                Mean return:              6.6875
                Policy entropy:           0.7233
                Pseudo loss:              3.75187
                Total gradient norm:      0.30802
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.129
                
Iteration (8500/50001) took 0.112 seconds.
                Mean final reward:        9.6562
                Mean return:              6.6875
                Policy entropy:           0.6616
                Pseudo loss:              2.65576
                Total gradient norm:      0.24960
                Solved trajectories:      31 / 32
                Avg steps to solve:       3.774
                
Iteration (8600/50001) took 0.102 seconds.
                Mean final reward:        9.3125
                Mean return:              5.1875
                Policy entropy:           0.7710
                Pseudo loss:              4.39431
                Total gradient norm:      0.40702
                Solved trajectories:      28 / 32
                Avg steps to solve:       4.429
                
Iteration (8700/50001) took 0.096 seconds.
                Mean final reward:        8.9688
                Mean return:              5.5625
                Policy entropy:           0.7699
                Pseudo loss:              2.48921
                Total gradient norm:      0.28209
                Solved trajectories:      29 / 32
                Avg steps to solve:       3.828
                
Iteration (8800/50001) took 0.062 seconds.
                Mean final reward:        10.0000
                Mean return:              6.8125
                Policy entropy:           0.7025
                Pseudo loss:              3.84478
                Total gradient norm:      0.36611
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.000
                
Iteration (8900/50001) took 0.115 seconds.
                Mean final reward:        10.0000
                Mean return:              6.5312
                Policy entropy:           0.7022
                Pseudo loss:              3.69085
                Total gradient norm:      0.34411
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.290
                
Iteration (9000/50001) took 0.130 seconds.
                Mean final reward:        9.6562
                Mean return:              6.4375
                Policy entropy:           0.7183
                Pseudo loss:              3.57431
                Total gradient norm:      0.40957
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.032
                
Iteration (9100/50001) took 0.096 seconds.
                Mean final reward:        10.0000
                Mean return:              7.1875
                Policy entropy:           0.6229
                Pseudo loss:              2.73036
                Total gradient norm:      0.28319
                Solved trajectories:      31 / 32
                Avg steps to solve:       3.613
                
Iteration (9200/50001) took 0.117 seconds.
                Mean final reward:        10.0000
                Mean return:              6.7500
                Policy entropy:           0.7076
                Pseudo loss:              3.62330
                Total gradient norm:      0.28079
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.250
                
Iteration (9300/50001) took 0.076 seconds.
                Mean final reward:        9.3125
                Mean return:              4.9062
                Policy entropy:           0.7835
                Pseudo loss:              4.85513
                Total gradient norm:      0.50975
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.100
                
Iteration (9400/50001) took 0.059 seconds.
                Mean final reward:        9.6562
                Mean return:              5.5312
                Policy entropy:           0.7511
                Pseudo loss:              4.20323
                Total gradient norm:      0.42185
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.800
                
Iteration (9500/50001) took 0.077 seconds.
                Mean final reward:        9.6562
                Mean return:              6.4375
                Policy entropy:           0.6818
                Pseudo loss:              3.15219
                Total gradient norm:      0.29712
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.032
                
Iteration (9600/50001) took 0.064 seconds.
                Mean final reward:        10.0000
                Mean return:              6.8438
                Policy entropy:           0.6730
                Pseudo loss:              3.30827
                Total gradient norm:      0.25862
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.156
                
Iteration (9700/50001) took 0.126 seconds.
                Mean final reward:        10.0000
                Mean return:              6.8438
                Policy entropy:           0.6876
                Pseudo loss:              3.31797
                Total gradient norm:      0.40549
                Solved trajectories:      31 / 32
                Avg steps to solve:       3.968
                
Iteration (9800/50001) took 0.080 seconds.
                Mean final reward:        9.6562
                Mean return:              6.0000
                Policy entropy:           0.7352
                Pseudo loss:              3.72003
                Total gradient norm:      0.38873
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.300
                
Iteration (9900/50001) took 0.046 seconds.
                Mean final reward:        10.0000
                Mean return:              6.7188
                Policy entropy:           0.6860
                Pseudo loss:              3.01823
                Total gradient norm:      0.30900
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.281
                
Iteration (10000/50001) took 0.117 seconds.
                Mean final reward:        9.6562
                Mean return:              6.2500
                Policy entropy:           0.7003
                Pseudo loss:              3.40105
                Total gradient norm:      0.34239
                Solved trajectories:      29 / 32
                Avg steps to solve:       3.828
                
Iteration (10100/50001) took 0.129 seconds.
                Mean final reward:        9.6562
                Mean return:              5.8125
                Policy entropy:           0.7544
                Pseudo loss:              4.01638
                Total gradient norm:      0.31287
                Solved trajectories:      29 / 32
                Avg steps to solve:       4.310
                
Iteration (10200/50001) took 0.114 seconds.
                Mean final reward:        10.0000
                Mean return:              6.7812
                Policy entropy:           0.7152
                Pseudo loss:              2.95372
                Total gradient norm:      0.29316
                Solved trajectories:      30 / 32
                Avg steps to solve:       3.833
                
Iteration (10300/50001) took 0.039 seconds.
                Mean final reward:        10.0000
                Mean return:              6.6250
                Policy entropy:           0.6774
                Pseudo loss:              3.67352
                Total gradient norm:      0.39138
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.375
                
Iteration (10400/50001) took 0.058 seconds.
                Mean final reward:        9.6562
                Mean return:              5.8125
                Policy entropy:           0.6921
                Pseudo loss:              4.22378
                Total gradient norm:      0.42656
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.677
                
Iteration (10500/50001) took 0.105 seconds.
                Mean final reward:        9.6562
                Mean return:              6.5000
                Policy entropy:           0.6761
                Pseudo loss:              2.68795
                Total gradient norm:      0.29626
                Solved trajectories:      31 / 32
                Avg steps to solve:       3.968
                
Iteration (10600/50001) took 0.094 seconds.
                Mean final reward:        9.3125
                Mean return:              6.1250
                Policy entropy:           0.6725
                Pseudo loss:              2.69111
                Total gradient norm:      0.28572
                Solved trajectories:      30 / 32
                Avg steps to solve:       3.800
                
Iteration (10700/50001) took 0.079 seconds.
                Mean final reward:        9.3125
                Mean return:              5.2812
                Policy entropy:           0.7744
                Pseudo loss:              3.74545
                Total gradient norm:      0.35000
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.700
                
Iteration (10800/50001) took 0.149 seconds.
                Mean final reward:        10.0000
                Mean return:              6.5000
                Policy entropy:           0.6754
                Pseudo loss:              3.71695
                Total gradient norm:      0.28421
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.500
                
Iteration (10900/50001) took 0.073 seconds.
                Mean final reward:        10.0000
                Mean return:              6.3438
                Policy entropy:           0.7253
                Pseudo loss:              4.02869
                Total gradient norm:      0.35520
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.656
                
Iteration (11000/50001) took 0.069 seconds.
                Mean final reward:        10.0000
                Mean return:              7.2500
                Policy entropy:           0.6156
                Pseudo loss:              2.52150
                Total gradient norm:      0.24272
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.750
                
Iteration (11100/50001) took 0.116 seconds.
                Mean final reward:        9.6562
                Mean return:              6.5000
                Policy entropy:           0.6607
                Pseudo loss:              2.68834
                Total gradient norm:      0.29634
                Solved trajectories:      31 / 32
                Avg steps to solve:       3.968
                
Iteration (11200/50001) took 0.084 seconds.
                Mean final reward:        9.3125
                Mean return:              5.5625
                Policy entropy:           0.7199
                Pseudo loss:              3.21854
                Total gradient norm:      0.28589
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.400
                
Iteration (11300/50001) took 0.109 seconds.
                Mean final reward:        9.6562
                Mean return:              6.5625
                Policy entropy:           0.6530
                Pseudo loss:              2.90553
                Total gradient norm:      0.38933
                Solved trajectories:      31 / 32
                Avg steps to solve:       3.903
                
Iteration (11400/50001) took 0.077 seconds.
                Mean final reward:        9.6562
                Mean return:              6.3750
                Policy entropy:           0.7060
                Pseudo loss:              2.94149
                Total gradient norm:      0.31867
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.097
                
Iteration (11500/50001) took 0.079 seconds.
                Mean final reward:        9.6562
                Mean return:              6.4375
                Policy entropy:           0.6809
                Pseudo loss:              2.95498
                Total gradient norm:      0.29980
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.032
                
Iteration (11600/50001) took 0.087 seconds.
                Mean final reward:        10.0000
                Mean return:              5.9062
                Policy entropy:           0.7137
                Pseudo loss:              4.38455
                Total gradient norm:      0.31239
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.935
                
Iteration (11700/50001) took 0.107 seconds.
                Mean final reward:        10.0000
                Mean return:              6.8438
                Policy entropy:           0.6548
                Pseudo loss:              3.00769
                Total gradient norm:      0.31002
                Solved trajectories:      31 / 32
                Avg steps to solve:       3.968
                
Iteration (11800/50001) took 0.085 seconds.
                Mean final reward:        10.0000
                Mean return:              7.1250
                Policy entropy:           0.6170
                Pseudo loss:              2.85636
                Total gradient norm:      0.25878
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.875
                
Iteration (11900/50001) took 0.154 seconds.
                Mean final reward:        10.0000
                Mean return:              7.0938
                Policy entropy:           0.6208
                Pseudo loss:              2.33016
                Total gradient norm:      0.25987
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.906
                
Iteration (12000/50001) took 0.115 seconds.
                Mean final reward:        9.6562
                Mean return:              6.3438
                Policy entropy:           0.6722
                Pseudo loss:              2.95608
                Total gradient norm:      0.27089
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.129
                
Iteration (12100/50001) took 0.098 seconds.
                Mean final reward:        10.0000
                Mean return:              7.7500
                Policy entropy:           0.6007
                Pseudo loss:              1.96129
                Total gradient norm:      0.22138
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.250
                
Iteration (12200/50001) took 0.116 seconds.
                Mean final reward:        10.0000
                Mean return:              6.9688
                Policy entropy:           0.6289
                Pseudo loss:              2.90292
                Total gradient norm:      0.21899
                Solved trajectories:      31 / 32
                Avg steps to solve:       3.839
                
Iteration (12300/50001) took 0.146 seconds.
                Mean final reward:        10.0000
                Mean return:              6.2188
                Policy entropy:           0.6781
                Pseudo loss:              3.85604
                Total gradient norm:      0.37713
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.433
                
Iteration (12400/50001) took 0.134 seconds.
                Mean final reward:        9.6562
                Mean return:              6.1562
                Policy entropy:           0.6621
                Pseudo loss:              3.50653
                Total gradient norm:      0.32960
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.133
                
Iteration (12500/50001) took 0.152 seconds.
                Mean final reward:        10.0000
                Mean return:              6.7188
                Policy entropy:           0.6424
                Pseudo loss:              3.10727
                Total gradient norm:      0.28102
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.281
                
Iteration (12600/50001) took 0.140 seconds.
                Mean final reward:        10.0000
                Mean return:              6.3750
                Policy entropy:           0.6969
                Pseudo loss:              3.72895
                Total gradient norm:      0.25765
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.625
                
Iteration (12700/50001) took 0.143 seconds.
                Mean final reward:        10.0000
                Mean return:              6.4688
                Policy entropy:           0.6976
                Pseudo loss:              3.32175
                Total gradient norm:      0.33717
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.531
                
Iteration (12800/50001) took 0.115 seconds.
                Mean final reward:        10.0000
                Mean return:              6.2500
                Policy entropy:           0.6824
                Pseudo loss:              3.86619
                Total gradient norm:      0.32529
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.581
                
Iteration (12900/50001) took 0.132 seconds.
                Mean final reward:        9.6562
                Mean return:              6.2812
                Policy entropy:           0.6725
                Pseudo loss:              3.04294
                Total gradient norm:      0.36628
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.000
                
Iteration (13000/50001) took 0.130 seconds.
                Mean final reward:        10.0000
                Mean return:              6.8438
                Policy entropy:           0.6290
                Pseudo loss:              2.76427
                Total gradient norm:      0.31420
                Solved trajectories:      31 / 32
                Avg steps to solve:       3.968
                
Iteration (13100/50001) took 0.135 seconds.
                Mean final reward:        8.9688
                Mean return:              5.0938
                Policy entropy:           0.7254
                Pseudo loss:              3.78401
                Total gradient norm:      0.36152
                Solved trajectories:      28 / 32
                Avg steps to solve:       4.143
                
Iteration (13200/50001) took 0.091 seconds.
                Mean final reward:        9.6562
                Mean return:              6.1562
                Policy entropy:           0.6710
                Pseudo loss:              3.28892
                Total gradient norm:      0.28352
                Solved trajectories:      29 / 32
                Avg steps to solve:       3.931
                
Iteration (13300/50001) took 0.068 seconds.
                Mean final reward:        9.6562
                Mean return:              5.9688
                Policy entropy:           0.7063
                Pseudo loss:              3.58425
                Total gradient norm:      0.39354
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.333
                
Iteration (13400/50001) took 0.069 seconds.
                Mean final reward:        9.6562
                Mean return:              6.5000
                Policy entropy:           0.6437
                Pseudo loss:              3.27944
                Total gradient norm:      0.31174
                Solved trajectories:      31 / 32
                Avg steps to solve:       3.968
                
Iteration (13500/50001) took 0.076 seconds.
                Mean final reward:        10.0000
                Mean return:              6.4688
                Policy entropy:           0.6821
                Pseudo loss:              3.60523
                Total gradient norm:      0.29037
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.531
                
Iteration (13600/50001) took 0.128 seconds.
                Mean final reward:        10.0000
                Mean return:              6.8438
                Policy entropy:           0.6558
                Pseudo loss:              3.04283
                Total gradient norm:      0.27074
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.156
                
Iteration (13700/50001) took 0.083 seconds.
                Mean final reward:        10.0000
                Mean return:              6.5938
                Policy entropy:           0.6788
                Pseudo loss:              3.31723
                Total gradient norm:      0.32268
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.406
                
Iteration (13800/50001) took 0.109 seconds.
                Mean final reward:        10.0000
                Mean return:              6.5938
                Policy entropy:           0.6505
                Pseudo loss:              3.40608
                Total gradient norm:      0.33512
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.226
                
Iteration (13900/50001) took 0.074 seconds.
                Mean final reward:        10.0000
                Mean return:              6.0312
                Policy entropy:           0.6846
                Pseudo loss:              4.43095
                Total gradient norm:      0.38702
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.806
                
Iteration (14000/50001) took 0.121 seconds.
                Mean final reward:        9.6562
                Mean return:              5.9062
                Policy entropy:           0.6855
                Pseudo loss:              3.60265
                Total gradient norm:      0.37081
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.581
                
Iteration (14100/50001) took 0.151 seconds.
                Mean final reward:        9.6562
                Mean return:              6.4375
                Policy entropy:           0.6522
                Pseudo loss:              2.87920
                Total gradient norm:      0.31650
                Solved trajectories:      30 / 32
                Avg steps to solve:       3.833
                
Iteration (14200/50001) took 0.122 seconds.
                Mean final reward:        9.6562
                Mean return:              6.5312
                Policy entropy:           0.6497
                Pseudo loss:              2.76768
                Total gradient norm:      0.25619
                Solved trajectories:      30 / 32
                Avg steps to solve:       3.733
                
Iteration (14300/50001) took 0.123 seconds.
                Mean final reward:        10.0000
                Mean return:              5.8438
                Policy entropy:           0.6680
                Pseudo loss:              4.80254
                Total gradient norm:      0.46931
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.156
                
Iteration (14400/50001) took 0.158 seconds.
                Mean final reward:        10.0000
                Mean return:              6.6250
                Policy entropy:           0.6610
                Pseudo loss:              3.74752
                Total gradient norm:      0.41354
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.375
                
Iteration (14500/50001) took 0.063 seconds.
                Mean final reward:        10.0000
                Mean return:              7.1250
                Policy entropy:           0.6219
                Pseudo loss:              3.05164
                Total gradient norm:      0.32469
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.875
                
Iteration (14600/50001) took 0.133 seconds.
                Mean final reward:        9.6562
                Mean return:              5.9062
                Policy entropy:           0.6751
                Pseudo loss:              3.54805
                Total gradient norm:      0.37018
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.581
                
Iteration (14700/50001) took 0.113 seconds.
                Mean final reward:        10.0000
                Mean return:              6.6562
                Policy entropy:           0.6294
                Pseudo loss:              3.63685
                Total gradient norm:      0.32242
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.161
                
Iteration (14800/50001) took 0.071 seconds.
                Mean final reward:        10.0000
                Mean return:              6.3438
                Policy entropy:           0.6510
                Pseudo loss:              4.19358
                Total gradient norm:      0.41549
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.300
                
Iteration (14900/50001) took 0.181 seconds.
                Mean final reward:        9.6562
                Mean return:              5.9062
                Policy entropy:           0.6818
                Pseudo loss:              4.18496
                Total gradient norm:      0.34774
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.581
                
Iteration (15000/50001) took 0.179 seconds.
                Mean final reward:        10.0000
                Mean return:              6.4375
                Policy entropy:           0.6597
                Pseudo loss:              3.86263
                Total gradient norm:      0.32055
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.562
                
Iteration (15100/50001) took 0.107 seconds.
                Mean final reward:        10.0000
                Mean return:              7.2188
                Policy entropy:           0.6043
                Pseudo loss:              2.73838
                Total gradient norm:      0.26850
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.781
                
Iteration (15200/50001) took 0.149 seconds.
                Mean final reward:        10.0000
                Mean return:              6.9375
                Policy entropy:           0.6258
                Pseudo loss:              2.78262
                Total gradient norm:      0.28562
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.062
                
Iteration (15300/50001) took 0.116 seconds.
                Mean final reward:        10.0000
                Mean return:              7.0625
                Policy entropy:           0.6202
                Pseudo loss:              2.82978
                Total gradient norm:      0.28805
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.938
                
Iteration (15400/50001) took 0.147 seconds.
                Mean final reward:        10.0000
                Mean return:              7.1250
                Policy entropy:           0.6287
                Pseudo loss:              2.80588
                Total gradient norm:      0.28677
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.875
                
Iteration (15500/50001) took 0.068 seconds.
                Mean final reward:        10.0000
                Mean return:              7.0312
                Policy entropy:           0.6341
                Pseudo loss:              2.80986
                Total gradient norm:      0.24285
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.969
                
Iteration (15600/50001) took 0.054 seconds.
                Mean final reward:        10.0000
                Mean return:              6.9062
                Policy entropy:           0.6259
                Pseudo loss:              3.04330
                Total gradient norm:      0.25527
                Solved trajectories:      31 / 32
                Avg steps to solve:       3.903
                
Iteration (15700/50001) took 0.041 seconds.
                Mean final reward:        9.6562
                Mean return:              6.5938
                Policy entropy:           0.6584
                Pseudo loss:              2.60567
                Total gradient norm:      0.28578
                Solved trajectories:      31 / 32
                Avg steps to solve:       3.871
                
Iteration (15800/50001) took 0.106 seconds.
                Mean final reward:        9.3125
                Mean return:              6.5312
                Policy entropy:           0.6423
                Pseudo loss:              1.74877
                Total gradient norm:      0.22692
                Solved trajectories:      30 / 32
                Avg steps to solve:       3.367
                
Iteration (15900/50001) took 0.113 seconds.
                Mean final reward:        10.0000
                Mean return:              7.5312
                Policy entropy:           0.6041
                Pseudo loss:              1.87519
                Total gradient norm:      0.20679
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.469
                
Iteration (16000/50001) took 0.081 seconds.
                Mean final reward:        9.6562
                Mean return:              6.3125
                Policy entropy:           0.6595
                Pseudo loss:              3.01416
                Total gradient norm:      0.29266
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.161
                
Iteration (16100/50001) took 0.157 seconds.
                Mean final reward:        9.6562
                Mean return:              6.5000
                Policy entropy:           0.6634
                Pseudo loss:              2.75191
                Total gradient norm:      0.27816
                Solved trajectories:      31 / 32
                Avg steps to solve:       3.968
                
Iteration (16200/50001) took 0.142 seconds.
                Mean final reward:        10.0000
                Mean return:              7.0625
                Policy entropy:           0.6092
                Pseudo loss:              2.51907
                Total gradient norm:      0.24059
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.938
                
Iteration (16300/50001) took 0.129 seconds.
                Mean final reward:        10.0000
                Mean return:              6.6875
                Policy entropy:           0.6382
                Pseudo loss:              3.24759
                Total gradient norm:      0.26167
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.312
                
Iteration (16400/50001) took 0.097 seconds.
                Mean final reward:        10.0000
                Mean return:              6.4688
                Policy entropy:           0.6627
                Pseudo loss:              3.39545
                Total gradient norm:      0.29308
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.355
                
Iteration (16500/50001) took 0.146 seconds.
                Mean final reward:        9.6562
                Mean return:              6.7812
                Policy entropy:           0.6381
                Pseudo loss:              2.23228
                Total gradient norm:      0.25811
                Solved trajectories:      31 / 32
                Avg steps to solve:       3.677
                
Iteration (16600/50001) took 0.134 seconds.
                Mean final reward:        9.6562
                Mean return:              6.5625
                Policy entropy:           0.6462
                Pseudo loss:              2.72023
                Total gradient norm:      0.25721
                Solved trajectories:      31 / 32
                Avg steps to solve:       3.903
                
Iteration (16700/50001) took 0.151 seconds.
                Mean final reward:        9.6562
                Mean return:              6.0312
                Policy entropy:           0.6687
                Pseudo loss:              3.65921
                Total gradient norm:      0.44287
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.267
                
Iteration (16800/50001) took 0.139 seconds.
                Mean final reward:        9.3125
                Mean return:              5.8750
                Policy entropy:           0.6514
                Pseudo loss:              3.33309
                Total gradient norm:      0.35271
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.067
                
Iteration (16900/50001) took 0.091 seconds.
                Mean final reward:        9.6562
                Mean return:              6.1562
                Policy entropy:           0.6869
                Pseudo loss:              3.43739
                Total gradient norm:      0.35357
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.133
                
Iteration (17000/50001) took 0.079 seconds.
                Mean final reward:        10.0000
                Mean return:              7.0000
                Policy entropy:           0.6276
                Pseudo loss:              2.98668
                Total gradient norm:      0.30124
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.000
                
Iteration (17100/50001) took 0.061 seconds.
                Mean final reward:        10.0000
                Mean return:              7.6875
                Policy entropy:           0.5378
                Pseudo loss:              1.95634
                Total gradient norm:      0.29321
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.312
                
Iteration (17200/50001) took 0.126 seconds.
                Mean final reward:        10.0000
                Mean return:              6.8125
                Policy entropy:           0.6097
                Pseudo loss:              2.93627
                Total gradient norm:      0.26626
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.188
                
Iteration (17300/50001) took 0.124 seconds.
                Mean final reward:        10.0000
                Mean return:              6.4062
                Policy entropy:           0.6655
                Pseudo loss:              4.01663
                Total gradient norm:      0.44578
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.419
                
Iteration (17400/50001) took 0.094 seconds.
                Mean final reward:        9.6562
                Mean return:              6.9062
                Policy entropy:           0.5962
                Pseudo loss:              2.55086
                Total gradient norm:      0.24888
                Solved trajectories:      31 / 32
                Avg steps to solve:       3.548
                
Iteration (17500/50001) took 0.130 seconds.
                Mean final reward:        10.0000
                Mean return:              6.2500
                Policy entropy:           0.6709
                Pseudo loss:              4.21361
                Total gradient norm:      0.25819
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.750
                
Iteration (17600/50001) took 0.065 seconds.
                Mean final reward:        10.0000
                Mean return:              7.1562
                Policy entropy:           0.6195
                Pseudo loss:              2.88563
                Total gradient norm:      0.32368
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.844
                
Iteration (17700/50001) took 0.114 seconds.
                Mean final reward:        10.0000
                Mean return:              6.7500
                Policy entropy:           0.6436
                Pseudo loss:              3.46128
                Total gradient norm:      0.36867
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.250
                
Iteration (17800/50001) took 0.127 seconds.
                Mean final reward:        9.3125
                Mean return:              5.9375
                Policy entropy:           0.6761
                Pseudo loss:              3.09389
                Total gradient norm:      0.37814
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.000
                
Iteration (17900/50001) took 0.077 seconds.
                Mean final reward:        10.0000
                Mean return:              7.0938
                Policy entropy:           0.6335
                Pseudo loss:              2.28996
                Total gradient norm:      0.24697
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.906
                
Iteration (18000/50001) took 0.130 seconds.
                Mean final reward:        10.0000
                Mean return:              6.8125
                Policy entropy:           0.6004
                Pseudo loss:              3.11319
                Total gradient norm:      0.26815
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.188
                
Iteration (18100/50001) took 0.072 seconds.
                Mean final reward:        10.0000
                Mean return:              6.9375
                Policy entropy:           0.6125
                Pseudo loss:              2.44256
                Total gradient norm:      0.24694
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.062
                
Iteration (18200/50001) took 0.082 seconds.
                Mean final reward:        10.0000
                Mean return:              6.8125
                Policy entropy:           0.6381
                Pseudo loss:              3.35621
                Total gradient norm:      0.33205
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.188
                
Iteration (18300/50001) took 0.091 seconds.
                Mean final reward:        10.0000
                Mean return:              7.4688
                Policy entropy:           0.5684
                Pseudo loss:              2.28284
                Total gradient norm:      0.19981
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.531
                
Iteration (18400/50001) took 0.079 seconds.
                Mean final reward:        10.0000
                Mean return:              6.6875
                Policy entropy:           0.6471
                Pseudo loss:              3.04509
                Total gradient norm:      0.31838
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.129
                
Iteration (18500/50001) took 0.135 seconds.
                Mean final reward:        10.0000
                Mean return:              6.9688
                Policy entropy:           0.6034
                Pseudo loss:              3.25259
                Total gradient norm:      0.23777
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.031
                
Iteration (18600/50001) took 0.071 seconds.
                Mean final reward:        9.3125
                Mean return:              6.1875
                Policy entropy:           0.6460
                Pseudo loss:              2.27841
                Total gradient norm:      0.26060
                Solved trajectories:      30 / 32
                Avg steps to solve:       3.733
                
Iteration (18700/50001) took 0.121 seconds.
                Mean final reward:        10.0000
                Mean return:              6.6250
                Policy entropy:           0.6684
                Pseudo loss:              3.60396
                Total gradient norm:      0.37369
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.375
                
Iteration (18800/50001) took 0.089 seconds.
                Mean final reward:        10.0000
                Mean return:              6.8750
                Policy entropy:           0.6446
                Pseudo loss:              3.25763
                Total gradient norm:      0.36009
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.125
                
Iteration (18900/50001) took 0.136 seconds.
                Mean final reward:        10.0000
                Mean return:              7.0312
                Policy entropy:           0.6258
                Pseudo loss:              2.64547
                Total gradient norm:      0.26357
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.969
                
Iteration (19000/50001) took 0.081 seconds.
                Mean final reward:        10.0000
                Mean return:              6.7188
                Policy entropy:           0.6289
                Pseudo loss:              3.36426
                Total gradient norm:      0.31333
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.281
                
Iteration (19100/50001) took 0.123 seconds.
                Mean final reward:        10.0000
                Mean return:              6.9375
                Policy entropy:           0.6326
                Pseudo loss:              3.08595
                Total gradient norm:      0.30013
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.062
                
Iteration (19200/50001) took 0.146 seconds.
                Mean final reward:        10.0000
                Mean return:              6.4062
                Policy entropy:           0.6552
                Pseudo loss:              3.68867
                Total gradient norm:      0.31904
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.594
                
Iteration (19300/50001) took 0.151 seconds.
                Mean final reward:        10.0000
                Mean return:              6.5312
                Policy entropy:           0.6693
                Pseudo loss:              3.75957
                Total gradient norm:      0.28658
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.100
                
Iteration (19400/50001) took 0.120 seconds.
                Mean final reward:        10.0000
                Mean return:              6.4688
                Policy entropy:           0.6657
                Pseudo loss:              3.49483
                Total gradient norm:      0.28279
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.355
                
Iteration (19500/50001) took 0.135 seconds.
                Mean final reward:        10.0000
                Mean return:              6.6562
                Policy entropy:           0.6618
                Pseudo loss:              3.17377
                Total gradient norm:      0.33510
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.344
                
Iteration (19600/50001) took 0.052 seconds.
                Mean final reward:        9.6562
                Mean return:              6.0312
                Policy entropy:           0.6697
                Pseudo loss:              3.60162
                Total gradient norm:      0.32072
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.267
                
Iteration (19700/50001) took 0.071 seconds.
                Mean final reward:        10.0000
                Mean return:              6.6562
                Policy entropy:           0.6732
                Pseudo loss:              3.22428
                Total gradient norm:      0.26012
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.161
                
Iteration (19800/50001) took 0.121 seconds.
                Mean final reward:        9.6562
                Mean return:              6.3750
                Policy entropy:           0.6440
                Pseudo loss:              2.75135
                Total gradient norm:      0.25667
                Solved trajectories:      30 / 32
                Avg steps to solve:       3.900
                
Iteration (19900/50001) took 0.095 seconds.
                Mean final reward:        9.3125
                Mean return:              5.6875
                Policy entropy:           0.6967
                Pseudo loss:              3.27246
                Total gradient norm:      0.28027
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.267
                
Iteration (20000/50001) took 0.134 seconds.
                Mean final reward:        10.0000
                Mean return:              6.8750
                Policy entropy:           0.6220
                Pseudo loss:              2.95384
                Total gradient norm:      0.21471
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.125
                
Iteration (20100/50001) took 0.110 seconds.
                Mean final reward:        10.0000
                Mean return:              6.6250
                Policy entropy:           0.6389
                Pseudo loss:              3.36084
                Total gradient norm:      0.28601
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.375
                
Iteration (20200/50001) took 0.070 seconds.
                Mean final reward:        10.0000
                Mean return:              7.1875
                Policy entropy:           0.5877
                Pseudo loss:              3.06174
                Total gradient norm:      0.28206
                Solved trajectories:      31 / 32
                Avg steps to solve:       3.613
                
Iteration (20300/50001) took 0.095 seconds.
                Mean final reward:        10.0000
                Mean return:              6.6562
                Policy entropy:           0.6356
                Pseudo loss:              3.81363
                Total gradient norm:      0.38205
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.161
                
Iteration (20400/50001) took 0.118 seconds.
                Mean final reward:        9.6562
                Mean return:              6.1562
                Policy entropy:           0.6707
                Pseudo loss:              2.80318
                Total gradient norm:      0.34367
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.133
                
Iteration (20500/50001) took 0.078 seconds.
                Mean final reward:        10.0000
                Mean return:              6.7812
                Policy entropy:           0.6287
                Pseudo loss:              3.13035
                Total gradient norm:      0.31975
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.219
                
Iteration (20600/50001) took 0.064 seconds.
                Mean final reward:        10.0000
                Mean return:              6.7500
                Policy entropy:           0.6088
                Pseudo loss:              3.84837
                Total gradient norm:      0.46058
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.250
                
Iteration (20700/50001) took 0.092 seconds.
                Mean final reward:        10.0000
                Mean return:              6.9688
                Policy entropy:           0.5767
                Pseudo loss:              3.37071
                Total gradient norm:      0.43747
                Solved trajectories:      31 / 32
                Avg steps to solve:       3.839
                
Iteration (20800/50001) took 0.133 seconds.
                Mean final reward:        9.6562
                Mean return:              6.4375
                Policy entropy:           0.6504
                Pseudo loss:              2.97698
                Total gradient norm:      0.24795
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.032
                
Iteration (20900/50001) took 0.074 seconds.
                Mean final reward:        10.0000
                Mean return:              7.5312
                Policy entropy:           0.5543
                Pseudo loss:              1.80409
                Total gradient norm:      0.19144
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.469
                
Iteration (21000/50001) took 0.060 seconds.
                Mean final reward:        10.0000
                Mean return:              7.5312
                Policy entropy:           0.5507
                Pseudo loss:              2.25621
                Total gradient norm:      0.21039
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.469
                
Iteration (21100/50001) took 0.075 seconds.
                Mean final reward:        10.0000
                Mean return:              6.3750
                Policy entropy:           0.6724
                Pseudo loss:              3.63667
                Total gradient norm:      0.29663
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.625
                
Iteration (21200/50001) took 0.096 seconds.
                Mean final reward:        9.6562
                Mean return:              6.5312
                Policy entropy:           0.6318
                Pseudo loss:              2.99331
                Total gradient norm:      0.33095
                Solved trajectories:      31 / 32
                Avg steps to solve:       3.935
                
Iteration (21300/50001) took 0.113 seconds.
                Mean final reward:        10.0000
                Mean return:              7.2812
                Policy entropy:           0.6112
                Pseudo loss:              2.65429
                Total gradient norm:      0.27625
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.719
                
Iteration (21400/50001) took 0.105 seconds.
                Mean final reward:        9.6562
                Mean return:              6.5938
                Policy entropy:           0.6252
                Pseudo loss:              2.52826
                Total gradient norm:      0.29624
                Solved trajectories:      31 / 32
                Avg steps to solve:       3.871
                
Iteration (21500/50001) took 0.077 seconds.
                Mean final reward:        10.0000
                Mean return:              6.7812
                Policy entropy:           0.6347
                Pseudo loss:              3.31411
                Total gradient norm:      0.32898
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.032
                
Iteration (21600/50001) took 0.075 seconds.
                Mean final reward:        10.0000
                Mean return:              6.8750
                Policy entropy:           0.6358
                Pseudo loss:              2.93764
                Total gradient norm:      0.29390
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.125
                
Iteration (21700/50001) took 0.127 seconds.
                Mean final reward:        10.0000
                Mean return:              5.8125
                Policy entropy:           0.6776
                Pseudo loss:              4.92185
                Total gradient norm:      0.46121
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.032
                
Iteration (21800/50001) took 0.084 seconds.
                Mean final reward:        10.0000
                Mean return:              6.6875
                Policy entropy:           0.6172
                Pseudo loss:              2.95871
                Total gradient norm:      0.26860
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.129
                
Iteration (21900/50001) took 0.097 seconds.
                Mean final reward:        10.0000
                Mean return:              5.8125
                Policy entropy:           0.6652
                Pseudo loss:              5.12282
                Total gradient norm:      0.45608
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.188
                
Iteration (22000/50001) took 0.073 seconds.
                Mean final reward:        10.0000
                Mean return:              7.0938
                Policy entropy:           0.6004
                Pseudo loss:              2.36850
                Total gradient norm:      0.24324
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.906
                
Iteration (22100/50001) took 0.070 seconds.
                Mean final reward:        10.0000
                Mean return:              7.4688
                Policy entropy:           0.5919
                Pseudo loss:              1.93416
                Total gradient norm:      0.23222
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.531
                
Iteration (22200/50001) took 0.110 seconds.
                Mean final reward:        10.0000
                Mean return:              6.5000
                Policy entropy:           0.6640
                Pseudo loss:              3.52421
                Total gradient norm:      0.29277
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.323
                
Iteration (22300/50001) took 0.096 seconds.
                Mean final reward:        9.6562
                Mean return:              6.5312
                Policy entropy:           0.6563
                Pseudo loss:              2.50507
                Total gradient norm:      0.24941
                Solved trajectories:      31 / 32
                Avg steps to solve:       3.935
                
Iteration (22400/50001) took 0.073 seconds.
                Mean final reward:        9.6562
                Mean return:              6.3438
                Policy entropy:           0.6469
                Pseudo loss:              3.14185
                Total gradient norm:      0.29641
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.129
                
Iteration (22500/50001) took 0.063 seconds.
                Mean final reward:        10.0000
                Mean return:              7.0000
                Policy entropy:           0.6187
                Pseudo loss:              2.58527
                Total gradient norm:      0.26266
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.000
                
Iteration (22600/50001) took 0.083 seconds.
                Mean final reward:        10.0000
                Mean return:              6.6562
                Policy entropy:           0.6275
                Pseudo loss:              3.75222
                Total gradient norm:      0.29035
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.161
                
Iteration (22700/50001) took 0.074 seconds.
                Mean final reward:        9.6562
                Mean return:              7.0312
                Policy entropy:           0.6170
                Pseudo loss:              2.44767
                Total gradient norm:      0.29658
                Solved trajectories:      30 / 32
                Avg steps to solve:       3.200
                
Iteration (22800/50001) took 0.076 seconds.
                Mean final reward:        10.0000
                Mean return:              6.2188
                Policy entropy:           0.6703
                Pseudo loss:              3.87341
                Total gradient norm:      0.32001
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.781
                
Iteration (22900/50001) took 0.125 seconds.
                Mean final reward:        9.6562
                Mean return:              6.4375
                Policy entropy:           0.6364
                Pseudo loss:              3.15245
                Total gradient norm:      0.30313
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.032
                
Iteration (23000/50001) took 0.066 seconds.
                Mean final reward:        10.0000
                Mean return:              6.7500
                Policy entropy:           0.6204
                Pseudo loss:              2.73165
                Total gradient norm:      0.26521
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.250
                
Iteration (23100/50001) took 0.123 seconds.
                Mean final reward:        10.0000
                Mean return:              6.3438
                Policy entropy:           0.6552
                Pseudo loss:              3.82878
                Total gradient norm:      0.37988
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.300
                
Iteration (23200/50001) took 0.073 seconds.
                Mean final reward:        10.0000
                Mean return:              6.6562
                Policy entropy:           0.6478
                Pseudo loss:              3.20129
                Total gradient norm:      0.37348
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.344
                
Iteration (23300/50001) took 0.073 seconds.
                Mean final reward:        10.0000
                Mean return:              7.0625
                Policy entropy:           0.6126
                Pseudo loss:              2.80622
                Total gradient norm:      0.24596
                Solved trajectories:      31 / 32
                Avg steps to solve:       3.742
                
Iteration (23400/50001) took 0.080 seconds.
                Mean final reward:        10.0000
                Mean return:              6.8750
                Policy entropy:           0.6401
                Pseudo loss:              3.12095
                Total gradient norm:      0.30173
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.125
                
Iteration (23500/50001) took 0.077 seconds.
                Mean final reward:        10.0000
                Mean return:              6.6875
                Policy entropy:           0.6504
                Pseudo loss:              3.54442
                Total gradient norm:      0.34062
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.312
                
Iteration (23600/50001) took 0.106 seconds.
                Mean final reward:        9.6562
                Mean return:              6.9375
                Policy entropy:           0.6316
                Pseudo loss:              2.77877
                Total gradient norm:      0.32961
                Solved trajectories:      31 / 32
                Avg steps to solve:       3.516
                
Iteration (23700/50001) took 0.077 seconds.
                Mean final reward:        8.9688
                Mean return:              5.4688
                Policy entropy:           0.7139
                Pseudo loss:              2.35995
                Total gradient norm:      0.30057
                Solved trajectories:      29 / 32
                Avg steps to solve:       3.931
                
Iteration (23800/50001) took 0.138 seconds.
                Mean final reward:        9.6562
                Mean return:              6.2500
                Policy entropy:           0.6195
                Pseudo loss:              3.00505
                Total gradient norm:      0.29507
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.033
                
Iteration (23900/50001) took 0.067 seconds.
                Mean final reward:        10.0000
                Mean return:              7.4062
                Policy entropy:           0.5716
                Pseudo loss:              2.29398
                Total gradient norm:      0.25816
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.594
                
Iteration (24000/50001) took 0.110 seconds.
                Mean final reward:        10.0000
                Mean return:              6.8438
                Policy entropy:           0.6345
                Pseudo loss:              3.59538
                Total gradient norm:      0.33345
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.156
                
Iteration (24100/50001) took 0.064 seconds.
                Mean final reward:        9.6562
                Mean return:              6.5938
                Policy entropy:           0.6140
                Pseudo loss:              2.63592
                Total gradient norm:      0.22520
                Solved trajectories:      31 / 32
                Avg steps to solve:       3.871
                
Iteration (24200/50001) took 0.123 seconds.
                Mean final reward:        9.6562
                Mean return:              5.8750
                Policy entropy:           0.6377
                Pseudo loss:              4.02646
                Total gradient norm:      0.45514
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.613
                
Iteration (24300/50001) took 0.099 seconds.
                Mean final reward:        10.0000
                Mean return:              7.5938
                Policy entropy:           0.5575
                Pseudo loss:              1.81026
                Total gradient norm:      0.21107
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.406
                
Iteration (24400/50001) took 0.095 seconds.
                Mean final reward:        10.0000
                Mean return:              7.5625
                Policy entropy:           0.5421
                Pseudo loss:              2.13745
                Total gradient norm:      0.23153
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.438
                
Iteration (24500/50001) took 0.140 seconds.
                Mean final reward:        9.6562
                Mean return:              5.9688
                Policy entropy:           0.6794
                Pseudo loss:              3.76531
                Total gradient norm:      0.36730
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.516
                
Iteration (24600/50001) took 0.109 seconds.
                Mean final reward:        10.0000
                Mean return:              6.8750
                Policy entropy:           0.6233
                Pseudo loss:              3.23085
                Total gradient norm:      0.30068
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.125
                
Iteration (24700/50001) took 0.126 seconds.
                Mean final reward:        9.6562
                Mean return:              6.8750
                Policy entropy:           0.6234
                Pseudo loss:              2.28823
                Total gradient norm:      0.27216
                Solved trajectories:      31 / 32
                Avg steps to solve:       3.581
                
Iteration (24800/50001) took 0.131 seconds.
                Mean final reward:        10.0000
                Mean return:              6.9375
                Policy entropy:           0.6420
                Pseudo loss:              3.19057
                Total gradient norm:      0.42444
                Solved trajectories:      31 / 32
                Avg steps to solve:       3.871
                
Iteration (24900/50001) took 0.108 seconds.
                Mean final reward:        10.0000
                Mean return:              7.0000
                Policy entropy:           0.6018
                Pseudo loss:              3.43849
                Total gradient norm:      0.23512
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.000
                
Iteration (25000/50001) took 0.090 seconds.
                Mean final reward:        9.6562
                Mean return:              6.7500
                Policy entropy:           0.6563
                Pseudo loss:              2.33345
                Total gradient norm:      0.28016
                Solved trajectories:      31 / 32
                Avg steps to solve:       3.710
                
Iteration (25100/50001) took 0.138 seconds.
                Mean final reward:        9.6562
                Mean return:              6.2188
                Policy entropy:           0.6542
                Pseudo loss:              3.28812
                Total gradient norm:      0.33463
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.258
                
Iteration (25200/50001) took 0.115 seconds.
                Mean final reward:        10.0000
                Mean return:              7.1562
                Policy entropy:           0.6050
                Pseudo loss:              2.84238
                Total gradient norm:      0.28877
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.844
                
Iteration (25300/50001) took 0.089 seconds.
                Mean final reward:        9.6562
                Mean return:              6.4062
                Policy entropy:           0.6492
                Pseudo loss:              3.00644
                Total gradient norm:      0.33980
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.065
                
Iteration (25400/50001) took 0.086 seconds.
                Mean final reward:        10.0000
                Mean return:              6.7812
                Policy entropy:           0.6025
                Pseudo loss:              3.72861
                Total gradient norm:      0.33406
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.032
                
Iteration (25500/50001) took 0.093 seconds.
                Mean final reward:        10.0000
                Mean return:              6.8125
                Policy entropy:           0.6136
                Pseudo loss:              3.04041
                Total gradient norm:      0.26559
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.188
                
Iteration (25600/50001) took 0.104 seconds.
                Mean final reward:        10.0000
                Mean return:              6.5625
                Policy entropy:           0.6492
                Pseudo loss:              3.07949
                Total gradient norm:      0.31573
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.438
                
Iteration (25700/50001) took 0.047 seconds.
                Mean final reward:        10.0000
                Mean return:              7.0000
                Policy entropy:           0.6293
                Pseudo loss:              2.82862
                Total gradient norm:      0.27309
                Solved trajectories:      31 / 32
                Avg steps to solve:       3.806
                
Iteration (25800/50001) took 0.085 seconds.
                Mean final reward:        10.0000
                Mean return:              6.9062
                Policy entropy:           0.6108
                Pseudo loss:              2.94914
                Total gradient norm:      0.37526
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.094
                
Iteration (25900/50001) took 0.132 seconds.
                Mean final reward:        10.0000
                Mean return:              6.6875
                Policy entropy:           0.6385
                Pseudo loss:              3.53119
                Total gradient norm:      0.30923
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.129
                
Iteration (26000/50001) took 0.077 seconds.
                Mean final reward:        10.0000
                Mean return:              7.0312
                Policy entropy:           0.5891
                Pseudo loss:              2.71325
                Total gradient norm:      0.30254
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.969
                
Iteration (26100/50001) took 0.102 seconds.
                Mean final reward:        10.0000
                Mean return:              7.0625
                Policy entropy:           0.6148
                Pseudo loss:              2.68429
                Total gradient norm:      0.26404
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.938
                
Iteration (26200/50001) took 0.083 seconds.
                Mean final reward:        10.0000
                Mean return:              6.6250
                Policy entropy:           0.6469
                Pseudo loss:              4.05380
                Total gradient norm:      0.38300
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.375
                
Iteration (26300/50001) took 0.146 seconds.
                Mean final reward:        10.0000
                Mean return:              6.9688
                Policy entropy:           0.5835
                Pseudo loss:              2.99227
                Total gradient norm:      0.29907
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.031
                
Iteration (26400/50001) took 0.136 seconds.
                Mean final reward:        10.0000
                Mean return:              6.7188
                Policy entropy:           0.6408
                Pseudo loss:              3.09828
                Total gradient norm:      0.30641
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.281
                
Iteration (26500/50001) took 0.134 seconds.
                Mean final reward:        10.0000
                Mean return:              6.9062
                Policy entropy:           0.5889
                Pseudo loss:              3.48747
                Total gradient norm:      0.35177
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.094
                
Iteration (26600/50001) took 0.056 seconds.
                Mean final reward:        10.0000
                Mean return:              7.1562
                Policy entropy:           0.5779
                Pseudo loss:              2.72989
                Total gradient norm:      0.26221
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.844
                
Iteration (26700/50001) took 0.103 seconds.
                Mean final reward:        10.0000
                Mean return:              7.0000
                Policy entropy:           0.6080
                Pseudo loss:              2.68861
                Total gradient norm:      0.33178
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.000
                
Iteration (26800/50001) took 0.071 seconds.
                Mean final reward:        10.0000
                Mean return:              6.7188
                Policy entropy:           0.6075
                Pseudo loss:              3.07531
                Total gradient norm:      0.24933
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.281
                
Iteration (26900/50001) took 0.095 seconds.
                Mean final reward:        10.0000
                Mean return:              7.0625
                Policy entropy:           0.6005
                Pseudo loss:              2.89976
                Total gradient norm:      0.25378
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.938
                
Iteration (27000/50001) took 0.119 seconds.
                Mean final reward:        10.0000
                Mean return:              7.3125
                Policy entropy:           0.5793
                Pseudo loss:              2.70857
                Total gradient norm:      0.25739
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.688
                
Iteration (27100/50001) took 0.128 seconds.
                Mean final reward:        9.6562
                Mean return:              6.1250
                Policy entropy:           0.6558
                Pseudo loss:              3.41130
                Total gradient norm:      0.33566
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.355
                
Iteration (27200/50001) took 0.151 seconds.
                Mean final reward:        9.6562
                Mean return:              6.0938
                Policy entropy:           0.6669
                Pseudo loss:              3.27407
                Total gradient norm:      0.30812
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.200
                
Iteration (27300/50001) took 0.064 seconds.
                Mean final reward:        10.0000
                Mean return:              6.9688
                Policy entropy:           0.5944
                Pseudo loss:              3.22567
                Total gradient norm:      0.27408
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.031
                
Iteration (27400/50001) took 0.073 seconds.
                Mean final reward:        10.0000
                Mean return:              6.9375
                Policy entropy:           0.5979
                Pseudo loss:              2.96540
                Total gradient norm:      0.29731
                Solved trajectories:      30 / 32
                Avg steps to solve:       3.667
                
Iteration (27500/50001) took 0.102 seconds.
                Mean final reward:        10.0000
                Mean return:              7.0625
                Policy entropy:           0.6044
                Pseudo loss:              2.60932
                Total gradient norm:      0.13664
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.938
                
Iteration (27600/50001) took 0.105 seconds.
                Mean final reward:        9.6562
                Mean return:              6.0938
                Policy entropy:           0.6642
                Pseudo loss:              3.60260
                Total gradient norm:      0.36251
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.387
                
Iteration (27700/50001) took 0.105 seconds.
                Mean final reward:        10.0000
                Mean return:              7.0625
                Policy entropy:           0.5937
                Pseudo loss:              2.80834
                Total gradient norm:      0.40662
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.938
                
Iteration (27800/50001) took 0.059 seconds.
                Mean final reward:        10.0000
                Mean return:              7.1250
                Policy entropy:           0.5862
                Pseudo loss:              2.23191
                Total gradient norm:      0.21291
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.875
                
Iteration (27900/50001) took 0.113 seconds.
                Mean final reward:        9.6562
                Mean return:              5.7500
                Policy entropy:           0.6580
                Pseudo loss:              4.06242
                Total gradient norm:      0.41886
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.567
                
Iteration (28000/50001) took 0.091 seconds.
                Mean final reward:        10.0000
                Mean return:              7.2500
                Policy entropy:           0.5994
                Pseudo loss:              2.71156
                Total gradient norm:      0.28056
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.750
                
Iteration (28100/50001) took 0.116 seconds.
                Mean final reward:        10.0000
                Mean return:              6.6562
                Policy entropy:           0.6480
                Pseudo loss:              3.59700
                Total gradient norm:      0.32536
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.344
                
Iteration (28200/50001) took 0.134 seconds.
                Mean final reward:        10.0000
                Mean return:              6.1250
                Policy entropy:           0.6773
                Pseudo loss:              4.34628
                Total gradient norm:      0.39458
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.875
                
Iteration (28300/50001) took 0.130 seconds.
                Mean final reward:        9.6562
                Mean return:              6.5625
                Policy entropy:           0.6338
                Pseudo loss:              3.29985
                Total gradient norm:      0.38572
                Solved trajectories:      31 / 32
                Avg steps to solve:       3.903
                
Iteration (28400/50001) took 0.107 seconds.
                Mean final reward:        9.3125
                Mean return:              5.8438
                Policy entropy:           0.6768
                Pseudo loss:              2.78493
                Total gradient norm:      0.25299
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.100
                
Iteration (28500/50001) took 0.126 seconds.
                Mean final reward:        10.0000
                Mean return:              7.0938
                Policy entropy:           0.5950
                Pseudo loss:              2.39104
                Total gradient norm:      0.20516
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.906
                
Iteration (28600/50001) took 0.109 seconds.
                Mean final reward:        9.6562
                Mean return:              6.1562
                Policy entropy:           0.6662
                Pseudo loss:              3.24540
                Total gradient norm:      0.32978
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.323
                
Iteration (28700/50001) took 0.126 seconds.
                Mean final reward:        10.0000
                Mean return:              6.9375
                Policy entropy:           0.6147
                Pseudo loss:              3.09787
                Total gradient norm:      0.28594
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.062
                
Iteration (28800/50001) took 0.102 seconds.
                Mean final reward:        10.0000
                Mean return:              6.9688
                Policy entropy:           0.6248
                Pseudo loss:              3.06072
                Total gradient norm:      0.35672
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.031
                
Iteration (28900/50001) took 0.074 seconds.
                Mean final reward:        10.0000
                Mean return:              6.3125
                Policy entropy:           0.6338
                Pseudo loss:              4.10336
                Total gradient norm:      0.32141
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.688
                
Iteration (29000/50001) took 0.123 seconds.
                Mean final reward:        10.0000
                Mean return:              5.7188
                Policy entropy:           0.6832
                Pseudo loss:              5.27168
                Total gradient norm:      0.39607
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.129
                
Iteration (29100/50001) took 0.115 seconds.
                Mean final reward:        10.0000
                Mean return:              6.5312
                Policy entropy:           0.6461
                Pseudo loss:              4.02211
                Total gradient norm:      0.38384
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.290
                
Iteration (29200/50001) took 0.157 seconds.
                Mean final reward:        10.0000
                Mean return:              6.9375
                Policy entropy:           0.6022
                Pseudo loss:              2.96006
                Total gradient norm:      0.30381
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.062
                
Iteration (29300/50001) took 0.114 seconds.
                Mean final reward:        10.0000
                Mean return:              7.4688
                Policy entropy:           0.5728
                Pseudo loss:              2.05687
                Total gradient norm:      0.23129
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.531
                
Iteration (29400/50001) took 0.114 seconds.
                Mean final reward:        10.0000
                Mean return:              6.8125
                Policy entropy:           0.6225
                Pseudo loss:              3.37094
                Total gradient norm:      0.30831
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.188
                
Iteration (29500/50001) took 0.071 seconds.
                Mean final reward:        10.0000
                Mean return:              6.5625
                Policy entropy:           0.6383
                Pseudo loss:              3.68332
                Total gradient norm:      0.35381
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.438
                
Iteration (29600/50001) took 0.060 seconds.
                Mean final reward:        9.3125
                Mean return:              6.3750
                Policy entropy:           0.6324
                Pseudo loss:              2.01497
                Total gradient norm:      0.22365
                Solved trajectories:      30 / 32
                Avg steps to solve:       3.533
                
Iteration (29700/50001) took 0.089 seconds.
                Mean final reward:        10.0000
                Mean return:              7.0938
                Policy entropy:           0.5895
                Pseudo loss:              2.89875
                Total gradient norm:      0.31724
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.906
                
Iteration (29800/50001) took 0.113 seconds.
                Mean final reward:        9.6562
                Mean return:              6.0312
                Policy entropy:           0.6539
                Pseudo loss:              3.50597
                Total gradient norm:      0.40685
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.452
                
Iteration (29900/50001) took 0.067 seconds.
                Mean final reward:        9.6562
                Mean return:              6.3438
                Policy entropy:           0.6523
                Pseudo loss:              3.28117
                Total gradient norm:      0.34733
                Solved trajectories:      30 / 32
                Avg steps to solve:       3.933
                
Iteration (30000/50001) took 0.067 seconds.
                Mean final reward:        10.0000
                Mean return:              6.9688
                Policy entropy:           0.6041
                Pseudo loss:              2.70801
                Total gradient norm:      0.23621
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.031
                
Iteration (30100/50001) took 0.082 seconds.
                Mean final reward:        10.0000
                Mean return:              7.0938
                Policy entropy:           0.6098
                Pseudo loss:              2.82829
                Total gradient norm:      0.25554
                Solved trajectories:      31 / 32
                Avg steps to solve:       3.710
                
Iteration (30200/50001) took 0.116 seconds.
                Mean final reward:        9.6562
                Mean return:              6.5000
                Policy entropy:           0.6367
                Pseudo loss:              2.83193
                Total gradient norm:      0.24347
                Solved trajectories:      31 / 32
                Avg steps to solve:       3.968
                
Iteration (30300/50001) took 0.069 seconds.
                Mean final reward:        10.0000
                Mean return:              7.3750
                Policy entropy:           0.5680
                Pseudo loss:              2.20586
                Total gradient norm:      0.26196
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.625
                
Iteration (30400/50001) took 0.067 seconds.
                Mean final reward:        10.0000
                Mean return:              7.1562
                Policy entropy:           0.5920
                Pseudo loss:              2.72233
                Total gradient norm:      0.26010
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.844
                
Iteration (30500/50001) took 0.082 seconds.
                Mean final reward:        10.0000
                Mean return:              6.7500
                Policy entropy:           0.6137
                Pseudo loss:              3.43272
                Total gradient norm:      0.25300
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.065
                
Iteration (30600/50001) took 0.140 seconds.
                Mean final reward:        10.0000
                Mean return:              6.5625
                Policy entropy:           0.6252
                Pseudo loss:              3.96574
                Total gradient norm:      0.36751
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.258
                
Iteration (30700/50001) took 0.075 seconds.
                Mean final reward:        10.0000
                Mean return:              6.7188
                Policy entropy:           0.6200
                Pseudo loss:              3.17556
                Total gradient norm:      0.26759
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.281
                
Iteration (30800/50001) took 0.098 seconds.
                Mean final reward:        10.0000
                Mean return:              7.1250
                Policy entropy:           0.5809
                Pseudo loss:              2.47715
                Total gradient norm:      0.26744
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.875
                
Iteration (30900/50001) took 0.103 seconds.
                Mean final reward:        10.0000
                Mean return:              6.0938
                Policy entropy:           0.6494
                Pseudo loss:              4.42386
                Total gradient norm:      0.40046
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.906
                
Iteration (31000/50001) took 0.125 seconds.
                Mean final reward:        9.6562
                Mean return:              6.1250
                Policy entropy:           0.6708
                Pseudo loss:              3.30795
                Total gradient norm:      0.33771
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.167
                
Iteration (31100/50001) took 0.072 seconds.
                Mean final reward:        10.0000
                Mean return:              6.7812
                Policy entropy:           0.6117
                Pseudo loss:              2.99149
                Total gradient norm:      0.27275
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.032
                
Iteration (31200/50001) took 0.119 seconds.
                Mean final reward:        10.0000
                Mean return:              7.3750
                Policy entropy:           0.5695
                Pseudo loss:              2.09746
                Total gradient norm:      0.27961
                Solved trajectories:      31 / 32
                Avg steps to solve:       3.419
                
Iteration (31300/50001) took 0.130 seconds.
                Mean final reward:        10.0000
                Mean return:              6.4062
                Policy entropy:           0.6584
                Pseudo loss:              3.92595
                Total gradient norm:      0.37592
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.594
                
Iteration (31400/50001) took 0.066 seconds.
                Mean final reward:        10.0000
                Mean return:              6.9375
                Policy entropy:           0.6105
                Pseudo loss:              2.78799
                Total gradient norm:      0.27180
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.062
                
Iteration (31500/50001) took 0.062 seconds.
                Mean final reward:        9.6562
                Mean return:              6.3438
                Policy entropy:           0.6445
                Pseudo loss:              3.14469
                Total gradient norm:      0.31365
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.129
                
Iteration (31600/50001) took 0.119 seconds.
                Mean final reward:        10.0000
                Mean return:              7.0625
                Policy entropy:           0.5941
                Pseudo loss:              2.73622
                Total gradient norm:      0.21914
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.938
                
Iteration (31700/50001) took 0.076 seconds.
                Mean final reward:        10.0000
                Mean return:              6.7188
                Policy entropy:           0.6331
                Pseudo loss:              3.10452
                Total gradient norm:      0.25588
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.281
                
Iteration (31800/50001) took 0.163 seconds.
                Mean final reward:        10.0000
                Mean return:              6.3125
                Policy entropy:           0.6468
                Pseudo loss:              3.88829
                Total gradient norm:      0.47073
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.516
                
Iteration (31900/50001) took 0.140 seconds.
                Mean final reward:        10.0000
                Mean return:              6.8750
                Policy entropy:           0.6004
                Pseudo loss:              2.98285
                Total gradient norm:      0.24914
                Solved trajectories:      31 / 32
                Avg steps to solve:       3.935
                
Iteration (32000/50001) took 0.070 seconds.
                Mean final reward:        9.6562
                Mean return:              6.5938
                Policy entropy:           0.6350
                Pseudo loss:              3.05214
                Total gradient norm:      0.33193
                Solved trajectories:      31 / 32
                Avg steps to solve:       3.871
                
Iteration (32100/50001) took 0.233 seconds.
                Mean final reward:        9.3125
                Mean return:              5.9688
                Policy entropy:           0.6440
                Pseudo loss:              2.37300
                Total gradient norm:      0.29259
                Solved trajectories:      30 / 32
                Avg steps to solve:       3.967
                
Iteration (32200/50001) took 0.113 seconds.
                Mean final reward:        10.0000
                Mean return:              6.7188
                Policy entropy:           0.6213
                Pseudo loss:              3.19059
                Total gradient norm:      0.26040
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.097
                
Iteration (32300/50001) took 0.119 seconds.
                Mean final reward:        10.0000
                Mean return:              7.1562
                Policy entropy:           0.6163
                Pseudo loss:              2.78730
                Total gradient norm:      0.24138
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.844
                
Iteration (32400/50001) took 0.094 seconds.
                Mean final reward:        10.0000
                Mean return:              6.4375
                Policy entropy:           0.6265
                Pseudo loss:              3.67792
                Total gradient norm:      0.37764
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.562
                
Iteration (32500/50001) took 0.074 seconds.
                Mean final reward:        10.0000
                Mean return:              6.7500
                Policy entropy:           0.6189
                Pseudo loss:              3.29293
                Total gradient norm:      0.26705
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.250
                
Iteration (32600/50001) took 0.067 seconds.
                Mean final reward:        10.0000
                Mean return:              7.2812
                Policy entropy:           0.5565
                Pseudo loss:              2.57238
                Total gradient norm:      0.27050
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.719
                
Iteration (32700/50001) took 0.130 seconds.
                Mean final reward:        10.0000
                Mean return:              7.0000
                Policy entropy:           0.5966
                Pseudo loss:              2.70766
                Total gradient norm:      0.21892
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.000
                
Iteration (32800/50001) took 0.135 seconds.
                Mean final reward:        10.0000
                Mean return:              7.3125
                Policy entropy:           0.5839
                Pseudo loss:              2.45720
                Total gradient norm:      0.23697
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.688
                
Iteration (32900/50001) took 0.065 seconds.
                Mean final reward:        10.0000
                Mean return:              7.3125
                Policy entropy:           0.5528
                Pseudo loss:              2.89618
                Total gradient norm:      0.25483
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.688
                
Iteration (33000/50001) took 0.063 seconds.
                Mean final reward:        10.0000
                Mean return:              6.5938
                Policy entropy:           0.6271
                Pseudo loss:              3.69536
                Total gradient norm:      0.28276
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.406
                
Iteration (33100/50001) took 0.142 seconds.
                Mean final reward:        10.0000
                Mean return:              6.7500
                Policy entropy:           0.5964
                Pseudo loss:              3.27147
                Total gradient norm:      0.25846
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.250
                
Iteration (33200/50001) took 0.070 seconds.
                Mean final reward:        10.0000
                Mean return:              6.8438
                Policy entropy:           0.5923
                Pseudo loss:              2.90883
                Total gradient norm:      0.25340
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.156
                
Iteration (33300/50001) took 0.104 seconds.
                Mean final reward:        10.0000
                Mean return:              6.7500
                Policy entropy:           0.5982
                Pseudo loss:              3.11189
                Total gradient norm:      0.32797
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.250
                
Iteration (33400/50001) took 0.047 seconds.
                Mean final reward:        9.6562
                Mean return:              6.3125
                Policy entropy:           0.6666
                Pseudo loss:              3.52446
                Total gradient norm:      0.41628
                Solved trajectories:      30 / 32
                Avg steps to solve:       3.967
                
Iteration (33500/50001) took 0.111 seconds.
                Mean final reward:        9.6562
                Mean return:              5.9375
                Policy entropy:           0.6533
                Pseudo loss:              3.83174
                Total gradient norm:      0.33599
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.367
                
Iteration (33600/50001) took 0.090 seconds.
                Mean final reward:        10.0000
                Mean return:              6.8438
                Policy entropy:           0.6122
                Pseudo loss:              3.26991
                Total gradient norm:      0.32285
                Solved trajectories:      31 / 32
                Avg steps to solve:       3.968
                
Iteration (33700/50001) took 0.090 seconds.
                Mean final reward:        10.0000
                Mean return:              6.7812
                Policy entropy:           0.6312
                Pseudo loss:              3.44339
                Total gradient norm:      0.31601
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.219
                
Iteration (33800/50001) took 0.086 seconds.
                Mean final reward:        10.0000
                Mean return:              7.1875
                Policy entropy:           0.5778
                Pseudo loss:              2.65200
                Total gradient norm:      0.32763
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.812
                
Iteration (33900/50001) took 0.067 seconds.
                Mean final reward:        10.0000
                Mean return:              7.0000
                Policy entropy:           0.5955
                Pseudo loss:              2.39872
                Total gradient norm:      0.21528
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.000
                
Iteration (34000/50001) took 0.077 seconds.
                Mean final reward:        9.6562
                Mean return:              6.3438
                Policy entropy:           0.6552
                Pseudo loss:              3.03867
                Total gradient norm:      0.28936
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.129
                
Iteration (34100/50001) took 0.120 seconds.
                Mean final reward:        10.0000
                Mean return:              6.9375
                Policy entropy:           0.5932
                Pseudo loss:              3.30005
                Total gradient norm:      0.29373
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.062
                
Iteration (34200/50001) took 0.108 seconds.
                Mean final reward:        9.6562
                Mean return:              6.3750
                Policy entropy:           0.6643
                Pseudo loss:              3.16442
                Total gradient norm:      0.27054
                Solved trajectories:      30 / 32
                Avg steps to solve:       3.900
                
Iteration (34300/50001) took 0.087 seconds.
                Mean final reward:        9.6562
                Mean return:              6.3438
                Policy entropy:           0.6472
                Pseudo loss:              2.92274
                Total gradient norm:      0.28146
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.129
                
Iteration (34400/50001) took 0.098 seconds.
                Mean final reward:        10.0000
                Mean return:              6.3750
                Policy entropy:           0.6373
                Pseudo loss:              3.79111
                Total gradient norm:      0.32227
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.625
                
Iteration (34500/50001) took 0.128 seconds.
                Mean final reward:        10.0000
                Mean return:              6.8438
                Policy entropy:           0.5879
                Pseudo loss:              3.14210
                Total gradient norm:      0.43068
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.156
                
Iteration (34600/50001) took 0.064 seconds.
                Mean final reward:        9.6562
                Mean return:              6.7500
                Policy entropy:           0.5961
                Pseudo loss:              2.59944
                Total gradient norm:      0.26112
                Solved trajectories:      31 / 32
                Avg steps to solve:       3.710
                
Iteration (34700/50001) took 0.074 seconds.
                Mean final reward:        9.3125
                Mean return:              5.3438
                Policy entropy:           0.6842
                Pseudo loss:              3.82710
                Total gradient norm:      0.44114
                Solved trajectories:      28 / 32
                Avg steps to solve:       4.250
                
Iteration (34800/50001) took 0.138 seconds.
                Mean final reward:        10.0000
                Mean return:              6.3750
                Policy entropy:           0.6411
                Pseudo loss:              3.65074
                Total gradient norm:      0.24780
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.452
                
Iteration (34900/50001) took 0.072 seconds.
                Mean final reward:        10.0000
                Mean return:              6.9688
                Policy entropy:           0.6233
                Pseudo loss:              2.54251
                Total gradient norm:      0.21877
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.031
                
Iteration (35000/50001) took 0.073 seconds.
                Mean final reward:        10.0000
                Mean return:              7.0000
                Policy entropy:           0.6062
                Pseudo loss:              3.04650
                Total gradient norm:      0.26256
                Solved trajectories:      31 / 32
                Avg steps to solve:       3.806
                
Iteration (35100/50001) took 0.106 seconds.
                Mean final reward:        10.0000
                Mean return:              7.2812
                Policy entropy:           0.5616
                Pseudo loss:              2.34773
                Total gradient norm:      0.24659
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.719
                
Iteration (35200/50001) took 0.066 seconds.
                Mean final reward:        10.0000
                Mean return:              6.9375
                Policy entropy:           0.6116
                Pseudo loss:              3.04892
                Total gradient norm:      0.32096
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.062
                
Iteration (35300/50001) took 0.068 seconds.
                Mean final reward:        10.0000
                Mean return:              6.8750
                Policy entropy:           0.6173
                Pseudo loss:              2.83580
                Total gradient norm:      0.24620
                Solved trajectories:      31 / 32
                Avg steps to solve:       3.935
                
Iteration (35400/50001) took 0.076 seconds.
                Mean final reward:        10.0000
                Mean return:              6.9688
                Policy entropy:           0.6083
                Pseudo loss:              2.86913
                Total gradient norm:      0.33311
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.031
                
Iteration (35500/50001) took 0.057 seconds.
                Mean final reward:        9.6562
                Mean return:              6.3438
                Policy entropy:           0.6261
                Pseudo loss:              3.12563
                Total gradient norm:      0.34099
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.129
                
Iteration (35600/50001) took 0.116 seconds.
                Mean final reward:        10.0000
                Mean return:              6.6562
                Policy entropy:           0.6320
                Pseudo loss:              3.60781
                Total gradient norm:      0.36865
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.161
                
Iteration (35700/50001) took 0.077 seconds.
                Mean final reward:        10.0000
                Mean return:              7.2812
                Policy entropy:           0.6161
                Pseudo loss:              2.63742
                Total gradient norm:      0.30177
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.719
                
Iteration (35800/50001) took 0.110 seconds.
                Mean final reward:        10.0000
                Mean return:              6.9375
                Policy entropy:           0.6311
                Pseudo loss:              2.98492
                Total gradient norm:      0.30411
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.062
                
Iteration (35900/50001) took 0.073 seconds.
                Mean final reward:        9.3125
                Mean return:              6.5312
                Policy entropy:           0.5932
                Pseudo loss:              2.14593
                Total gradient norm:      0.27899
                Solved trajectories:      30 / 32
                Avg steps to solve:       3.367
                
Iteration (36000/50001) took 0.043 seconds.
                Mean final reward:        10.0000
                Mean return:              7.3438
                Policy entropy:           0.5743
                Pseudo loss:              2.15519
                Total gradient norm:      0.17649
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.656
                
Iteration (36100/50001) took 0.133 seconds.
                Mean final reward:        10.0000
                Mean return:              6.3750
                Policy entropy:           0.6304
                Pseudo loss:              3.66177
                Total gradient norm:      0.23416
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.625
                
Iteration (36200/50001) took 0.111 seconds.
                Mean final reward:        10.0000
                Mean return:              7.3750
                Policy entropy:           0.5808
                Pseudo loss:              2.07473
                Total gradient norm:      0.23198
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.625
                
Iteration (36300/50001) took 0.083 seconds.
                Mean final reward:        10.0000
                Mean return:              6.7188
                Policy entropy:           0.6033
                Pseudo loss:              2.98175
                Total gradient norm:      0.28507
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.281
                
Iteration (36400/50001) took 0.090 seconds.
                Mean final reward:        10.0000
                Mean return:              6.7500
                Policy entropy:           0.6204
                Pseudo loss:              3.29552
                Total gradient norm:      0.40247
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.250
                
Iteration (36500/50001) took 0.108 seconds.
                Mean final reward:        10.0000
                Mean return:              6.7500
                Policy entropy:           0.6202
                Pseudo loss:              3.59989
                Total gradient norm:      0.34586
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.250
                
Iteration (36600/50001) took 0.068 seconds.
                Mean final reward:        10.0000
                Mean return:              6.4062
                Policy entropy:           0.6229
                Pseudo loss:              3.96737
                Total gradient norm:      0.38781
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.594
                
Iteration (36700/50001) took 0.074 seconds.
                Mean final reward:        10.0000
                Mean return:              6.3750
                Policy entropy:           0.6553
                Pseudo loss:              4.07996
                Total gradient norm:      0.32931
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.452
                
Iteration (36800/50001) took 0.100 seconds.
                Mean final reward:        10.0000
                Mean return:              6.8125
                Policy entropy:           0.6170
                Pseudo loss:              3.05820
                Total gradient norm:      0.29897
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.188
                
Iteration (36900/50001) took 0.060 seconds.
                Mean final reward:        9.6562
                Mean return:              6.7812
                Policy entropy:           0.5764
                Pseudo loss:              2.29703
                Total gradient norm:      0.22867
                Solved trajectories:      31 / 32
                Avg steps to solve:       3.677
                
Iteration (37000/50001) took 0.141 seconds.
                Mean final reward:        10.0000
                Mean return:              6.3750
                Policy entropy:           0.6238
                Pseudo loss:              3.51402
                Total gradient norm:      0.34058
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.452
                
Iteration (37100/50001) took 0.062 seconds.
                Mean final reward:        10.0000
                Mean return:              7.5312
                Policy entropy:           0.5438
                Pseudo loss:              1.74066
                Total gradient norm:      0.22043
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.469
                
Iteration (37200/50001) took 0.073 seconds.
                Mean final reward:        9.6562
                Mean return:              6.8750
                Policy entropy:           0.6188
                Pseudo loss:              2.34377
                Total gradient norm:      0.30255
                Solved trajectories:      31 / 32
                Avg steps to solve:       3.581
                
Iteration (37300/50001) took 0.091 seconds.
                Mean final reward:        10.0000
                Mean return:              6.9062
                Policy entropy:           0.6123
                Pseudo loss:              2.86978
                Total gradient norm:      0.29304
                Solved trajectories:      31 / 32
                Avg steps to solve:       3.903
                
Iteration (37400/50001) took 0.070 seconds.
                Mean final reward:        10.0000
                Mean return:              7.0938
                Policy entropy:           0.5880
                Pseudo loss:              2.59229
                Total gradient norm:      0.22210
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.906
                
Iteration (37500/50001) took 0.087 seconds.
                Mean final reward:        10.0000
                Mean return:              6.6875
                Policy entropy:           0.5878
                Pseudo loss:              3.48468
                Total gradient norm:      0.34341
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.129
                
Iteration (37600/50001) took 0.071 seconds.
                Mean final reward:        10.0000
                Mean return:              7.2812
                Policy entropy:           0.5543
                Pseudo loss:              2.63506
                Total gradient norm:      0.27871
                Solved trajectories:      31 / 32
                Avg steps to solve:       3.516
                
Iteration (37700/50001) took 0.128 seconds.
                Mean final reward:        10.0000
                Mean return:              6.8125
                Policy entropy:           0.6407
                Pseudo loss:              3.05484
                Total gradient norm:      0.32870
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.188
                
Iteration (37800/50001) took 0.067 seconds.
                Mean final reward:        10.0000
                Mean return:              6.9688
                Policy entropy:           0.6283
                Pseudo loss:              3.49161
                Total gradient norm:      0.38021
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.031
                
Iteration (37900/50001) took 0.112 seconds.
                Mean final reward:        10.0000
                Mean return:              6.6875
                Policy entropy:           0.6332
                Pseudo loss:              3.86511
                Total gradient norm:      0.37885
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.312
                
Iteration (38000/50001) took 0.081 seconds.
                Mean final reward:        9.6562
                Mean return:              5.8438
                Policy entropy:           0.6752
                Pseudo loss:              3.70560
                Total gradient norm:      0.39839
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.645
                
Iteration (38100/50001) took 0.066 seconds.
                Mean final reward:        9.3125
                Mean return:              6.2500
                Policy entropy:           0.6251
                Pseudo loss:              2.40329
                Total gradient norm:      0.24069
                Solved trajectories:      30 / 32
                Avg steps to solve:       3.667
                
Iteration (38200/50001) took 0.081 seconds.
                Mean final reward:        10.0000
                Mean return:              6.8438
                Policy entropy:           0.6293
                Pseudo loss:              2.85863
                Total gradient norm:      0.24929
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.156
                
Iteration (38300/50001) took 0.074 seconds.
                Mean final reward:        10.0000
                Mean return:              6.8438
                Policy entropy:           0.6167
                Pseudo loss:              3.27212
                Total gradient norm:      0.32376
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.156
                
Iteration (38400/50001) took 0.072 seconds.
                Mean final reward:        10.0000
                Mean return:              7.0625
                Policy entropy:           0.6011
                Pseudo loss:              2.52885
                Total gradient norm:      0.26496
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.938
                
Iteration (38500/50001) took 0.116 seconds.
                Mean final reward:        10.0000
                Mean return:              6.4062
                Policy entropy:           0.6735
                Pseudo loss:              3.67864
                Total gradient norm:      0.32822
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.594
                
Iteration (38600/50001) took 0.114 seconds.
                Mean final reward:        10.0000
                Mean return:              6.6562
                Policy entropy:           0.6476
                Pseudo loss:              3.75748
                Total gradient norm:      0.43911
                Solved trajectories:      30 / 32
                Avg steps to solve:       3.967
                
Iteration (38700/50001) took 0.083 seconds.
                Mean final reward:        10.0000
                Mean return:              5.8750
                Policy entropy:           0.6541
                Pseudo loss:              4.61539
                Total gradient norm:      0.68383
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.968
                
Iteration (38800/50001) took 0.092 seconds.
                Mean final reward:        10.0000
                Mean return:              7.1250
                Policy entropy:           0.5950
                Pseudo loss:              2.40578
                Total gradient norm:      0.24348
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.875
                
Iteration (38900/50001) took 0.075 seconds.
                Mean final reward:        10.0000
                Mean return:              7.4062
                Policy entropy:           0.5783
                Pseudo loss:              2.30020
                Total gradient norm:      0.25787
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.594
                
Iteration (39000/50001) took 0.081 seconds.
                Mean final reward:        10.0000
                Mean return:              7.3438
                Policy entropy:           0.5762
                Pseudo loss:              2.63272
                Total gradient norm:      0.22774
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.656
                
Iteration (39100/50001) took 0.061 seconds.
                Mean final reward:        10.0000
                Mean return:              6.2812
                Policy entropy:           0.6591
                Pseudo loss:              4.05085
                Total gradient norm:      0.28166
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.719
                
Iteration (39200/50001) took 0.041 seconds.
                Mean final reward:        10.0000
                Mean return:              7.3750
                Policy entropy:           0.5724
                Pseudo loss:              2.39058
                Total gradient norm:      0.24981
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.625
                
Iteration (39300/50001) took 0.130 seconds.
                Mean final reward:        10.0000
                Mean return:              7.0312
                Policy entropy:           0.5972
                Pseudo loss:              2.63546
                Total gradient norm:      0.24371
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.969
                
Iteration (39400/50001) took 0.167 seconds.
                Mean final reward:        10.0000
                Mean return:              6.6875
                Policy entropy:           0.6337
                Pseudo loss:              2.97523
                Total gradient norm:      0.25044
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.129
                
Iteration (39500/50001) took 0.114 seconds.
                Mean final reward:        9.6562
                Mean return:              6.5625
                Policy entropy:           0.6124
                Pseudo loss:              2.58690
                Total gradient norm:      0.24078
                Solved trajectories:      31 / 32
                Avg steps to solve:       3.903
                
Iteration (39600/50001) took 0.088 seconds.
                Mean final reward:        10.0000
                Mean return:              7.0312
                Policy entropy:           0.6083
                Pseudo loss:              2.57920
                Total gradient norm:      0.22652
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.969
                
Iteration (39700/50001) took 0.159 seconds.
                Mean final reward:        10.0000
                Mean return:              6.9688
                Policy entropy:           0.6043
                Pseudo loss:              2.83292
                Total gradient norm:      0.24833
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.031
                
Iteration (39800/50001) took 0.173 seconds.
                Mean final reward:        9.6562
                Mean return:              6.1250
                Policy entropy:           0.6362
                Pseudo loss:              2.79308
                Total gradient norm:      0.32537
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.355
                
Iteration (39900/50001) took 0.099 seconds.
                Mean final reward:        10.0000
                Mean return:              7.2812
                Policy entropy:           0.5836
                Pseudo loss:              2.01340
                Total gradient norm:      0.21295
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.719
                
Iteration (40000/50001) took 0.144 seconds.
                Mean final reward:        10.0000
                Mean return:              6.5000
                Policy entropy:           0.6309
                Pseudo loss:              3.15297
                Total gradient norm:      0.25689
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.500
                
Iteration (40100/50001) took 0.117 seconds.
                Mean final reward:        10.0000
                Mean return:              6.8438
                Policy entropy:           0.5907
                Pseudo loss:              2.96076
                Total gradient norm:      0.29216
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.156
                
Iteration (40200/50001) took 0.149 seconds.
                Mean final reward:        9.6562
                Mean return:              6.1875
                Policy entropy:           0.6523
                Pseudo loss:              2.99618
                Total gradient norm:      0.32821
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.290
                
Iteration (40300/50001) took 0.093 seconds.
                Mean final reward:        9.6562
                Mean return:              6.4062
                Policy entropy:           0.6271
                Pseudo loss:              2.62131
                Total gradient norm:      0.28413
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.065
                
Iteration (40400/50001) took 0.138 seconds.
                Mean final reward:        10.0000
                Mean return:              6.9688
                Policy entropy:           0.6049
                Pseudo loss:              2.78229
                Total gradient norm:      0.30687
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.031
                
Iteration (40500/50001) took 0.047 seconds.
                Mean final reward:        10.0000
                Mean return:              6.5625
                Policy entropy:           0.6141
                Pseudo loss:              3.52882
                Total gradient norm:      0.36045
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.067
                
Iteration (40600/50001) took 0.082 seconds.
                Mean final reward:        10.0000
                Mean return:              7.6875
                Policy entropy:           0.5549
                Pseudo loss:              1.85981
                Total gradient norm:      0.21430
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.312
                
Iteration (40700/50001) took 0.054 seconds.
                Mean final reward:        10.0000
                Mean return:              7.2812
                Policy entropy:           0.5971
                Pseudo loss:              2.09328
                Total gradient norm:      0.26734
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.719
                
Iteration (40800/50001) took 0.140 seconds.
                Mean final reward:        10.0000
                Mean return:              6.7812
                Policy entropy:           0.5894
                Pseudo loss:              3.17371
                Total gradient norm:      0.26812
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.219
                
Iteration (40900/50001) took 0.075 seconds.
                Mean final reward:        9.6562
                Mean return:              6.9688
                Policy entropy:           0.6053
                Pseudo loss:              2.36991
                Total gradient norm:      0.30352
                Solved trajectories:      31 / 32
                Avg steps to solve:       3.484
                
Iteration (41000/50001) took 0.129 seconds.
                Mean final reward:        10.0000
                Mean return:              6.7188
                Policy entropy:           0.6226
                Pseudo loss:              3.03662
                Total gradient norm:      0.30426
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.281
                
Iteration (41100/50001) took 0.137 seconds.
                Mean final reward:        10.0000
                Mean return:              7.0625
                Policy entropy:           0.5829
                Pseudo loss:              2.72026
                Total gradient norm:      0.23983
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.938
                
Iteration (41200/50001) took 0.176 seconds.
                Mean final reward:        10.0000
                Mean return:              7.1250
                Policy entropy:           0.5976
                Pseudo loss:              2.61856
                Total gradient norm:      0.24669
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.875
                
Iteration (41300/50001) took 0.087 seconds.
                Mean final reward:        10.0000
                Mean return:              7.2500
                Policy entropy:           0.5513
                Pseudo loss:              2.25565
                Total gradient norm:      0.24526
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.750
                
Iteration (41400/50001) took 0.129 seconds.
                Mean final reward:        10.0000
                Mean return:              6.8438
                Policy entropy:           0.6124
                Pseudo loss:              3.18168
                Total gradient norm:      0.43493
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.156
                
Iteration (41500/50001) took 0.146 seconds.
                Mean final reward:        10.0000
                Mean return:              7.1250
                Policy entropy:           0.5680
                Pseudo loss:              2.57052
                Total gradient norm:      0.29208
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.875
                
Iteration (41600/50001) took 0.053 seconds.
                Mean final reward:        10.0000
                Mean return:              7.2500
                Policy entropy:           0.5604
                Pseudo loss:              2.79528
                Total gradient norm:      0.31070
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.750
                
Iteration (41700/50001) took 0.126 seconds.
                Mean final reward:        10.0000
                Mean return:              6.6875
                Policy entropy:           0.6339
                Pseudo loss:              3.81505
                Total gradient norm:      0.33352
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.312
                
Iteration (41800/50001) took 0.120 seconds.
                Mean final reward:        10.0000
                Mean return:              6.8125
                Policy entropy:           0.6109
                Pseudo loss:              3.31292
                Total gradient norm:      0.33345
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.000
                
Iteration (41900/50001) took 0.087 seconds.
                Mean final reward:        10.0000
                Mean return:              6.8438
                Policy entropy:           0.6365
                Pseudo loss:              2.95910
                Total gradient norm:      0.31040
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.156
                
Iteration (42000/50001) took 0.150 seconds.
                Mean final reward:        10.0000
                Mean return:              6.9375
                Policy entropy:           0.6173
                Pseudo loss:              3.40916
                Total gradient norm:      0.31448
                Solved trajectories:      30 / 32
                Avg steps to solve:       3.667
                
Iteration (42100/50001) took 0.139 seconds.
                Mean final reward:        9.6562
                Mean return:              6.3438
                Policy entropy:           0.6537
                Pseudo loss:              2.46927
                Total gradient norm:      0.26346
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.129
                
Iteration (42200/50001) took 0.115 seconds.
                Mean final reward:        10.0000
                Mean return:              7.3125
                Policy entropy:           0.5879
                Pseudo loss:              2.84057
                Total gradient norm:      0.41155
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.688
                
Iteration (42300/50001) took 0.085 seconds.
                Mean final reward:        9.6562
                Mean return:              6.3125
                Policy entropy:           0.6167
                Pseudo loss:              3.05713
                Total gradient norm:      0.27676
                Solved trajectories:      30 / 32
                Avg steps to solve:       3.967
                
Iteration (42400/50001) took 0.117 seconds.
                Mean final reward:        10.0000
                Mean return:              7.2812
                Policy entropy:           0.5990
                Pseudo loss:              2.65326
                Total gradient norm:      0.25605
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.719
                
Iteration (42500/50001) took 0.139 seconds.
                Mean final reward:        9.6562
                Mean return:              6.1875
                Policy entropy:           0.6592
                Pseudo loss:              3.18079
                Total gradient norm:      0.30505
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.290
                
Iteration (42600/50001) took 0.178 seconds.
                Mean final reward:        10.0000
                Mean return:              6.9688
                Policy entropy:           0.6188
                Pseudo loss:              3.13310
                Total gradient norm:      0.31400
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.031
                
Iteration (42700/50001) took 0.130 seconds.
                Mean final reward:        10.0000
                Mean return:              6.7812
                Policy entropy:           0.6079
                Pseudo loss:              3.10835
                Total gradient norm:      0.22674
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.219
                
Iteration (42800/50001) took 0.129 seconds.
                Mean final reward:        10.0000
                Mean return:              6.6562
                Policy entropy:           0.6271
                Pseudo loss:              3.74503
                Total gradient norm:      0.27258
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.344
                
Iteration (42900/50001) took 0.078 seconds.
                Mean final reward:        10.0000
                Mean return:              7.2188
                Policy entropy:           0.5837
                Pseudo loss:              2.42507
                Total gradient norm:      0.25040
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.781
                
Iteration (43000/50001) took 0.070 seconds.
                Mean final reward:        10.0000
                Mean return:              6.7500
                Policy entropy:           0.6181
                Pseudo loss:              3.26406
                Total gradient norm:      0.24444
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.250
                
Iteration (43100/50001) took 0.095 seconds.
                Mean final reward:        10.0000
                Mean return:              7.2188
                Policy entropy:           0.6176
                Pseudo loss:              2.72129
                Total gradient norm:      0.30006
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.781
                
Iteration (43200/50001) took 0.075 seconds.
                Mean final reward:        10.0000
                Mean return:              6.6250
                Policy entropy:           0.6308
                Pseudo loss:              3.44691
                Total gradient norm:      0.27199
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.375
                
Iteration (43300/50001) took 0.086 seconds.
                Mean final reward:        10.0000
                Mean return:              6.9375
                Policy entropy:           0.6038
                Pseudo loss:              3.11872
                Total gradient norm:      0.30189
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.062
                
Iteration (43400/50001) took 0.095 seconds.
                Mean final reward:        9.6562
                Mean return:              6.6875
                Policy entropy:           0.6025
                Pseudo loss:              2.58535
                Total gradient norm:      0.32509
                Solved trajectories:      31 / 32
                Avg steps to solve:       3.774
                
Iteration (43500/50001) took 0.065 seconds.
                Mean final reward:        10.0000
                Mean return:              7.5938
                Policy entropy:           0.5878
                Pseudo loss:              2.11299
                Total gradient norm:      0.24115
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.406
                
Iteration (43600/50001) took 0.131 seconds.
                Mean final reward:        10.0000
                Mean return:              6.4688
                Policy entropy:           0.6232
                Pseudo loss:              3.48673
                Total gradient norm:      0.36564
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.531
                
Iteration (43700/50001) took 0.113 seconds.
                Mean final reward:        10.0000
                Mean return:              7.0312
                Policy entropy:           0.6024
                Pseudo loss:              2.60491
                Total gradient norm:      0.26391
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.969
                
Iteration (43800/50001) took 0.072 seconds.
                Mean final reward:        10.0000
                Mean return:              6.8438
                Policy entropy:           0.6122
                Pseudo loss:              2.92031
                Total gradient norm:      0.27526
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.156
                
Iteration (43900/50001) took 0.068 seconds.
                Mean final reward:        10.0000
                Mean return:              7.2500
                Policy entropy:           0.5763
                Pseudo loss:              2.34093
                Total gradient norm:      0.26776
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.750
                
Iteration (44000/50001) took 0.079 seconds.
                Mean final reward:        10.0000
                Mean return:              6.7500
                Policy entropy:           0.6341
                Pseudo loss:              3.02093
                Total gradient norm:      0.30103
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.250
                
Iteration (44100/50001) took 0.078 seconds.
                Mean final reward:        9.6562
                Mean return:              6.6250
                Policy entropy:           0.6155
                Pseudo loss:              2.61257
                Total gradient norm:      0.26708
                Solved trajectories:      31 / 32
                Avg steps to solve:       3.839
                
Iteration (44200/50001) took 0.064 seconds.
                Mean final reward:        10.0000
                Mean return:              6.7812
                Policy entropy:           0.6262
                Pseudo loss:              3.26820
                Total gradient norm:      0.27282
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.219
                
Iteration (44300/50001) took 0.111 seconds.
                Mean final reward:        10.0000
                Mean return:              6.6250
                Policy entropy:           0.6264
                Pseudo loss:              3.39094
                Total gradient norm:      0.30575
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.375
                
Iteration (44400/50001) took 0.048 seconds.
                Mean final reward:        10.0000
                Mean return:              6.9062
                Policy entropy:           0.6242
                Pseudo loss:              3.21275
                Total gradient norm:      0.32146
                Solved trajectories:      31 / 32
                Avg steps to solve:       3.903
                
Iteration (44500/50001) took 0.212 seconds.
                Mean final reward:        10.0000
                Mean return:              6.9375
                Policy entropy:           0.6038
                Pseudo loss:              3.27010
                Total gradient norm:      0.37159
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.062
                
Iteration (44600/50001) took 0.062 seconds.
                Mean final reward:        9.6562
                Mean return:              6.4375
                Policy entropy:           0.6232
                Pseudo loss:              3.02164
                Total gradient norm:      0.26784
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.032
                
Iteration (44700/50001) took 0.089 seconds.
                Mean final reward:        10.0000
                Mean return:              6.9375
                Policy entropy:           0.5931
                Pseudo loss:              2.74522
                Total gradient norm:      0.27945
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.062
                
Iteration (44800/50001) took 0.065 seconds.
                Mean final reward:        10.0000
                Mean return:              7.3125
                Policy entropy:           0.6147
                Pseudo loss:              2.33071
                Total gradient norm:      0.24792
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.688
                
Iteration (44900/50001) took 0.073 seconds.
                Mean final reward:        10.0000
                Mean return:              6.4062
                Policy entropy:           0.6539
                Pseudo loss:              3.65941
                Total gradient norm:      0.29741
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.594
                
Iteration (45000/50001) took 0.063 seconds.
                Mean final reward:        10.0000
                Mean return:              7.4062
                Policy entropy:           0.5654
                Pseudo loss:              1.96835
                Total gradient norm:      0.21688
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.594
                
Iteration (45100/50001) took 0.153 seconds.
                Mean final reward:        10.0000
                Mean return:              6.6875
                Policy entropy:           0.6255
                Pseudo loss:              3.13000
                Total gradient norm:      0.34935
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.312
                
Iteration (45200/50001) took 0.079 seconds.
                Mean final reward:        9.6562
                Mean return:              5.5625
                Policy entropy:           0.6682
                Pseudo loss:              4.15810
                Total gradient norm:      0.34599
                Solved trajectories:      28 / 32
                Avg steps to solve:       4.393
                
Iteration (45300/50001) took 0.067 seconds.
                Mean final reward:        9.6562
                Mean return:              6.3125
                Policy entropy:           0.6349
                Pseudo loss:              3.43771
                Total gradient norm:      0.36586
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.161
                
Iteration (45400/50001) took 0.077 seconds.
                Mean final reward:        10.0000
                Mean return:              7.1562
                Policy entropy:           0.5959
                Pseudo loss:              2.57542
                Total gradient norm:      0.31012
                Solved trajectories:      31 / 32
                Avg steps to solve:       3.645
                
Iteration (45500/50001) took 0.110 seconds.
                Mean final reward:        10.0000
                Mean return:              7.0312
                Policy entropy:           0.5989
                Pseudo loss:              2.63451
                Total gradient norm:      0.32653
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.969
                
Iteration (45600/50001) took 0.081 seconds.
                Mean final reward:        10.0000
                Mean return:              6.7500
                Policy entropy:           0.6052
                Pseudo loss:              3.18814
                Total gradient norm:      0.28165
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.065
                
Iteration (45700/50001) took 0.148 seconds.
                Mean final reward:        10.0000
                Mean return:              6.8125
                Policy entropy:           0.6203
                Pseudo loss:              3.10255
                Total gradient norm:      0.31306
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.188
                
Iteration (45800/50001) took 0.076 seconds.
                Mean final reward:        10.0000
                Mean return:              6.9688
                Policy entropy:           0.5868
                Pseudo loss:              3.03615
                Total gradient norm:      0.35941
                Solved trajectories:      31 / 32
                Avg steps to solve:       3.839
                
Iteration (45900/50001) took 0.089 seconds.
                Mean final reward:        10.0000
                Mean return:              7.1875
                Policy entropy:           0.5947
                Pseudo loss:              2.59947
                Total gradient norm:      0.22275
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.812
                
Iteration (46000/50001) took 0.094 seconds.
                Mean final reward:        10.0000
                Mean return:              6.8750
                Policy entropy:           0.6215
                Pseudo loss:              2.90067
                Total gradient norm:      0.25537
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.125
                
Iteration (46100/50001) took 0.078 seconds.
                Mean final reward:        10.0000
                Mean return:              7.0000
                Policy entropy:           0.6083
                Pseudo loss:              2.45365
                Total gradient norm:      0.27393
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.000
                
Iteration (46200/50001) took 0.096 seconds.
                Mean final reward:        9.6562
                Mean return:              6.0000
                Policy entropy:           0.6655
                Pseudo loss:              3.39090
                Total gradient norm:      0.39781
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.484
                
Iteration (46300/50001) took 0.107 seconds.
                Mean final reward:        10.0000
                Mean return:              7.1875
                Policy entropy:           0.5737
                Pseudo loss:              2.95881
                Total gradient norm:      0.40362
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.812
                
Iteration (46400/50001) took 0.095 seconds.
                Mean final reward:        9.6562
                Mean return:              6.0625
                Policy entropy:           0.6408
                Pseudo loss:              3.48162
                Total gradient norm:      0.38498
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.233
                
Iteration (46500/50001) took 0.051 seconds.
                Mean final reward:        10.0000
                Mean return:              6.8125
                Policy entropy:           0.6339
                Pseudo loss:              3.15026
                Total gradient norm:      0.27295
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.000
                
Iteration (46600/50001) took 0.084 seconds.
                Mean final reward:        10.0000
                Mean return:              6.8125
                Policy entropy:           0.6296
                Pseudo loss:              2.76391
                Total gradient norm:      0.23717
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.188
                
Iteration (46700/50001) took 0.061 seconds.
                Mean final reward:        10.0000
                Mean return:              7.0938
                Policy entropy:           0.5942
                Pseudo loss:              2.86750
                Total gradient norm:      0.21225
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.906
                
Iteration (46800/50001) took 0.069 seconds.
                Mean final reward:        9.6562
                Mean return:              7.0625
                Policy entropy:           0.5763
                Pseudo loss:              1.84823
                Total gradient norm:      0.20380
                Solved trajectories:      31 / 32
                Avg steps to solve:       3.387
                
Iteration (46900/50001) took 0.167 seconds.
                Mean final reward:        9.6562
                Mean return:              5.9375
                Policy entropy:           0.6913
                Pseudo loss:              3.96026
                Total gradient norm:      0.35303
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.548
                
Iteration (47000/50001) took 0.175 seconds.
                Mean final reward:        10.0000
                Mean return:              7.1250
                Policy entropy:           0.5975
                Pseudo loss:              2.66987
                Total gradient norm:      0.33209
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.875
                
Iteration (47100/50001) took 0.117 seconds.
                Mean final reward:        9.6562
                Mean return:              5.9375
                Policy entropy:           0.6654
                Pseudo loss:              3.62734
                Total gradient norm:      0.32736
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.367
                
Iteration (47200/50001) took 0.064 seconds.
                Mean final reward:        10.0000
                Mean return:              6.9688
                Policy entropy:           0.5983
                Pseudo loss:              2.93359
                Total gradient norm:      0.34783
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.031
                
Iteration (47300/50001) took 0.106 seconds.
                Mean final reward:        10.0000
                Mean return:              6.4688
                Policy entropy:           0.6249
                Pseudo loss:              3.47886
                Total gradient norm:      0.27406
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.531
                
Iteration (47400/50001) took 0.053 seconds.
                Mean final reward:        10.0000
                Mean return:              6.6562
                Policy entropy:           0.6165
                Pseudo loss:              2.92081
                Total gradient norm:      0.30900
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.344
                
Iteration (47500/50001) took 0.061 seconds.
                Mean final reward:        9.6562
                Mean return:              6.1562
                Policy entropy:           0.6348
                Pseudo loss:              3.34818
                Total gradient norm:      0.28025
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.133
                
Iteration (47600/50001) took 0.051 seconds.
                Mean final reward:        10.0000
                Mean return:              7.1562
                Policy entropy:           0.5977
                Pseudo loss:              2.31938
                Total gradient norm:      0.29276
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.844
                
Iteration (47700/50001) took 0.074 seconds.
                Mean final reward:        10.0000
                Mean return:              7.0000
                Policy entropy:           0.6080
                Pseudo loss:              2.61814
                Total gradient norm:      0.25505
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.000
                
Iteration (47800/50001) took 0.102 seconds.
                Mean final reward:        9.6562
                Mean return:              6.4062
                Policy entropy:           0.6333
                Pseudo loss:              3.20808
                Total gradient norm:      0.23983
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.065
                
Iteration (47900/50001) took 0.074 seconds.
                Mean final reward:        10.0000
                Mean return:              6.9375
                Policy entropy:           0.6000
                Pseudo loss:              2.77038
                Total gradient norm:      0.25064
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.062
                
Iteration (48000/50001) took 0.106 seconds.
                Mean final reward:        10.0000
                Mean return:              6.9062
                Policy entropy:           0.5954
                Pseudo loss:              3.22646
                Total gradient norm:      0.30178
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.094
                
Iteration (48100/50001) took 0.073 seconds.
                Mean final reward:        10.0000
                Mean return:              6.6250
                Policy entropy:           0.5955
                Pseudo loss:              3.35357
                Total gradient norm:      0.31015
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.375
                
Iteration (48200/50001) took 0.069 seconds.
                Mean final reward:        10.0000
                Mean return:              6.6875
                Policy entropy:           0.6377
                Pseudo loss:              3.11939
                Total gradient norm:      0.22382
                Solved trajectories:      30 / 32
                Avg steps to solve:       3.933
                
Iteration (48300/50001) took 0.065 seconds.
                Mean final reward:        10.0000
                Mean return:              7.1875
                Policy entropy:           0.5886
                Pseudo loss:              2.59012
                Total gradient norm:      0.26436
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.812
                
Iteration (48400/50001) took 0.099 seconds.
                Mean final reward:        10.0000
                Mean return:              7.0000
                Policy entropy:           0.5684
                Pseudo loss:              2.73109
                Total gradient norm:      0.25619
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.000
                
Iteration (48500/50001) took 0.061 seconds.
                Mean final reward:        10.0000
                Mean return:              6.8750
                Policy entropy:           0.5953
                Pseudo loss:              3.31961
                Total gradient norm:      0.21970
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.125
                
Iteration (48600/50001) took 0.059 seconds.
                Mean final reward:        10.0000
                Mean return:              7.2812
                Policy entropy:           0.5670
                Pseudo loss:              2.16227
                Total gradient norm:      0.24207
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.719
                
Iteration (48700/50001) took 0.065 seconds.
                Mean final reward:        10.0000
                Mean return:              6.6875
                Policy entropy:           0.6298
                Pseudo loss:              3.18142
                Total gradient norm:      0.29790
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.312
                
Iteration (48800/50001) took 0.063 seconds.
                Mean final reward:        9.6562
                Mean return:              6.1250
                Policy entropy:           0.6486
                Pseudo loss:              3.06888
                Total gradient norm:      0.30104
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.355
                
Iteration (48900/50001) took 0.066 seconds.
                Mean final reward:        10.0000
                Mean return:              6.8125
                Policy entropy:           0.5979
                Pseudo loss:              3.29544
                Total gradient norm:      0.32127
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.000
                
Iteration (49000/50001) took 0.065 seconds.
                Mean final reward:        10.0000
                Mean return:              7.0938
                Policy entropy:           0.5829
                Pseudo loss:              2.70193
                Total gradient norm:      0.33055
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.906
                
Iteration (49100/50001) took 0.056 seconds.
                Mean final reward:        10.0000
                Mean return:              7.0938
                Policy entropy:           0.5915
                Pseudo loss:              2.49520
                Total gradient norm:      0.28359
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.906
                
Iteration (49200/50001) took 0.071 seconds.
                Mean final reward:        9.6562
                Mean return:              6.5312
                Policy entropy:           0.6244
                Pseudo loss:              3.01339
                Total gradient norm:      0.35068
                Solved trajectories:      31 / 32
                Avg steps to solve:       3.935
                
Iteration (49300/50001) took 0.052 seconds.
                Mean final reward:        10.0000
                Mean return:              6.8438
                Policy entropy:           0.6174
                Pseudo loss:              3.31147
                Total gradient norm:      0.32478
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.156
                
Iteration (49400/50001) took 0.067 seconds.
                Mean final reward:        10.0000
                Mean return:              7.1875
                Policy entropy:           0.5782
                Pseudo loss:              2.44422
                Total gradient norm:      0.23673
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.812
                
Iteration (49500/50001) took 0.078 seconds.
                Mean final reward:        9.6562
                Mean return:              6.6875
                Policy entropy:           0.6301
                Pseudo loss:              2.89707
                Total gradient norm:      0.30845
                Solved trajectories:      30 / 32
                Avg steps to solve:       3.567
                
Iteration (49600/50001) took 0.060 seconds.
                Mean final reward:        10.0000
                Mean return:              6.4688
                Policy entropy:           0.6463
                Pseudo loss:              3.42927
                Total gradient norm:      0.40337
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.167
                
Iteration (49700/50001) took 0.066 seconds.
                Mean final reward:        10.0000
                Mean return:              6.8125
                Policy entropy:           0.6108
                Pseudo loss:              2.97728
                Total gradient norm:      0.35918
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.188
                
Iteration (49800/50001) took 0.041 seconds.
                Mean final reward:        10.0000
                Mean return:              7.0000
                Policy entropy:           0.5882
                Pseudo loss:              2.69141
                Total gradient norm:      0.29984
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.000
                
Iteration (49900/50001) took 0.069 seconds.
                Mean final reward:        10.0000
                Mean return:              7.0000
                Policy entropy:           0.5824
                Pseudo loss:              3.02247
                Total gradient norm:      0.29131
                Solved trajectories:      31 / 32
                Avg steps to solve:       3.806
                
Iteration (50000/50001) took 0.060 seconds.
                Mean final reward:        10.0000
                Mean return:              7.2500
                Policy entropy:           0.5752
                Pseudo loss:              2.27045
                Total gradient norm:      0.24755
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.750
                
Training took 5096.835 seconds.
