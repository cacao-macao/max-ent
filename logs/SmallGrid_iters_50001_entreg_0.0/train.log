Using device: cpu

Using optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)

Iteration (0/50001) took 0.098 seconds.
                Mean final reward:        -0.3125
                Mean return:              -9.0625
                Policy entropy:           1.5783
                Pseudo loss:              -7.84773
                Total gradient norm:      0.46207
                Solved trajectories:      1 / 32
                Avg steps to solve:       2.000
                
Iteration (100/50001) took 0.086 seconds.
                Mean final reward:        0.0312
                Mean return:              -8.3125
                Policy entropy:           1.5803
                Pseudo loss:              -7.65703
                Total gradient norm:      0.43550
                Solved trajectories:      3 / 32
                Avg steps to solve:       3.000
                
Iteration (200/50001) took 0.139 seconds.
                Mean final reward:        1.0625
                Mean return:              -7.1250
                Policy entropy:           1.5754
                Pseudo loss:              -6.29010
                Total gradient norm:      0.42260
                Solved trajectories:      4 / 32
                Avg steps to solve:       3.500
                
Iteration (300/50001) took 0.098 seconds.
                Mean final reward:        2.4375
                Mean return:              -4.7500
                Policy entropy:           1.5697
                Pseudo loss:              -4.83231
                Total gradient norm:      0.47853
                Solved trajectories:      10 / 32
                Avg steps to solve:       4.200
                
Iteration (400/50001) took 0.236 seconds.
                Mean final reward:        1.4062
                Mean return:              -6.6250
                Policy entropy:           1.5507
                Pseudo loss:              -6.21968
                Total gradient norm:      0.45176
                Solved trajectories:      7 / 32
                Avg steps to solve:       5.571
                
Iteration (500/50001) took 0.177 seconds.
                Mean final reward:        0.7188
                Mean return:              -7.3438
                Policy entropy:           1.5365
                Pseudo loss:              -7.75469
                Total gradient norm:      0.54743
                Solved trajectories:      5 / 32
                Avg steps to solve:       4.000
                
Iteration (600/50001) took 0.206 seconds.
                Mean final reward:        2.4375
                Mean return:              -5.2812
                Policy entropy:           1.5157
                Pseudo loss:              -5.18614
                Total gradient norm:      0.48467
                Solved trajectories:      9 / 32
                Avg steps to solve:       5.444
                
Iteration (700/50001) took 0.125 seconds.
                Mean final reward:        1.7500
                Mean return:              -6.2812
                Policy entropy:           1.4745
                Pseudo loss:              -6.47435
                Total gradient norm:      0.52424
                Solved trajectories:      8 / 32
                Avg steps to solve:       6.125
                
Iteration (800/50001) took 0.145 seconds.
                Mean final reward:        5.1875
                Mean return:              -1.0938
                Policy entropy:           1.4346
                Pseudo loss:              -2.51008
                Total gradient norm:      0.56613
                Solved trajectories:      17 / 32
                Avg steps to solve:       4.882
                
Iteration (900/50001) took 0.170 seconds.
                Mean final reward:        4.1562
                Mean return:              -2.3750
                Policy entropy:           1.3820
                Pseudo loss:              -3.98500
                Total gradient norm:      0.47401
                Solved trajectories:      14 / 32
                Avg steps to solve:       4.357
                
Iteration (1000/50001) took 0.178 seconds.
                Mean final reward:        4.5000
                Mean return:              -2.2812
                Policy entropy:           1.3493
                Pseudo loss:              -3.61322
                Total gradient norm:      0.54037
                Solved trajectories:      14 / 32
                Avg steps to solve:       4.929
                
Iteration (1100/50001) took 0.128 seconds.
                Mean final reward:        2.0938
                Mean return:              -5.7188
                Policy entropy:           1.2883
                Pseudo loss:              -7.50935
                Total gradient norm:      0.65754
                Solved trajectories:      8 / 32
                Avg steps to solve:       5.250
                
Iteration (1200/50001) took 0.130 seconds.
                Mean final reward:        5.5312
                Mean return:              -0.5312
                Policy entropy:           1.2316
                Pseudo loss:              -3.00538
                Total gradient norm:      0.51697
                Solved trajectories:      18 / 32
                Avg steps to solve:       4.778
                
Iteration (1300/50001) took 0.112 seconds.
                Mean final reward:        3.1250
                Mean return:              -4.1562
                Policy entropy:           1.1740
                Pseudo loss:              -6.41731
                Total gradient norm:      0.57575
                Solved trajectories:      11 / 32
                Avg steps to solve:       5.000
                
Iteration (1400/50001) took 0.200 seconds.
                Mean final reward:        5.1875
                Mean return:              -1.1250
                Policy entropy:           1.1260
                Pseudo loss:              -3.88631
                Total gradient norm:      0.55847
                Solved trajectories:      18 / 32
                Avg steps to solve:       5.222
                
Iteration (1500/50001) took 0.228 seconds.
                Mean final reward:        3.4688
                Mean return:              -3.7188
                Policy entropy:           1.0514
                Pseudo loss:              -6.84174
                Total gradient norm:      0.72984
                Solved trajectories:      12 / 32
                Avg steps to solve:       5.167
                
Iteration (1600/50001) took 0.169 seconds.
                Mean final reward:        5.1875
                Mean return:              -0.8750
                Policy entropy:           0.9767
                Pseudo loss:              -4.90159
                Total gradient norm:      0.57152
                Solved trajectories:      17 / 32
                Avg steps to solve:       4.471
                
Iteration (1700/50001) took 0.108 seconds.
                Mean final reward:        6.5625
                Mean return:              0.9062
                Policy entropy:           0.9508
                Pseudo loss:              -2.68485
                Total gradient norm:      0.52609
                Solved trajectories:      20 / 32
                Avg steps to solve:       4.650
                
Iteration (1800/50001) took 0.087 seconds.
                Mean final reward:        5.8750
                Mean return:              0.4062
                Policy entropy:           0.8326
                Pseudo loss:              -4.93560
                Total gradient norm:      0.72343
                Solved trajectories:      20 / 32
                Avg steps to solve:       4.350
                
Iteration (1900/50001) took 0.169 seconds.
                Mean final reward:        4.5000
                Mean return:              -2.3750
                Policy entropy:           0.8153
                Pseudo loss:              -6.62251
                Total gradient norm:      0.76248
                Solved trajectories:      12 / 32
                Avg steps to solve:       4.333
                
Iteration (2000/50001) took 0.175 seconds.
                Mean final reward:        4.1562
                Mean return:              -2.9062
                Policy entropy:           0.7584
                Pseudo loss:              -7.71657
                Total gradient norm:      0.83397
                Solved trajectories:      14 / 32
                Avg steps to solve:       5.571
                
Iteration (2100/50001) took 0.127 seconds.
                Mean final reward:        7.9375
                Mean return:              3.0625
                Policy entropy:           0.8144
                Pseudo loss:              -1.15727
                Total gradient norm:      0.34116
                Solved trajectories:      25 / 32
                Avg steps to solve:       4.720
                
Iteration (2200/50001) took 0.132 seconds.
                Mean final reward:        7.2500
                Mean return:              1.5938
                Policy entropy:           0.7691
                Pseudo loss:              -2.15796
                Total gradient norm:      0.43420
                Solved trajectories:      22 / 32
                Avg steps to solve:       5.136
                
Iteration (2300/50001) took 0.147 seconds.
                Mean final reward:        7.2500
                Mean return:              1.8125
                Policy entropy:           0.6246
                Pseudo loss:              -3.29287
                Total gradient norm:      0.60473
                Solved trajectories:      21 / 32
                Avg steps to solve:       4.571
                
Iteration (2400/50001) took 0.083 seconds.
                Mean final reward:        6.5625
                Mean return:              0.7500
                Policy entropy:           0.6525
                Pseudo loss:              -4.18141
                Total gradient norm:      0.51014
                Solved trajectories:      21 / 32
                Avg steps to solve:       5.143
                
Iteration (2500/50001) took 0.217 seconds.
                Mean final reward:        8.2812
                Mean return:              3.0000
                Policy entropy:           0.6732
                Pseudo loss:              -0.41146
                Total gradient norm:      0.29979
                Solved trajectories:      26 / 32
                Avg steps to solve:       5.423
                
Iteration (2600/50001) took 0.122 seconds.
                Mean final reward:        8.2812
                Mean return:              3.2188
                Policy entropy:           0.6104
                Pseudo loss:              -0.60512
                Total gradient norm:      0.32321
                Solved trajectories:      26 / 32
                Avg steps to solve:       5.154
                
Iteration (2700/50001) took 0.122 seconds.
                Mean final reward:        7.9375
                Mean return:              2.5625
                Policy entropy:           0.6502
                Pseudo loss:              -1.52121
                Total gradient norm:      0.37889
                Solved trajectories:      25 / 32
                Avg steps to solve:       5.360
                
Iteration (2800/50001) took 0.164 seconds.
                Mean final reward:        7.2500
                Mean return:              2.0312
                Policy entropy:           0.5339
                Pseudo loss:              -4.16427
                Total gradient norm:      0.62046
                Solved trajectories:      23 / 32
                Avg steps to solve:       4.739
                
Iteration (2900/50001) took 0.132 seconds.
                Mean final reward:        7.5938
                Mean return:              2.8750
                Policy entropy:           0.5846
                Pseudo loss:              -3.17200
                Total gradient norm:      0.42226
                Solved trajectories:      25 / 32
                Avg steps to solve:       4.520
                
Iteration (3000/50001) took 0.066 seconds.
                Mean final reward:        7.9375
                Mean return:              2.7500
                Policy entropy:           0.6006
                Pseudo loss:              -1.85295
                Total gradient norm:      0.40206
                Solved trajectories:      26 / 32
                Avg steps to solve:       5.308
                
Iteration (3100/50001) took 0.117 seconds.
                Mean final reward:        8.2812
                Mean return:              3.4062
                Policy entropy:           0.5751
                Pseudo loss:              -1.01695
                Total gradient norm:      0.27222
                Solved trajectories:      26 / 32
                Avg steps to solve:       4.923
                
Iteration (3200/50001) took 0.169 seconds.
                Mean final reward:        7.2500
                Mean return:              1.9688
                Policy entropy:           0.5130
                Pseudo loss:              -4.64715
                Total gradient norm:      0.52063
                Solved trajectories:      24 / 32
                Avg steps to solve:       5.042
                
Iteration (3300/50001) took 0.078 seconds.
                Mean final reward:        8.2812
                Mean return:              3.1562
                Policy entropy:           0.5046
                Pseudo loss:              -1.99067
                Total gradient norm:      0.41786
                Solved trajectories:      25 / 32
                Avg steps to solve:       5.040
                
Iteration (3400/50001) took 0.167 seconds.
                Mean final reward:        7.5938
                Mean return:              2.1250
                Policy entropy:           0.4625
                Pseudo loss:              -4.40822
                Total gradient norm:      0.60576
                Solved trajectories:      23 / 32
                Avg steps to solve:       5.087
                
Iteration (3500/50001) took 0.172 seconds.
                Mean final reward:        7.5938
                Mean return:              2.0000
                Policy entropy:           0.5395
                Pseudo loss:              -3.14859
                Total gradient norm:      0.38132
                Solved trajectories:      22 / 32
                Avg steps to solve:       5.045
                
Iteration (3600/50001) took 0.153 seconds.
                Mean final reward:        7.5938
                Mean return:              2.1250
                Policy entropy:           0.5150
                Pseudo loss:              -3.55964
                Total gradient norm:      0.43015
                Solved trajectories:      23 / 32
                Avg steps to solve:       5.087
                
Iteration (3700/50001) took 0.173 seconds.
                Mean final reward:        9.3125
                Mean return:              4.7188
                Policy entropy:           0.5130
                Pseudo loss:              1.02042
                Total gradient norm:      0.33120
                Solved trajectories:      28 / 32
                Avg steps to solve:       4.964
                
Iteration (3800/50001) took 0.097 seconds.
                Mean final reward:        10.0000
                Mean return:              6.1875
                Policy entropy:           0.4841
                Pseudo loss:              1.94706
                Total gradient norm:      0.19587
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.467
                
Iteration (3900/50001) took 0.165 seconds.
                Mean final reward:        9.3125
                Mean return:              5.0000
                Policy entropy:           0.5104
                Pseudo loss:              0.24112
                Total gradient norm:      0.20366
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.000
                
Iteration (4000/50001) took 0.120 seconds.
                Mean final reward:        9.3125
                Mean return:              4.7188
                Policy entropy:           0.4819
                Pseudo loss:              0.80771
                Total gradient norm:      0.24126
                Solved trajectories:      27 / 32
                Avg steps to solve:       4.778
                
Iteration (4100/50001) took 0.128 seconds.
                Mean final reward:        9.3125
                Mean return:              4.6250
                Policy entropy:           0.5022
                Pseudo loss:              0.35382
                Total gradient norm:      0.21822
                Solved trajectories:      27 / 32
                Avg steps to solve:       4.889
                
Iteration (4200/50001) took 0.187 seconds.
                Mean final reward:        7.9375
                Mean return:              2.6562
                Policy entropy:           0.4596
                Pseudo loss:              -3.46538
                Total gradient norm:      0.48347
                Solved trajectories:      25 / 32
                Avg steps to solve:       5.240
                
Iteration (4300/50001) took 0.138 seconds.
                Mean final reward:        8.2812
                Mean return:              3.4375
                Policy entropy:           0.3715
                Pseudo loss:              -3.45182
                Total gradient norm:      0.42778
                Solved trajectories:      26 / 32
                Avg steps to solve:       4.885
                
Iteration (4400/50001) took 0.129 seconds.
                Mean final reward:        9.6562
                Mean return:              5.2812
                Policy entropy:           0.4584
                Pseudo loss:              0.76722
                Total gradient norm:      0.19610
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.067
                
Iteration (4500/50001) took 0.072 seconds.
                Mean final reward:        9.6562
                Mean return:              5.7812
                Policy entropy:           0.4594
                Pseudo loss:              0.86239
                Total gradient norm:      0.15776
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.710
                
Iteration (4600/50001) took 0.150 seconds.
                Mean final reward:        8.2812
                Mean return:              3.5312
                Policy entropy:           0.4272
                Pseudo loss:              -3.04205
                Total gradient norm:      0.41031
                Solved trajectories:      26 / 32
                Avg steps to solve:       4.769
                
Iteration (4700/50001) took 0.134 seconds.
                Mean final reward:        9.3125
                Mean return:              4.6250
                Policy entropy:           0.4188
                Pseudo loss:              -0.44729
                Total gradient norm:      0.22179
                Solved trajectories:      28 / 32
                Avg steps to solve:       5.071
                
Iteration (4800/50001) took 0.071 seconds.
                Mean final reward:        9.6562
                Mean return:              5.2188
                Policy entropy:           0.4553
                Pseudo loss:              1.04591
                Total gradient norm:      0.18309
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.133
                
Iteration (4900/50001) took 0.100 seconds.
                Mean final reward:        10.0000
                Mean return:              6.0000
                Policy entropy:           0.4246
                Pseudo loss:              2.05045
                Total gradient norm:      0.16142
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.000
                
Iteration (5000/50001) took 0.086 seconds.
                Mean final reward:        8.2812
                Mean return:              3.5625
                Policy entropy:           0.3570
                Pseudo loss:              -4.12808
                Total gradient norm:      0.42056
                Solved trajectories:      26 / 32
                Avg steps to solve:       4.731
                
Iteration (5100/50001) took 0.132 seconds.
                Mean final reward:        8.6250
                Mean return:              3.9375
                Policy entropy:           0.4132
                Pseudo loss:              -2.87751
                Total gradient norm:      0.38634
                Solved trajectories:      26 / 32
                Avg steps to solve:       4.692
                
Iteration (5200/50001) took 0.105 seconds.
                Mean final reward:        8.6250
                Mean return:              3.9688
                Policy entropy:           0.3730
                Pseudo loss:              -3.34926
                Total gradient norm:      0.42261
                Solved trajectories:      27 / 32
                Avg steps to solve:       4.852
                
Iteration (5300/50001) took 0.062 seconds.
                Mean final reward:        9.6562
                Mean return:              5.1250
                Policy entropy:           0.4420
                Pseudo loss:              0.79661
                Total gradient norm:      0.23106
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.233
                
Iteration (5400/50001) took 0.144 seconds.
                Mean final reward:        9.6562
                Mean return:              5.7500
                Policy entropy:           0.3799
                Pseudo loss:              0.32129
                Total gradient norm:      0.17261
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.742
                
Iteration (5500/50001) took 0.050 seconds.
                Mean final reward:        9.6562
                Mean return:              5.8125
                Policy entropy:           0.4175
                Pseudo loss:              0.67082
                Total gradient norm:      0.12095
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.500
                
Iteration (5600/50001) took 0.110 seconds.
                Mean final reward:        8.2812
                Mean return:              3.8750
                Policy entropy:           0.3659
                Pseudo loss:              -4.92933
                Total gradient norm:      0.42832
                Solved trajectories:      27 / 32
                Avg steps to solve:       4.556
                
Iteration (5700/50001) took 0.136 seconds.
                Mean final reward:        9.3125
                Mean return:              4.8438
                Policy entropy:           0.3922
                Pseudo loss:              -0.45994
                Total gradient norm:      0.21011
                Solved trajectories:      29 / 32
                Avg steps to solve:       5.000
                
Iteration (5800/50001) took 0.125 seconds.
                Mean final reward:        8.9688
                Mean return:              4.2188
                Policy entropy:           0.3889
                Pseudo loss:              -1.35207
                Total gradient norm:      0.21527
                Solved trajectories:      28 / 32
                Avg steps to solve:       5.143
                
Iteration (5900/50001) took 0.052 seconds.
                Mean final reward:        9.6562
                Mean return:              4.9062
                Policy entropy:           0.3775
                Pseudo loss:              0.83990
                Total gradient norm:      0.13400
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.467
                
Iteration (6000/50001) took 0.136 seconds.
                Mean final reward:        8.6250
                Mean return:              3.4688
                Policy entropy:           0.3946
                Pseudo loss:              -2.03729
                Total gradient norm:      0.29043
                Solved trajectories:      26 / 32
                Avg steps to solve:       5.269
                
Iteration (6100/50001) took 0.069 seconds.
                Mean final reward:        9.6562
                Mean return:              4.9375
                Policy entropy:           0.3972
                Pseudo loss:              1.37467
                Total gradient norm:      0.14859
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.581
                
Iteration (6200/50001) took 0.116 seconds.
                Mean final reward:        8.9688
                Mean return:              4.3750
                Policy entropy:           0.3671
                Pseudo loss:              -1.73728
                Total gradient norm:      0.26359
                Solved trajectories:      29 / 32
                Avg steps to solve:       5.138
                
Iteration (6300/50001) took 0.119 seconds.
                Mean final reward:        8.9688
                Mean return:              4.6562
                Policy entropy:           0.3626
                Pseudo loss:              -2.88094
                Total gradient norm:      0.36738
                Solved trajectories:      28 / 32
                Avg steps to solve:       4.643
                
Iteration (6400/50001) took 0.088 seconds.
                Mean final reward:        9.3125
                Mean return:              4.9688
                Policy entropy:           0.3667
                Pseudo loss:              -1.19673
                Total gradient norm:      0.23969
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.033
                
Iteration (6500/50001) took 0.158 seconds.
                Mean final reward:        8.9688
                Mean return:              4.4688
                Policy entropy:           0.3028
                Pseudo loss:              -2.84677
                Total gradient norm:      0.35389
                Solved trajectories:      29 / 32
                Avg steps to solve:       5.034
                
Iteration (6600/50001) took 0.147 seconds.
                Mean final reward:        9.6562
                Mean return:              5.2500
                Policy entropy:           0.3472
                Pseudo loss:              0.60893
                Total gradient norm:      0.16735
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.258
                
Iteration (6700/50001) took 0.081 seconds.
                Mean final reward:        9.6562
                Mean return:              5.3125
                Policy entropy:           0.3211
                Pseudo loss:              0.01675
                Total gradient norm:      0.14154
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.194
                
Iteration (6800/50001) took 0.154 seconds.
                Mean final reward:        9.3125
                Mean return:              4.6250
                Policy entropy:           0.2826
                Pseudo loss:              -1.45986
                Total gradient norm:      0.23539
                Solved trajectories:      27 / 32
                Avg steps to solve:       4.889
                
Iteration (6900/50001) took 0.098 seconds.
                Mean final reward:        10.0000
                Mean return:              6.2500
                Policy entropy:           0.3204
                Pseudo loss:              1.46914
                Total gradient norm:      0.10738
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.581
                
Iteration (7000/50001) took 0.126 seconds.
                Mean final reward:        9.3125
                Mean return:              4.8750
                Policy entropy:           0.3196
                Pseudo loss:              -1.16406
                Total gradient norm:      0.23751
                Solved trajectories:      29 / 32
                Avg steps to solve:       4.966
                
Iteration (7100/50001) took 0.085 seconds.
                Mean final reward:        9.6562
                Mean return:              4.8125
                Policy entropy:           0.3372
                Pseudo loss:              1.00971
                Total gradient norm:      0.16400
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.567
                
Iteration (7200/50001) took 0.157 seconds.
                Mean final reward:        10.0000
                Mean return:              6.0625
                Policy entropy:           0.3295
                Pseudo loss:              1.30232
                Total gradient norm:      0.08009
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.600
                
Iteration (7300/50001) took 0.146 seconds.
                Mean final reward:        9.3125
                Mean return:              4.8750
                Policy entropy:           0.2906
                Pseudo loss:              -0.62554
                Total gradient norm:      0.17863
                Solved trajectories:      29 / 32
                Avg steps to solve:       4.966
                
Iteration (7400/50001) took 0.059 seconds.
                Mean final reward:        10.0000
                Mean return:              5.6250
                Policy entropy:           0.3306
                Pseudo loss:              1.92051
                Total gradient norm:      0.10899
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.226
                
Iteration (7500/50001) took 0.059 seconds.
                Mean final reward:        9.6562
                Mean return:              5.8750
                Policy entropy:           0.2909
                Pseudo loss:              -0.56515
                Total gradient norm:      0.13757
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.433
                
Iteration (7600/50001) took 0.171 seconds.
                Mean final reward:        10.0000
                Mean return:              5.5000
                Policy entropy:           0.3353
                Pseudo loss:              1.90461
                Total gradient norm:      0.14269
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.355
                
Iteration (7700/50001) took 0.170 seconds.
                Mean final reward:        9.3125
                Mean return:              5.0312
                Policy entropy:           0.2939
                Pseudo loss:              -0.65291
                Total gradient norm:      0.14625
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.967
                
Iteration (7800/50001) took 0.125 seconds.
                Mean final reward:        10.0000
                Mean return:              6.2812
                Policy entropy:           0.3053
                Pseudo loss:              1.13162
                Total gradient norm:      0.13760
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.719
                
Iteration (7900/50001) took 0.184 seconds.
                Mean final reward:        9.6562
                Mean return:              5.5938
                Policy entropy:           0.2751
                Pseudo loss:              -0.73887
                Total gradient norm:      0.14516
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.733
                
Iteration (8000/50001) took 0.068 seconds.
                Mean final reward:        9.6562
                Mean return:              5.0938
                Policy entropy:           0.3131
                Pseudo loss:              0.10377
                Total gradient norm:      0.16594
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.267
                
Iteration (8100/50001) took 0.089 seconds.
                Mean final reward:        9.6562
                Mean return:              5.4688
                Policy entropy:           0.2842
                Pseudo loss:              -0.43787
                Total gradient norm:      0.14975
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.867
                
Iteration (8200/50001) took 0.190 seconds.
                Mean final reward:        10.0000
                Mean return:              5.4062
                Policy entropy:           0.2843
                Pseudo loss:              1.61764
                Total gradient norm:      0.14231
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.452
                
Iteration (8300/50001) took 0.087 seconds.
                Mean final reward:        9.6562
                Mean return:              4.7812
                Policy entropy:           0.2996
                Pseudo loss:              -0.12385
                Total gradient norm:      0.20856
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.600
                
Iteration (8400/50001) took 0.157 seconds.
                Mean final reward:        10.0000
                Mean return:              6.3125
                Policy entropy:           0.2509
                Pseudo loss:              0.81473
                Total gradient norm:      0.10745
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.516
                
Iteration (8500/50001) took 0.208 seconds.
                Mean final reward:        9.6562
                Mean return:              5.5000
                Policy entropy:           0.2792
                Pseudo loss:              -0.18585
                Total gradient norm:      0.15692
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.000
                
Iteration (8600/50001) took 0.085 seconds.
                Mean final reward:        10.0000
                Mean return:              5.5938
                Policy entropy:           0.3138
                Pseudo loss:              1.76444
                Total gradient norm:      0.15956
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.406
                
Iteration (8700/50001) took 0.098 seconds.
                Mean final reward:        9.3125
                Mean return:              4.4375
                Policy entropy:           0.2978
                Pseudo loss:              -0.53108
                Total gradient norm:      0.18243
                Solved trajectories:      29 / 32
                Avg steps to solve:       5.448
                
Iteration (8800/50001) took 0.121 seconds.
                Mean final reward:        9.3125
                Mean return:              5.3750
                Policy entropy:           0.2479
                Pseudo loss:              -2.34795
                Total gradient norm:      0.24781
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.600
                
Iteration (8900/50001) took 0.158 seconds.
                Mean final reward:        10.0000
                Mean return:              5.5938
                Policy entropy:           0.2949
                Pseudo loss:              1.37527
                Total gradient norm:      0.12403
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.258
                
Iteration (9000/50001) took 0.093 seconds.
                Mean final reward:        9.3125
                Mean return:              5.5000
                Policy entropy:           0.2480
                Pseudo loss:              -1.94120
                Total gradient norm:      0.22713
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.467
                
Iteration (9100/50001) took 0.080 seconds.
                Mean final reward:        9.3125
                Mean return:              5.4375
                Policy entropy:           0.2778
                Pseudo loss:              -1.19175
                Total gradient norm:      0.14321
                Solved trajectories:      29 / 32
                Avg steps to solve:       4.345
                
Iteration (9200/50001) took 0.133 seconds.
                Mean final reward:        9.6562
                Mean return:              5.4375
                Policy entropy:           0.3099
                Pseudo loss:              -0.22371
                Total gradient norm:      0.14423
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.065
                
Iteration (9300/50001) took 0.086 seconds.
                Mean final reward:        10.0000
                Mean return:              5.4375
                Policy entropy:           0.3000
                Pseudo loss:              1.77023
                Total gradient norm:      0.13264
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.562
                
Iteration (9400/50001) took 0.155 seconds.
                Mean final reward:        9.6562
                Mean return:              5.0938
                Policy entropy:           0.2892
                Pseudo loss:              -0.03623
                Total gradient norm:      0.14789
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.267
                
Iteration (9500/50001) took 0.055 seconds.
                Mean final reward:        10.0000
                Mean return:              5.8750
                Policy entropy:           0.2640
                Pseudo loss:              1.15078
                Total gradient norm:      0.08686
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.968
                
Iteration (9600/50001) took 0.112 seconds.
                Mean final reward:        10.0000
                Mean return:              6.8438
                Policy entropy:           0.2031
                Pseudo loss:              0.68916
                Total gradient norm:      0.06350
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.156
                
Iteration (9700/50001) took 0.100 seconds.
                Mean final reward:        10.0000
                Mean return:              5.8438
                Policy entropy:           0.2606
                Pseudo loss:              1.16590
                Total gradient norm:      0.10386
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.000
                
Iteration (9800/50001) took 0.077 seconds.
                Mean final reward:        9.6562
                Mean return:              5.5938
                Policy entropy:           0.2608
                Pseudo loss:              0.21626
                Total gradient norm:      0.13560
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.903
                
Iteration (9900/50001) took 0.131 seconds.
                Mean final reward:        9.6562
                Mean return:              5.5312
                Policy entropy:           0.2931
                Pseudo loss:              0.13883
                Total gradient norm:      0.09604
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.968
                
Iteration (10000/50001) took 0.139 seconds.
                Mean final reward:        9.6562
                Mean return:              5.3750
                Policy entropy:           0.2489
                Pseudo loss:              -0.64651
                Total gradient norm:      0.13313
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.967
                
Iteration (10100/50001) took 0.163 seconds.
                Mean final reward:        9.6562
                Mean return:              5.0625
                Policy entropy:           0.3079
                Pseudo loss:              -0.13203
                Total gradient norm:      0.14868
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.300
                
Iteration (10200/50001) took 0.150 seconds.
                Mean final reward:        10.0000
                Mean return:              5.8438
                Policy entropy:           0.2551
                Pseudo loss:              1.16713
                Total gradient norm:      0.10699
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.156
                
Iteration (10300/50001) took 0.129 seconds.
                Mean final reward:        9.6562
                Mean return:              5.9688
                Policy entropy:           0.2824
                Pseudo loss:              -0.47906
                Total gradient norm:      0.13831
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.516
                
Iteration (10400/50001) took 0.158 seconds.
                Mean final reward:        10.0000
                Mean return:              5.7812
                Policy entropy:           0.2630
                Pseudo loss:              1.33208
                Total gradient norm:      0.09865
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.219
                
Iteration (10500/50001) took 0.142 seconds.
                Mean final reward:        10.0000
                Mean return:              5.9688
                Policy entropy:           0.2680
                Pseudo loss:              1.31958
                Total gradient norm:      0.10490
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.031
                
Iteration (10600/50001) took 0.157 seconds.
                Mean final reward:        9.6562
                Mean return:              5.4688
                Policy entropy:           0.2595
                Pseudo loss:              -0.39259
                Total gradient norm:      0.12502
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.867
                
Iteration (10700/50001) took 0.146 seconds.
                Mean final reward:        9.6562
                Mean return:              5.1875
                Policy entropy:           0.2867
                Pseudo loss:              0.63859
                Total gradient norm:      0.09562
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.323
                
Iteration (10800/50001) took 0.139 seconds.
                Mean final reward:        9.6562
                Mean return:              5.1250
                Policy entropy:           0.2571
                Pseudo loss:              -0.44679
                Total gradient norm:      0.13783
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.387
                
Iteration (10900/50001) took 0.056 seconds.
                Mean final reward:        10.0000
                Mean return:              5.9062
                Policy entropy:           0.2578
                Pseudo loss:              0.98667
                Total gradient norm:      0.08895
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.094
                
Iteration (11000/50001) took 0.049 seconds.
                Mean final reward:        9.6562
                Mean return:              5.7188
                Policy entropy:           0.2151
                Pseudo loss:              -0.73193
                Total gradient norm:      0.12713
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.774
                
Iteration (11100/50001) took 0.126 seconds.
                Mean final reward:        10.0000
                Mean return:              6.3438
                Policy entropy:           0.2244
                Pseudo loss:              0.51174
                Total gradient norm:      0.06707
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.656
                
Iteration (11200/50001) took 0.157 seconds.
                Mean final reward:        9.6562
                Mean return:              5.3438
                Policy entropy:           0.2409
                Pseudo loss:              -0.92988
                Total gradient norm:      0.14009
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.161
                
Iteration (11300/50001) took 0.112 seconds.
                Mean final reward:        10.0000
                Mean return:              5.7188
                Policy entropy:           0.2722
                Pseudo loss:              1.28403
                Total gradient norm:      0.16100
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.129
                
Iteration (11400/50001) took 0.078 seconds.
                Mean final reward:        9.6562
                Mean return:              5.3750
                Policy entropy:           0.2229
                Pseudo loss:              -0.11381
                Total gradient norm:      0.11626
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.967
                
Iteration (11500/50001) took 0.215 seconds.
                Mean final reward:        10.0000
                Mean return:              5.2500
                Policy entropy:           0.2352
                Pseudo loss:              1.81431
                Total gradient norm:      0.12306
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.467
                
Iteration (11600/50001) took 0.148 seconds.
                Mean final reward:        10.0000
                Mean return:              5.7188
                Policy entropy:           0.2470
                Pseudo loss:              1.24888
                Total gradient norm:      0.07507
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.129
                
Iteration (11700/50001) took 0.128 seconds.
                Mean final reward:        10.0000
                Mean return:              5.7500
                Policy entropy:           0.2216
                Pseudo loss:              1.23955
                Total gradient norm:      0.10719
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.250
                
Iteration (11800/50001) took 0.157 seconds.
                Mean final reward:        10.0000
                Mean return:              5.5312
                Policy entropy:           0.2299
                Pseudo loss:              1.34887
                Total gradient norm:      0.09904
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.323
                
Iteration (11900/50001) took 0.141 seconds.
                Mean final reward:        9.6562
                Mean return:              4.9375
                Policy entropy:           0.2734
                Pseudo loss:              -0.90066
                Total gradient norm:      0.17369
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.581
                
Iteration (12000/50001) took 0.075 seconds.
                Mean final reward:        10.0000
                Mean return:              6.4375
                Policy entropy:           0.1936
                Pseudo loss:              0.96685
                Total gradient norm:      0.09522
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.562
                
Iteration (12100/50001) took 0.051 seconds.
                Mean final reward:        10.0000
                Mean return:              6.3750
                Policy entropy:           0.2178
                Pseudo loss:              0.70633
                Total gradient norm:      0.06165
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.625
                
Iteration (12200/50001) took 0.084 seconds.
                Mean final reward:        9.6562
                Mean return:              5.4375
                Policy entropy:           0.2103
                Pseudo loss:              -0.40024
                Total gradient norm:      0.09879
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.065
                
Iteration (12300/50001) took 0.112 seconds.
                Mean final reward:        10.0000
                Mean return:              6.6875
                Policy entropy:           0.1912
                Pseudo loss:              0.49764
                Total gradient norm:      0.05987
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.312
                
Iteration (12400/50001) took 0.122 seconds.
                Mean final reward:        10.0000
                Mean return:              6.5312
                Policy entropy:           0.2265
                Pseudo loss:              0.63652
                Total gradient norm:      0.09252
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.469
                
Iteration (12500/50001) took 0.106 seconds.
                Mean final reward:        10.0000
                Mean return:              5.5625
                Policy entropy:           0.2353
                Pseudo loss:              0.88749
                Total gradient norm:      0.07569
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.438
                
Iteration (12600/50001) took 0.131 seconds.
                Mean final reward:        10.0000
                Mean return:              5.9688
                Policy entropy:           0.2378
                Pseudo loss:              0.94793
                Total gradient norm:      0.09508
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.031
                
Iteration (12700/50001) took 0.112 seconds.
                Mean final reward:        10.0000
                Mean return:              6.1875
                Policy entropy:           0.1936
                Pseudo loss:              0.64748
                Total gradient norm:      0.07217
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.812
                
Iteration (12800/50001) took 0.195 seconds.
                Mean final reward:        10.0000
                Mean return:              6.3125
                Policy entropy:           0.2174
                Pseudo loss:              0.69965
                Total gradient norm:      0.08384
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.516
                
Iteration (12900/50001) took 0.110 seconds.
                Mean final reward:        10.0000
                Mean return:              6.4688
                Policy entropy:           0.2101
                Pseudo loss:              0.61801
                Total gradient norm:      0.08010
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.531
                
Iteration (13000/50001) took 0.166 seconds.
                Mean final reward:        10.0000
                Mean return:              5.1875
                Policy entropy:           0.2406
                Pseudo loss:              1.08918
                Total gradient norm:      0.09766
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.677
                
Iteration (13100/50001) took 0.147 seconds.
                Mean final reward:        10.0000
                Mean return:              6.2188
                Policy entropy:           0.2063
                Pseudo loss:              0.80010
                Total gradient norm:      0.04820
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.613
                
Iteration (13200/50001) took 0.150 seconds.
                Mean final reward:        9.6562
                Mean return:              4.7188
                Policy entropy:           0.2581
                Pseudo loss:              0.05213
                Total gradient norm:      0.13010
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.806
                
Iteration (13300/50001) took 0.114 seconds.
                Mean final reward:        10.0000
                Mean return:              6.2812
                Policy entropy:           0.1890
                Pseudo loss:              0.64763
                Total gradient norm:      0.07978
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.719
                
Iteration (13400/50001) took 0.130 seconds.
                Mean final reward:        10.0000
                Mean return:              5.7812
                Policy entropy:           0.2263
                Pseudo loss:              1.22074
                Total gradient norm:      0.10830
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.065
                
Iteration (13500/50001) took 0.289 seconds.
                Mean final reward:        10.0000
                Mean return:              6.1875
                Policy entropy:           0.1761
                Pseudo loss:              0.73956
                Total gradient norm:      0.08394
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.812
                
Iteration (13600/50001) took 0.074 seconds.
                Mean final reward:        9.6562
                Mean return:              5.9375
                Policy entropy:           0.1828
                Pseudo loss:              -0.95945
                Total gradient norm:      0.16038
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.548
                
Iteration (13700/50001) took 0.137 seconds.
                Mean final reward:        10.0000
                Mean return:              6.1875
                Policy entropy:           0.1979
                Pseudo loss:              0.79029
                Total gradient norm:      0.06015
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.812
                
Iteration (13800/50001) took 0.083 seconds.
                Mean final reward:        10.0000
                Mean return:              5.2188
                Policy entropy:           0.2185
                Pseudo loss:              1.46482
                Total gradient norm:      0.11689
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.500
                
Iteration (13900/50001) took 0.118 seconds.
                Mean final reward:        9.6562
                Mean return:              5.1562
                Policy entropy:           0.2356
                Pseudo loss:              0.48069
                Total gradient norm:      0.11356
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.200
                
Iteration (14000/50001) took 0.110 seconds.
                Mean final reward:        10.0000
                Mean return:              6.2500
                Policy entropy:           0.1681
                Pseudo loss:              0.77541
                Total gradient norm:      0.06458
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.750
                
Iteration (14100/50001) took 0.075 seconds.
                Mean final reward:        10.0000
                Mean return:              5.9062
                Policy entropy:           0.2085
                Pseudo loss:              0.99304
                Total gradient norm:      0.14467
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.094
                
Iteration (14200/50001) took 0.168 seconds.
                Mean final reward:        10.0000
                Mean return:              5.7812
                Policy entropy:           0.2054
                Pseudo loss:              0.83345
                Total gradient norm:      0.06776
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.065
                
Iteration (14300/50001) took 0.130 seconds.
                Mean final reward:        10.0000
                Mean return:              6.3438
                Policy entropy:           0.2091
                Pseudo loss:              0.74736
                Total gradient norm:      0.08598
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.656
                
Iteration (14400/50001) took 0.159 seconds.
                Mean final reward:        10.0000
                Mean return:              5.5625
                Policy entropy:           0.2073
                Pseudo loss:              0.90586
                Total gradient norm:      0.09074
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.290
                
Iteration (14500/50001) took 0.146 seconds.
                Mean final reward:        10.0000
                Mean return:              5.9375
                Policy entropy:           0.2160
                Pseudo loss:              0.95212
                Total gradient norm:      0.09141
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.062
                
Iteration (14600/50001) took 0.167 seconds.
                Mean final reward:        10.0000
                Mean return:              5.9375
                Policy entropy:           0.2083
                Pseudo loss:              0.81824
                Total gradient norm:      0.07623
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.062
                
Iteration (14700/50001) took 0.110 seconds.
                Mean final reward:        10.0000
                Mean return:              6.3438
                Policy entropy:           0.1792
                Pseudo loss:              0.42229
                Total gradient norm:      0.10043
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.656
                
Iteration (14800/50001) took 0.090 seconds.
                Mean final reward:        10.0000
                Mean return:              5.6875
                Policy entropy:           0.1999
                Pseudo loss:              0.76151
                Total gradient norm:      0.07329
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.312
                
Iteration (14900/50001) took 0.086 seconds.
                Mean final reward:        10.0000
                Mean return:              6.3438
                Policy entropy:           0.1812
                Pseudo loss:              0.64951
                Total gradient norm:      0.07122
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.656
                
Iteration (15000/50001) took 0.144 seconds.
                Mean final reward:        9.6562
                Mean return:              5.4062
                Policy entropy:           0.1911
                Pseudo loss:              -0.57453
                Total gradient norm:      0.11740
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.097
                
Iteration (15100/50001) took 0.168 seconds.
                Mean final reward:        10.0000
                Mean return:              5.1250
                Policy entropy:           0.2275
                Pseudo loss:              0.96299
                Total gradient norm:      0.09509
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.875
                
Iteration (15200/50001) took 0.064 seconds.
                Mean final reward:        10.0000
                Mean return:              6.0625
                Policy entropy:           0.2134
                Pseudo loss:              0.73729
                Total gradient norm:      0.07878
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.938
                
Iteration (15300/50001) took 0.093 seconds.
                Mean final reward:        10.0000
                Mean return:              5.8438
                Policy entropy:           0.1866
                Pseudo loss:              0.65986
                Total gradient norm:      0.07175
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.156
                
Iteration (15400/50001) took 0.123 seconds.
                Mean final reward:        10.0000
                Mean return:              6.3438
                Policy entropy:           0.1710
                Pseudo loss:              0.31331
                Total gradient norm:      0.06119
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.656
                
Iteration (15500/50001) took 0.102 seconds.
                Mean final reward:        10.0000
                Mean return:              6.5938
                Policy entropy:           0.1785
                Pseudo loss:              0.47531
                Total gradient norm:      0.06758
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.406
                
Iteration (15600/50001) took 0.182 seconds.
                Mean final reward:        10.0000
                Mean return:              5.6875
                Policy entropy:           0.2100
                Pseudo loss:              0.67804
                Total gradient norm:      0.06295
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.312
                
Iteration (15700/50001) took 0.105 seconds.
                Mean final reward:        10.0000
                Mean return:              6.1875
                Policy entropy:           0.2244
                Pseudo loss:              0.63775
                Total gradient norm:      0.05785
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.812
                
Iteration (15800/50001) took 0.137 seconds.
                Mean final reward:        10.0000
                Mean return:              5.9688
                Policy entropy:           0.2138
                Pseudo loss:              0.56921
                Total gradient norm:      0.08210
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.031
                
Iteration (15900/50001) took 0.131 seconds.
                Mean final reward:        10.0000
                Mean return:              5.6562
                Policy entropy:           0.2134
                Pseudo loss:              0.89169
                Total gradient norm:      0.07229
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.344
                
Iteration (16000/50001) took 0.148 seconds.
                Mean final reward:        10.0000
                Mean return:              5.8750
                Policy entropy:           0.2101
                Pseudo loss:              0.63987
                Total gradient norm:      0.08612
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.125
                
Iteration (16100/50001) took 0.148 seconds.
                Mean final reward:        10.0000
                Mean return:              5.6562
                Policy entropy:           0.2169
                Pseudo loss:              0.79638
                Total gradient norm:      0.07090
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.194
                
Iteration (16200/50001) took 0.069 seconds.
                Mean final reward:        10.0000
                Mean return:              6.2188
                Policy entropy:           0.1686
                Pseudo loss:              0.42082
                Total gradient norm:      0.07067
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.781
                
Iteration (16300/50001) took 0.073 seconds.
                Mean final reward:        10.0000
                Mean return:              6.3750
                Policy entropy:           0.1740
                Pseudo loss:              0.41075
                Total gradient norm:      0.03755
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.625
                
Iteration (16400/50001) took 0.162 seconds.
                Mean final reward:        10.0000
                Mean return:              5.4688
                Policy entropy:           0.2492
                Pseudo loss:              0.99319
                Total gradient norm:      0.06655
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.387
                
Iteration (16500/50001) took 0.056 seconds.
                Mean final reward:        10.0000
                Mean return:              5.5625
                Policy entropy:           0.1983
                Pseudo loss:              0.66981
                Total gradient norm:      0.05567
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.438
                
Iteration (16600/50001) took 0.105 seconds.
                Mean final reward:        10.0000
                Mean return:              6.4062
                Policy entropy:           0.1830
                Pseudo loss:              0.37404
                Total gradient norm:      0.11280
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.594
                
Iteration (16700/50001) took 0.142 seconds.
                Mean final reward:        10.0000
                Mean return:              6.0000
                Policy entropy:           0.1948
                Pseudo loss:              0.81995
                Total gradient norm:      0.06474
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.000
                
Iteration (16800/50001) took 0.073 seconds.
                Mean final reward:        10.0000
                Mean return:              6.1875
                Policy entropy:           0.1919
                Pseudo loss:              0.43304
                Total gradient norm:      0.04985
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.812
                
Iteration (16900/50001) took 0.105 seconds.
                Mean final reward:        10.0000
                Mean return:              5.9062
                Policy entropy:           0.1856
                Pseudo loss:              0.73003
                Total gradient norm:      0.05959
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.094
                
Iteration (17000/50001) took 0.175 seconds.
                Mean final reward:        10.0000
                Mean return:              5.4688
                Policy entropy:           0.1978
                Pseudo loss:              0.56880
                Total gradient norm:      0.06893
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.531
                
Iteration (17100/50001) took 0.135 seconds.
                Mean final reward:        10.0000
                Mean return:              5.8438
                Policy entropy:           0.2106
                Pseudo loss:              0.86825
                Total gradient norm:      0.07528
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.000
                
Iteration (17200/50001) took 0.100 seconds.
                Mean final reward:        10.0000
                Mean return:              6.0625
                Policy entropy:           0.2089
                Pseudo loss:              0.64620
                Total gradient norm:      0.06383
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.938
                
Iteration (17300/50001) took 0.147 seconds.
                Mean final reward:        10.0000
                Mean return:              6.1250
                Policy entropy:           0.1953
                Pseudo loss:              0.45929
                Total gradient norm:      0.03414
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.875
                
Iteration (17400/50001) took 0.094 seconds.
                Mean final reward:        10.0000
                Mean return:              5.7500
                Policy entropy:           0.2277
                Pseudo loss:              0.96838
                Total gradient norm:      0.09381
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.097
                
Iteration (17500/50001) took 0.119 seconds.
                Mean final reward:        10.0000
                Mean return:              5.8438
                Policy entropy:           0.2085
                Pseudo loss:              0.91657
                Total gradient norm:      0.08241
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.156
                
Iteration (17600/50001) took 0.157 seconds.
                Mean final reward:        10.0000
                Mean return:              5.9688
                Policy entropy:           0.1875
                Pseudo loss:              0.65348
                Total gradient norm:      0.05727
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.031
                
Iteration (17700/50001) took 0.182 seconds.
                Mean final reward:        10.0000
                Mean return:              5.5625
                Policy entropy:           0.2047
                Pseudo loss:              0.81905
                Total gradient norm:      0.12329
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.438
                
Iteration (17800/50001) took 0.171 seconds.
                Mean final reward:        10.0000
                Mean return:              5.1562
                Policy entropy:           0.2080
                Pseudo loss:              0.50599
                Total gradient norm:      0.11712
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.844
                
Iteration (17900/50001) took 0.109 seconds.
                Mean final reward:        10.0000
                Mean return:              6.4688
                Policy entropy:           0.1733
                Pseudo loss:              0.42397
                Total gradient norm:      0.08647
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.531
                
Iteration (18000/50001) took 0.165 seconds.
                Mean final reward:        10.0000
                Mean return:              5.9062
                Policy entropy:           0.1887
                Pseudo loss:              0.70606
                Total gradient norm:      0.08584
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.094
                
Iteration (18100/50001) took 0.169 seconds.
                Mean final reward:        10.0000
                Mean return:              5.2812
                Policy entropy:           0.2137
                Pseudo loss:              1.21753
                Total gradient norm:      0.08809
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.581
                
Iteration (18200/50001) took 0.053 seconds.
                Mean final reward:        10.0000
                Mean return:              6.0000
                Policy entropy:           0.1587
                Pseudo loss:              0.48772
                Total gradient norm:      0.06325
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.000
                
Iteration (18300/50001) took 0.132 seconds.
                Mean final reward:        10.0000
                Mean return:              5.5625
                Policy entropy:           0.1917
                Pseudo loss:              0.62843
                Total gradient norm:      0.07415
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.438
                
Iteration (18400/50001) took 0.124 seconds.
                Mean final reward:        10.0000
                Mean return:              5.8438
                Policy entropy:           0.1872
                Pseudo loss:              0.54484
                Total gradient norm:      0.06831
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.156
                
Iteration (18500/50001) took 0.114 seconds.
                Mean final reward:        10.0000
                Mean return:              5.7500
                Policy entropy:           0.1923
                Pseudo loss:              0.54762
                Total gradient norm:      0.05282
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.250
                
Iteration (18600/50001) took 0.064 seconds.
                Mean final reward:        10.0000
                Mean return:              6.0000
                Policy entropy:           0.1824
                Pseudo loss:              0.76369
                Total gradient norm:      0.07987
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.839
                
Iteration (18700/50001) took 0.144 seconds.
                Mean final reward:        10.0000
                Mean return:              6.2812
                Policy entropy:           0.1509
                Pseudo loss:              0.55966
                Total gradient norm:      0.09057
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.719
                
Iteration (18800/50001) took 0.164 seconds.
                Mean final reward:        10.0000
                Mean return:              6.0938
                Policy entropy:           0.1717
                Pseudo loss:              0.50562
                Total gradient norm:      0.05173
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.906
                
Iteration (18900/50001) took 0.127 seconds.
                Mean final reward:        10.0000
                Mean return:              6.2500
                Policy entropy:           0.1637
                Pseudo loss:              0.49851
                Total gradient norm:      0.05311
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.750
                
Iteration (19000/50001) took 0.078 seconds.
                Mean final reward:        10.0000
                Mean return:              6.1562
                Policy entropy:           0.1644
                Pseudo loss:              0.37913
                Total gradient norm:      0.05667
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.844
                
Iteration (19100/50001) took 0.095 seconds.
                Mean final reward:        10.0000
                Mean return:              6.5312
                Policy entropy:           0.1398
                Pseudo loss:              0.40001
                Total gradient norm:      0.07933
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.469
                
Iteration (19200/50001) took 0.098 seconds.
                Mean final reward:        10.0000
                Mean return:              6.6250
                Policy entropy:           0.1542
                Pseudo loss:              0.47918
                Total gradient norm:      0.05901
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.375
                
Iteration (19300/50001) took 0.074 seconds.
                Mean final reward:        9.6562
                Mean return:              5.9688
                Policy entropy:           0.1734
                Pseudo loss:              -0.09847
                Total gradient norm:      0.07749
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.516
                
Iteration (19400/50001) took 0.082 seconds.
                Mean final reward:        10.0000
                Mean return:              6.3750
                Policy entropy:           0.1713
                Pseudo loss:              0.46048
                Total gradient norm:      0.07543
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.625
                
Iteration (19500/50001) took 0.087 seconds.
                Mean final reward:        10.0000
                Mean return:              6.1562
                Policy entropy:           0.1971
                Pseudo loss:              0.51601
                Total gradient norm:      0.09894
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.844
                
Iteration (19600/50001) took 0.114 seconds.
                Mean final reward:        10.0000
                Mean return:              6.3438
                Policy entropy:           0.1531
                Pseudo loss:              0.22060
                Total gradient norm:      0.05200
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.656
                
Iteration (19700/50001) took 0.080 seconds.
                Mean final reward:        10.0000
                Mean return:              6.3750
                Policy entropy:           0.1563
                Pseudo loss:              0.35747
                Total gradient norm:      0.06879
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.625
                
Iteration (19800/50001) took 0.071 seconds.
                Mean final reward:        10.0000
                Mean return:              6.8750
                Policy entropy:           0.1415
                Pseudo loss:              0.43359
                Total gradient norm:      0.05740
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.125
                
Iteration (19900/50001) took 0.130 seconds.
                Mean final reward:        10.0000
                Mean return:              5.5625
                Policy entropy:           0.2115
                Pseudo loss:              0.64339
                Total gradient norm:      0.06986
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.438
                
Iteration (20000/50001) took 0.095 seconds.
                Mean final reward:        10.0000
                Mean return:              6.2188
                Policy entropy:           0.1710
                Pseudo loss:              0.52575
                Total gradient norm:      0.10793
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.781
                
Iteration (20100/50001) took 0.070 seconds.
                Mean final reward:        10.0000
                Mean return:              6.3750
                Policy entropy:           0.1850
                Pseudo loss:              0.56390
                Total gradient norm:      0.05247
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.625
                
Iteration (20200/50001) took 0.103 seconds.
                Mean final reward:        10.0000
                Mean return:              6.4062
                Policy entropy:           0.1757
                Pseudo loss:              0.61568
                Total gradient norm:      0.06844
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.594
                
Iteration (20300/50001) took 0.153 seconds.
                Mean final reward:        10.0000
                Mean return:              5.6562
                Policy entropy:           0.1990
                Pseudo loss:              0.91629
                Total gradient norm:      0.08971
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.194
                
Iteration (20400/50001) took 0.081 seconds.
                Mean final reward:        10.0000
                Mean return:              5.7500
                Policy entropy:           0.1847
                Pseudo loss:              0.60084
                Total gradient norm:      0.08179
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.250
                
Iteration (20500/50001) took 0.082 seconds.
                Mean final reward:        10.0000
                Mean return:              6.0625
                Policy entropy:           0.1682
                Pseudo loss:              0.67544
                Total gradient norm:      0.05273
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.938
                
Iteration (20600/50001) took 0.127 seconds.
                Mean final reward:        10.0000
                Mean return:              6.2188
                Policy entropy:           0.1535
                Pseudo loss:              0.34206
                Total gradient norm:      0.05565
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.781
                
Iteration (20700/50001) took 0.074 seconds.
                Mean final reward:        10.0000
                Mean return:              5.9688
                Policy entropy:           0.1752
                Pseudo loss:              0.41049
                Total gradient norm:      0.09097
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.031
                
Iteration (20800/50001) took 0.112 seconds.
                Mean final reward:        10.0000
                Mean return:              5.7812
                Policy entropy:           0.1846
                Pseudo loss:              0.67669
                Total gradient norm:      0.10112
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.219
                
Iteration (20900/50001) took 0.120 seconds.
                Mean final reward:        10.0000
                Mean return:              5.9062
                Policy entropy:           0.1785
                Pseudo loss:              0.43944
                Total gradient norm:      0.05416
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.094
                
Iteration (21000/50001) took 0.086 seconds.
                Mean final reward:        9.6562
                Mean return:              5.8438
                Policy entropy:           0.1512
                Pseudo loss:              0.17899
                Total gradient norm:      0.07786
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.645
                
Iteration (21100/50001) took 0.070 seconds.
                Mean final reward:        10.0000
                Mean return:              5.5000
                Policy entropy:           0.1667
                Pseudo loss:              0.44572
                Total gradient norm:      0.05564
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.500
                
Iteration (21200/50001) took 0.084 seconds.
                Mean final reward:        10.0000
                Mean return:              5.4062
                Policy entropy:           0.1863
                Pseudo loss:              0.61297
                Total gradient norm:      0.07918
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.594
                
Iteration (21300/50001) took 0.086 seconds.
                Mean final reward:        10.0000
                Mean return:              6.3750
                Policy entropy:           0.1647
                Pseudo loss:              0.43037
                Total gradient norm:      0.07931
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.625
                
Iteration (21400/50001) took 0.058 seconds.
                Mean final reward:        10.0000
                Mean return:              6.0000
                Policy entropy:           0.1571
                Pseudo loss:              0.30527
                Total gradient norm:      0.03989
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.000
                
Iteration (21500/50001) took 0.119 seconds.
                Mean final reward:        10.0000
                Mean return:              6.0000
                Policy entropy:           0.1556
                Pseudo loss:              0.33021
                Total gradient norm:      0.04621
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.000
                
Iteration (21600/50001) took 0.157 seconds.
                Mean final reward:        10.0000
                Mean return:              6.5625
                Policy entropy:           0.1585
                Pseudo loss:              0.27573
                Total gradient norm:      0.06104
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.438
                
Iteration (21700/50001) took 0.145 seconds.
                Mean final reward:        10.0000
                Mean return:              5.9688
                Policy entropy:           0.1846
                Pseudo loss:              0.46315
                Total gradient norm:      0.05497
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.031
                
Iteration (21800/50001) took 0.110 seconds.
                Mean final reward:        10.0000
                Mean return:              6.0000
                Policy entropy:           0.1949
                Pseudo loss:              0.62505
                Total gradient norm:      0.05597
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.000
                
Iteration (21900/50001) took 0.178 seconds.
                Mean final reward:        10.0000
                Mean return:              5.7188
                Policy entropy:           0.1634
                Pseudo loss:              0.71675
                Total gradient norm:      0.09091
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.281
                
Iteration (22000/50001) took 0.058 seconds.
                Mean final reward:        10.0000
                Mean return:              6.1562
                Policy entropy:           0.1415
                Pseudo loss:              0.52953
                Total gradient norm:      0.05029
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.844
                
Iteration (22100/50001) took 0.174 seconds.
                Mean final reward:        10.0000
                Mean return:              6.1875
                Policy entropy:           0.1748
                Pseudo loss:              0.44621
                Total gradient norm:      0.08417
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.812
                
Iteration (22200/50001) took 0.135 seconds.
                Mean final reward:        10.0000
                Mean return:              6.2500
                Policy entropy:           0.1368
                Pseudo loss:              0.36111
                Total gradient norm:      0.04632
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.750
                
Iteration (22300/50001) took 0.167 seconds.
                Mean final reward:        10.0000
                Mean return:              5.7188
                Policy entropy:           0.1814
                Pseudo loss:              0.74061
                Total gradient norm:      0.06005
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.281
                
Iteration (22400/50001) took 0.144 seconds.
                Mean final reward:        10.0000
                Mean return:              6.0938
                Policy entropy:           0.1876
                Pseudo loss:              0.54178
                Total gradient norm:      0.05522
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.906
                
Iteration (22500/50001) took 0.132 seconds.
                Mean final reward:        10.0000
                Mean return:              5.3438
                Policy entropy:           0.2027
                Pseudo loss:              0.65561
                Total gradient norm:      0.09521
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.656
                
Iteration (22600/50001) took 0.081 seconds.
                Mean final reward:        10.0000
                Mean return:              6.0625
                Policy entropy:           0.1546
                Pseudo loss:              0.53384
                Total gradient norm:      0.05731
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.938
                
Iteration (22700/50001) took 0.083 seconds.
                Mean final reward:        10.0000
                Mean return:              5.5625
                Policy entropy:           0.2064
                Pseudo loss:              0.68643
                Total gradient norm:      0.07267
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.290
                
Iteration (22800/50001) took 0.084 seconds.
                Mean final reward:        10.0000
                Mean return:              5.6562
                Policy entropy:           0.1778
                Pseudo loss:              0.68184
                Total gradient norm:      0.06338
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.344
                
Iteration (22900/50001) took 0.116 seconds.
                Mean final reward:        10.0000
                Mean return:              5.9062
                Policy entropy:           0.1844
                Pseudo loss:              0.52441
                Total gradient norm:      0.05333
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.094
                
Iteration (23000/50001) took 0.086 seconds.
                Mean final reward:        10.0000
                Mean return:              5.6875
                Policy entropy:           0.1750
                Pseudo loss:              0.56652
                Total gradient norm:      0.08042
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.312
                
Iteration (23100/50001) took 0.143 seconds.
                Mean final reward:        10.0000
                Mean return:              6.0625
                Policy entropy:           0.1709
                Pseudo loss:              0.59654
                Total gradient norm:      0.05295
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.938
                
Iteration (23200/50001) took 0.077 seconds.
                Mean final reward:        10.0000
                Mean return:              6.1250
                Policy entropy:           0.1402
                Pseudo loss:              0.45199
                Total gradient norm:      0.07517
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.875
                
Iteration (23300/50001) took 0.068 seconds.
                Mean final reward:        10.0000
                Mean return:              6.2812
                Policy entropy:           0.1800
                Pseudo loss:              0.74706
                Total gradient norm:      0.09308
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.719
                
Iteration (23400/50001) took 0.090 seconds.
                Mean final reward:        10.0000
                Mean return:              6.8438
                Policy entropy:           0.1219
                Pseudo loss:              0.51058
                Total gradient norm:      0.06946
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.156
                
Iteration (23500/50001) took 0.066 seconds.
                Mean final reward:        10.0000
                Mean return:              6.8438
                Policy entropy:           0.1341
                Pseudo loss:              0.31171
                Total gradient norm:      0.02912
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.156
                
Iteration (23600/50001) took 0.097 seconds.
                Mean final reward:        10.0000
                Mean return:              5.7812
                Policy entropy:           0.1749
                Pseudo loss:              0.63940
                Total gradient norm:      0.06583
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.219
                
Iteration (23700/50001) took 0.117 seconds.
                Mean final reward:        10.0000
                Mean return:              6.1250
                Policy entropy:           0.1387
                Pseudo loss:              0.40023
                Total gradient norm:      0.06169
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.875
                
Iteration (23800/50001) took 0.068 seconds.
                Mean final reward:        10.0000
                Mean return:              5.9062
                Policy entropy:           0.1843
                Pseudo loss:              0.63050
                Total gradient norm:      0.13942
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.094
                
Iteration (23900/50001) took 0.110 seconds.
                Mean final reward:        10.0000
                Mean return:              6.3125
                Policy entropy:           0.1614
                Pseudo loss:              0.45646
                Total gradient norm:      0.04545
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.688
                
Iteration (24000/50001) took 0.132 seconds.
                Mean final reward:        10.0000
                Mean return:              6.1562
                Policy entropy:           0.1830
                Pseudo loss:              0.67207
                Total gradient norm:      0.10424
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.844
                
Iteration (24100/50001) took 0.096 seconds.
                Mean final reward:        10.0000
                Mean return:              5.5625
                Policy entropy:           0.1873
                Pseudo loss:              0.52725
                Total gradient norm:      0.04881
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.438
                
Iteration (24200/50001) took 0.101 seconds.
                Mean final reward:        10.0000
                Mean return:              6.1250
                Policy entropy:           0.1685
                Pseudo loss:              0.51638
                Total gradient norm:      0.07802
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.875
                
Iteration (24300/50001) took 0.114 seconds.
                Mean final reward:        10.0000
                Mean return:              6.5312
                Policy entropy:           0.1367
                Pseudo loss:              0.41992
                Total gradient norm:      0.05841
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.469
                
Iteration (24400/50001) took 0.057 seconds.
                Mean final reward:        10.0000
                Mean return:              6.1875
                Policy entropy:           0.1363
                Pseudo loss:              0.31189
                Total gradient norm:      0.05188
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.812
                
Iteration (24500/50001) took 0.131 seconds.
                Mean final reward:        10.0000
                Mean return:              6.1875
                Policy entropy:           0.1715
                Pseudo loss:              0.33980
                Total gradient norm:      0.06213
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.812
                
Iteration (24600/50001) took 0.188 seconds.
                Mean final reward:        10.0000
                Mean return:              5.4062
                Policy entropy:           0.1833
                Pseudo loss:              0.66007
                Total gradient norm:      0.06886
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.594
                
Iteration (24700/50001) took 0.079 seconds.
                Mean final reward:        10.0000
                Mean return:              6.0625
                Policy entropy:           0.1623
                Pseudo loss:              0.75658
                Total gradient norm:      0.08957
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.938
                
Iteration (24800/50001) took 0.165 seconds.
                Mean final reward:        10.0000
                Mean return:              6.1875
                Policy entropy:           0.1576
                Pseudo loss:              0.47984
                Total gradient norm:      0.04870
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.812
                
Iteration (24900/50001) took 0.052 seconds.
                Mean final reward:        10.0000
                Mean return:              6.1250
                Policy entropy:           0.1695
                Pseudo loss:              0.43884
                Total gradient norm:      0.06225
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.875
                
Iteration (25000/50001) took 0.108 seconds.
                Mean final reward:        10.0000
                Mean return:              6.0000
                Policy entropy:           0.1269
                Pseudo loss:              0.37148
                Total gradient norm:      0.04079
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.000
                
Iteration (25100/50001) took 0.134 seconds.
                Mean final reward:        9.6562
                Mean return:              5.4375
                Policy entropy:           0.1813
                Pseudo loss:              0.14405
                Total gradient norm:      0.07361
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.065
                
Iteration (25200/50001) took 0.169 seconds.
                Mean final reward:        10.0000
                Mean return:              6.0312
                Policy entropy:           0.1371
                Pseudo loss:              0.53363
                Total gradient norm:      0.08102
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.969
                
Iteration (25300/50001) took 0.134 seconds.
                Mean final reward:        10.0000
                Mean return:              5.6250
                Policy entropy:           0.1737
                Pseudo loss:              0.60876
                Total gradient norm:      0.07154
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.375
                
Iteration (25400/50001) took 0.080 seconds.
                Mean final reward:        10.0000
                Mean return:              6.0938
                Policy entropy:           0.1522
                Pseudo loss:              0.37564
                Total gradient norm:      0.06446
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.906
                
Iteration (25500/50001) took 0.156 seconds.
                Mean final reward:        10.0000
                Mean return:              5.4062
                Policy entropy:           0.1945
                Pseudo loss:              0.57057
                Total gradient norm:      0.06372
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.594
                
Iteration (25600/50001) took 0.136 seconds.
                Mean final reward:        10.0000
                Mean return:              6.2500
                Policy entropy:           0.1603
                Pseudo loss:              0.52487
                Total gradient norm:      0.11823
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.750
                
Iteration (25700/50001) took 0.104 seconds.
                Mean final reward:        10.0000
                Mean return:              5.9375
                Policy entropy:           0.1452
                Pseudo loss:              0.44035
                Total gradient norm:      0.08808
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.062
                
Iteration (25800/50001) took 0.156 seconds.
                Mean final reward:        10.0000
                Mean return:              5.2812
                Policy entropy:           0.1743
                Pseudo loss:              0.75741
                Total gradient norm:      0.08461
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.719
                
Iteration (25900/50001) took 0.151 seconds.
                Mean final reward:        10.0000
                Mean return:              5.5312
                Policy entropy:           0.1910
                Pseudo loss:              0.43239
                Total gradient norm:      0.05568
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.469
                
Iteration (26000/50001) took 0.144 seconds.
                Mean final reward:        10.0000
                Mean return:              6.4688
                Policy entropy:           0.1628
                Pseudo loss:              0.59257
                Total gradient norm:      0.09527
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.531
                
Iteration (26100/50001) took 0.176 seconds.
                Mean final reward:        10.0000
                Mean return:              5.7812
                Policy entropy:           0.1557
                Pseudo loss:              0.62077
                Total gradient norm:      0.10273
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.219
                
Iteration (26200/50001) took 0.102 seconds.
                Mean final reward:        10.0000
                Mean return:              6.6562
                Policy entropy:           0.1532
                Pseudo loss:              0.24960
                Total gradient norm:      0.07746
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.344
                
Iteration (26300/50001) took 0.125 seconds.
                Mean final reward:        10.0000
                Mean return:              5.5312
                Policy entropy:           0.1912
                Pseudo loss:              0.79476
                Total gradient norm:      0.09641
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.469
                
Iteration (26400/50001) took 0.092 seconds.
                Mean final reward:        10.0000
                Mean return:              6.5938
                Policy entropy:           0.1393
                Pseudo loss:              0.38834
                Total gradient norm:      0.05138
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.406
                
Iteration (26500/50001) took 0.062 seconds.
                Mean final reward:        10.0000
                Mean return:              5.6562
                Policy entropy:           0.1617
                Pseudo loss:              0.37202
                Total gradient norm:      0.06471
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.344
                
Iteration (26600/50001) took 0.081 seconds.
                Mean final reward:        10.0000
                Mean return:              6.0312
                Policy entropy:           0.2032
                Pseudo loss:              0.79703
                Total gradient norm:      0.15424
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.969
                
Iteration (26700/50001) took 0.126 seconds.
                Mean final reward:        10.0000
                Mean return:              6.0312
                Policy entropy:           0.1294
                Pseudo loss:              0.30301
                Total gradient norm:      0.03541
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.969
                
Iteration (26800/50001) took 0.119 seconds.
                Mean final reward:        10.0000
                Mean return:              6.0938
                Policy entropy:           0.1458
                Pseudo loss:              0.36314
                Total gradient norm:      0.03530
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.906
                
Iteration (26900/50001) took 0.109 seconds.
                Mean final reward:        10.0000
                Mean return:              5.9688
                Policy entropy:           0.1618
                Pseudo loss:              0.45488
                Total gradient norm:      0.04785
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.031
                
Iteration (27000/50001) took 0.109 seconds.
                Mean final reward:        10.0000
                Mean return:              5.7188
                Policy entropy:           0.1668
                Pseudo loss:              0.34664
                Total gradient norm:      0.03660
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.281
                
Iteration (27100/50001) took 0.058 seconds.
                Mean final reward:        9.6562
                Mean return:              5.6562
                Policy entropy:           0.1802
                Pseudo loss:              0.24036
                Total gradient norm:      0.12670
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.839
                
Iteration (27200/50001) took 0.106 seconds.
                Mean final reward:        10.0000
                Mean return:              6.0312
                Policy entropy:           0.1524
                Pseudo loss:              0.36546
                Total gradient norm:      0.05055
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.969
                
Iteration (27300/50001) took 0.137 seconds.
                Mean final reward:        10.0000
                Mean return:              6.1562
                Policy entropy:           0.1133
                Pseudo loss:              0.33761
                Total gradient norm:      0.03903
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.844
                
Iteration (27400/50001) took 0.104 seconds.
                Mean final reward:        10.0000
                Mean return:              5.9375
                Policy entropy:           0.1615
                Pseudo loss:              0.34150
                Total gradient norm:      0.08015
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.062
                
Iteration (27500/50001) took 0.161 seconds.
                Mean final reward:        10.0000
                Mean return:              6.0938
                Policy entropy:           0.1426
                Pseudo loss:              0.31546
                Total gradient norm:      0.06878
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.906
                
Iteration (27600/50001) took 0.213 seconds.
                Mean final reward:        10.0000
                Mean return:              6.0938
                Policy entropy:           0.1337
                Pseudo loss:              0.62106
                Total gradient norm:      0.06414
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.906
                
Iteration (27700/50001) took 0.165 seconds.
                Mean final reward:        10.0000
                Mean return:              5.5312
                Policy entropy:           0.1535
                Pseudo loss:              0.44119
                Total gradient norm:      0.04240
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.469
                
Iteration (27800/50001) took 0.162 seconds.
                Mean final reward:        10.0000
                Mean return:              5.6250
                Policy entropy:           0.1974
                Pseudo loss:              0.83106
                Total gradient norm:      0.15983
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.375
                
Iteration (27900/50001) took 0.163 seconds.
                Mean final reward:        10.0000
                Mean return:              5.1250
                Policy entropy:           0.2026
                Pseudo loss:              0.60611
                Total gradient norm:      0.05166
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.742
                
Iteration (28000/50001) took 0.126 seconds.
                Mean final reward:        10.0000
                Mean return:              6.2812
                Policy entropy:           0.1541
                Pseudo loss:              0.41057
                Total gradient norm:      0.04492
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.719
                
Iteration (28100/50001) took 0.164 seconds.
                Mean final reward:        10.0000
                Mean return:              5.9375
                Policy entropy:           0.1654
                Pseudo loss:              0.50917
                Total gradient norm:      0.10176
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.062
                
Iteration (28200/50001) took 0.157 seconds.
                Mean final reward:        10.0000
                Mean return:              6.0625
                Policy entropy:           0.1716
                Pseudo loss:              0.68931
                Total gradient norm:      0.12835
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.938
                
Iteration (28300/50001) took 0.138 seconds.
                Mean final reward:        10.0000
                Mean return:              6.1875
                Policy entropy:           0.1504
                Pseudo loss:              0.38769
                Total gradient norm:      0.04836
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.812
                
Iteration (28400/50001) took 0.108 seconds.
                Mean final reward:        10.0000
                Mean return:              5.2500
                Policy entropy:           0.1597
                Pseudo loss:              0.39826
                Total gradient norm:      0.05210
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.750
                
Iteration (28500/50001) took 0.109 seconds.
                Mean final reward:        10.0000
                Mean return:              6.5000
                Policy entropy:           0.1431
                Pseudo loss:              0.38733
                Total gradient norm:      0.04996
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.500
                
Iteration (28600/50001) took 0.146 seconds.
                Mean final reward:        10.0000
                Mean return:              6.1250
                Policy entropy:           0.1684
                Pseudo loss:              0.49577
                Total gradient norm:      0.09949
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.875
                
Iteration (28700/50001) took 0.131 seconds.
                Mean final reward:        10.0000
                Mean return:              6.0625
                Policy entropy:           0.1574
                Pseudo loss:              0.46164
                Total gradient norm:      0.11786
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.938
                
Iteration (28800/50001) took 0.117 seconds.
                Mean final reward:        10.0000
                Mean return:              5.9375
                Policy entropy:           0.1855
                Pseudo loss:              0.59671
                Total gradient norm:      0.06322
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.062
                
Iteration (28900/50001) took 0.078 seconds.
                Mean final reward:        9.6562
                Mean return:              5.9688
                Policy entropy:           0.1451
                Pseudo loss:              0.17002
                Total gradient norm:      0.06864
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.516
                
Iteration (29000/50001) took 0.119 seconds.
                Mean final reward:        10.0000
                Mean return:              5.6250
                Policy entropy:           0.1745
                Pseudo loss:              0.76141
                Total gradient norm:      0.07414
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.375
                
Iteration (29100/50001) took 0.174 seconds.
                Mean final reward:        10.0000
                Mean return:              5.7500
                Policy entropy:           0.1573
                Pseudo loss:              0.46866
                Total gradient norm:      0.05726
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.250
                
Iteration (29200/50001) took 0.152 seconds.
                Mean final reward:        10.0000
                Mean return:              6.1562
                Policy entropy:           0.1769
                Pseudo loss:              0.53700
                Total gradient norm:      0.09883
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.844
                
Iteration (29300/50001) took 0.154 seconds.
                Mean final reward:        10.0000
                Mean return:              5.5938
                Policy entropy:           0.1769
                Pseudo loss:              0.64923
                Total gradient norm:      0.03903
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.406
                
Iteration (29400/50001) took 0.104 seconds.
                Mean final reward:        10.0000
                Mean return:              5.1562
                Policy entropy:           0.1861
                Pseudo loss:              0.66888
                Total gradient norm:      0.10154
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.844
                
Iteration (29500/50001) took 0.091 seconds.
                Mean final reward:        10.0000
                Mean return:              6.2188
                Policy entropy:           0.1465
                Pseudo loss:              0.37396
                Total gradient norm:      0.05186
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.781
                
Iteration (29600/50001) took 0.077 seconds.
                Mean final reward:        10.0000
                Mean return:              5.6875
                Policy entropy:           0.1596
                Pseudo loss:              0.46006
                Total gradient norm:      0.06975
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.312
                
Iteration (29700/50001) took 0.099 seconds.
                Mean final reward:        10.0000
                Mean return:              5.8125
                Policy entropy:           0.1651
                Pseudo loss:              0.40424
                Total gradient norm:      0.05394
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.188
                
Iteration (29800/50001) took 0.106 seconds.
                Mean final reward:        10.0000
                Mean return:              6.7812
                Policy entropy:           0.1331
                Pseudo loss:              0.67382
                Total gradient norm:      0.10334
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.219
                
Iteration (29900/50001) took 0.134 seconds.
                Mean final reward:        9.3125
                Mean return:              4.9062
                Policy entropy:           0.2008
                Pseudo loss:              0.24096
                Total gradient norm:      0.09003
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.100
                
Iteration (30000/50001) took 0.145 seconds.
                Mean final reward:        10.0000
                Mean return:              6.0312
                Policy entropy:           0.1680
                Pseudo loss:              0.29037
                Total gradient norm:      0.10246
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.969
                
Iteration (30100/50001) took 0.143 seconds.
                Mean final reward:        10.0000
                Mean return:              6.2188
                Policy entropy:           0.1514
                Pseudo loss:              0.72467
                Total gradient norm:      0.09676
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.781
                
Iteration (30200/50001) took 0.082 seconds.
                Mean final reward:        10.0000
                Mean return:              5.8125
                Policy entropy:           0.1791
                Pseudo loss:              0.51491
                Total gradient norm:      0.03509
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.188
                
Iteration (30300/50001) took 0.066 seconds.
                Mean final reward:        10.0000
                Mean return:              6.4375
                Policy entropy:           0.1330
                Pseudo loss:              0.39041
                Total gradient norm:      0.05679
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.562
                
Iteration (30400/50001) took 0.066 seconds.
                Mean final reward:        10.0000
                Mean return:              6.1875
                Policy entropy:           0.1588
                Pseudo loss:              0.34346
                Total gradient norm:      0.09289
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.812
                
Iteration (30500/50001) took 0.075 seconds.
                Mean final reward:        10.0000
                Mean return:              5.8438
                Policy entropy:           0.1632
                Pseudo loss:              0.40793
                Total gradient norm:      0.11945
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.156
                
Iteration (30600/50001) took 0.073 seconds.
                Mean final reward:        10.0000
                Mean return:              5.6562
                Policy entropy:           0.1491
                Pseudo loss:              0.28185
                Total gradient norm:      0.08169
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.344
                
Iteration (30700/50001) took 0.071 seconds.
                Mean final reward:        10.0000
                Mean return:              6.1250
                Policy entropy:           0.1577
                Pseudo loss:              0.67332
                Total gradient norm:      0.10375
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.875
                
Iteration (30800/50001) took 0.078 seconds.
                Mean final reward:        10.0000
                Mean return:              5.6562
                Policy entropy:           0.1774
                Pseudo loss:              0.57872
                Total gradient norm:      0.09469
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.344
                
Iteration (30900/50001) took 0.143 seconds.
                Mean final reward:        10.0000
                Mean return:              5.8125
                Policy entropy:           0.1250
                Pseudo loss:              0.36634
                Total gradient norm:      0.05989
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.188
                
Iteration (31000/50001) took 0.060 seconds.
                Mean final reward:        10.0000
                Mean return:              6.1875
                Policy entropy:           0.1196
                Pseudo loss:              0.39493
                Total gradient norm:      0.06783
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.812
                
Iteration (31100/50001) took 0.101 seconds.
                Mean final reward:        10.0000
                Mean return:              5.8750
                Policy entropy:           0.1647
                Pseudo loss:              0.32415
                Total gradient norm:      0.08736
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.125
                
Iteration (31200/50001) took 0.124 seconds.
                Mean final reward:        10.0000
                Mean return:              6.4062
                Policy entropy:           0.1453
                Pseudo loss:              0.31852
                Total gradient norm:      0.09909
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.594
                
Iteration (31300/50001) took 0.156 seconds.
                Mean final reward:        10.0000
                Mean return:              5.2188
                Policy entropy:           0.1746
                Pseudo loss:              0.62296
                Total gradient norm:      0.07065
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.781
                
Iteration (31400/50001) took 0.107 seconds.
                Mean final reward:        10.0000
                Mean return:              5.4688
                Policy entropy:           0.1481
                Pseudo loss:              0.36501
                Total gradient norm:      0.06246
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.531
                
Iteration (31500/50001) took 0.078 seconds.
                Mean final reward:        10.0000
                Mean return:              5.7188
                Policy entropy:           0.1590
                Pseudo loss:              0.53182
                Total gradient norm:      0.09400
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.281
                
Iteration (31600/50001) took 0.137 seconds.
                Mean final reward:        10.0000
                Mean return:              5.6250
                Policy entropy:           0.1769
                Pseudo loss:              0.52689
                Total gradient norm:      0.10112
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.375
                
Iteration (31700/50001) took 0.078 seconds.
                Mean final reward:        10.0000
                Mean return:              5.7812
                Policy entropy:           0.1646
                Pseudo loss:              0.40983
                Total gradient norm:      0.04021
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.219
                
Iteration (31800/50001) took 0.077 seconds.
                Mean final reward:        10.0000
                Mean return:              5.8750
                Policy entropy:           0.1384
                Pseudo loss:              0.22030
                Total gradient norm:      0.06596
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.125
                
Iteration (31900/50001) took 0.115 seconds.
                Mean final reward:        10.0000
                Mean return:              6.1250
                Policy entropy:           0.1372
                Pseudo loss:              0.36160
                Total gradient norm:      0.05326
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.875
                
Iteration (32000/50001) took 0.099 seconds.
                Mean final reward:        10.0000
                Mean return:              5.5000
                Policy entropy:           0.1700
                Pseudo loss:              0.58080
                Total gradient norm:      0.07470
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.500
                
Iteration (32100/50001) took 0.087 seconds.
                Mean final reward:        10.0000
                Mean return:              5.9062
                Policy entropy:           0.1758
                Pseudo loss:              0.70686
                Total gradient norm:      0.08830
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.094
                
Iteration (32200/50001) took 0.140 seconds.
                Mean final reward:        10.0000
                Mean return:              5.9375
                Policy entropy:           0.1474
                Pseudo loss:              0.73780
                Total gradient norm:      0.08149
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.062
                
Iteration (32300/50001) took 0.085 seconds.
                Mean final reward:        10.0000
                Mean return:              6.0938
                Policy entropy:           0.1498
                Pseudo loss:              0.37118
                Total gradient norm:      0.04185
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.906
                
Iteration (32400/50001) took 0.099 seconds.
                Mean final reward:        10.0000
                Mean return:              5.6875
                Policy entropy:           0.1906
                Pseudo loss:              0.62030
                Total gradient norm:      0.12645
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.312
                
Iteration (32500/50001) took 0.088 seconds.
                Mean final reward:        10.0000
                Mean return:              6.1250
                Policy entropy:           0.1455
                Pseudo loss:              0.44160
                Total gradient norm:      0.08478
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.875
                
Iteration (32600/50001) took 0.107 seconds.
                Mean final reward:        10.0000
                Mean return:              6.0000
                Policy entropy:           0.1602
                Pseudo loss:              0.59364
                Total gradient norm:      0.10934
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.000
                
Iteration (32700/50001) took 0.107 seconds.
                Mean final reward:        10.0000
                Mean return:              6.2188
                Policy entropy:           0.1550
                Pseudo loss:              0.49983
                Total gradient norm:      0.12055
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.781
                
Iteration (32800/50001) took 0.126 seconds.
                Mean final reward:        10.0000
                Mean return:              5.5000
                Policy entropy:           0.1673
                Pseudo loss:              0.65571
                Total gradient norm:      0.05456
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.500
                
Iteration (32900/50001) took 0.140 seconds.
                Mean final reward:        10.0000
                Mean return:              6.5312
                Policy entropy:           0.1303
                Pseudo loss:              0.22921
                Total gradient norm:      0.06171
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.469
                
Iteration (33000/50001) took 0.127 seconds.
                Mean final reward:        10.0000
                Mean return:              5.5625
                Policy entropy:           0.1712
                Pseudo loss:              0.36315
                Total gradient norm:      0.06465
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.438
                
Iteration (33100/50001) took 0.103 seconds.
                Mean final reward:        10.0000
                Mean return:              6.1562
                Policy entropy:           0.1304
                Pseudo loss:              0.35584
                Total gradient norm:      0.07814
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.844
                
Iteration (33200/50001) took 0.126 seconds.
                Mean final reward:        10.0000
                Mean return:              6.0312
                Policy entropy:           0.1682
                Pseudo loss:              0.60218
                Total gradient norm:      0.05757
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.969
                
Iteration (33300/50001) took 0.086 seconds.
                Mean final reward:        10.0000
                Mean return:              5.3438
                Policy entropy:           0.1779
                Pseudo loss:              0.69748
                Total gradient norm:      0.06473
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.516
                
Iteration (33400/50001) took 0.114 seconds.
                Mean final reward:        10.0000
                Mean return:              5.9062
                Policy entropy:           0.1676
                Pseudo loss:              0.56371
                Total gradient norm:      0.08337
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.935
                
Iteration (33500/50001) took 0.106 seconds.
                Mean final reward:        10.0000
                Mean return:              6.1250
                Policy entropy:           0.1501
                Pseudo loss:              0.45655
                Total gradient norm:      0.08115
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.875
                
Iteration (33600/50001) took 0.074 seconds.
                Mean final reward:        10.0000
                Mean return:              5.6562
                Policy entropy:           0.1753
                Pseudo loss:              0.43700
                Total gradient norm:      0.04239
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.344
                
Iteration (33700/50001) took 0.113 seconds.
                Mean final reward:        10.0000
                Mean return:              5.6250
                Policy entropy:           0.1628
                Pseudo loss:              0.34626
                Total gradient norm:      0.06136
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.375
                
Iteration (33800/50001) took 0.084 seconds.
                Mean final reward:        10.0000
                Mean return:              6.3750
                Policy entropy:           0.1204
                Pseudo loss:              0.22212
                Total gradient norm:      0.03189
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.625
                
Iteration (33900/50001) took 0.054 seconds.
                Mean final reward:        10.0000
                Mean return:              6.4062
                Policy entropy:           0.1368
                Pseudo loss:              0.36549
                Total gradient norm:      0.07619
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.594
                
Iteration (34000/50001) took 0.047 seconds.
                Mean final reward:        10.0000
                Mean return:              6.4375
                Policy entropy:           0.1382
                Pseudo loss:              0.36252
                Total gradient norm:      0.05519
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.562
                
Iteration (34100/50001) took 0.086 seconds.
                Mean final reward:        10.0000
                Mean return:              6.2812
                Policy entropy:           0.1240
                Pseudo loss:              0.28762
                Total gradient norm:      0.05340
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.719
                
Iteration (34200/50001) took 0.096 seconds.
                Mean final reward:        10.0000
                Mean return:              5.8438
                Policy entropy:           0.1406
                Pseudo loss:              0.40964
                Total gradient norm:      0.08775
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.156
                
Iteration (34300/50001) took 0.063 seconds.
                Mean final reward:        10.0000
                Mean return:              5.7812
                Policy entropy:           0.1506
                Pseudo loss:              0.43062
                Total gradient norm:      0.09642
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.219
                
Iteration (34400/50001) took 0.105 seconds.
                Mean final reward:        10.0000
                Mean return:              6.0625
                Policy entropy:           0.1560
                Pseudo loss:              0.32977
                Total gradient norm:      0.04692
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.938
                
Iteration (34500/50001) took 0.064 seconds.
                Mean final reward:        10.0000
                Mean return:              6.5000
                Policy entropy:           0.0963
                Pseudo loss:              0.35094
                Total gradient norm:      0.05797
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.500
                
Iteration (34600/50001) took 0.082 seconds.
                Mean final reward:        10.0000
                Mean return:              6.0312
                Policy entropy:           0.1416
                Pseudo loss:              0.37430
                Total gradient norm:      0.06963
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.969
                
Iteration (34700/50001) took 0.060 seconds.
                Mean final reward:        10.0000
                Mean return:              6.0312
                Policy entropy:           0.1508
                Pseudo loss:              0.45306
                Total gradient norm:      0.05978
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.969
                
Iteration (34800/50001) took 0.117 seconds.
                Mean final reward:        10.0000
                Mean return:              5.7812
                Policy entropy:           0.1858
                Pseudo loss:              0.49941
                Total gradient norm:      0.08141
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.219
                
Iteration (34900/50001) took 0.141 seconds.
                Mean final reward:        10.0000
                Mean return:              5.7812
                Policy entropy:           0.1823
                Pseudo loss:              0.41618
                Total gradient norm:      0.06724
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.219
                
Iteration (35000/50001) took 0.114 seconds.
                Mean final reward:        10.0000
                Mean return:              5.6875
                Policy entropy:           0.1608
                Pseudo loss:              0.63124
                Total gradient norm:      0.07251
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.312
                
Iteration (35100/50001) took 0.259 seconds.
                Mean final reward:        10.0000
                Mean return:              5.2188
                Policy entropy:           0.1753
                Pseudo loss:              0.66112
                Total gradient norm:      0.07047
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.645
                
Iteration (35200/50001) took 0.135 seconds.
                Mean final reward:        10.0000
                Mean return:              5.8438
                Policy entropy:           0.1611
                Pseudo loss:              0.38693
                Total gradient norm:      0.05526
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.156
                
Iteration (35300/50001) took 0.101 seconds.
                Mean final reward:        10.0000
                Mean return:              6.2500
                Policy entropy:           0.1395
                Pseudo loss:              0.33056
                Total gradient norm:      0.05184
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.750
                
Iteration (35400/50001) took 0.202 seconds.
                Mean final reward:        10.0000
                Mean return:              6.2812
                Policy entropy:           0.1181
                Pseudo loss:              0.44364
                Total gradient norm:      0.04609
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.719
                
Iteration (35500/50001) took 0.109 seconds.
                Mean final reward:        10.0000
                Mean return:              6.4375
                Policy entropy:           0.1161
                Pseudo loss:              0.16858
                Total gradient norm:      0.06278
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.562
                
Iteration (35600/50001) took 0.169 seconds.
                Mean final reward:        10.0000
                Mean return:              6.0938
                Policy entropy:           0.1394
                Pseudo loss:              0.45543
                Total gradient norm:      0.06668
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.906
                
Iteration (35700/50001) took 0.145 seconds.
                Mean final reward:        10.0000
                Mean return:              6.0000
                Policy entropy:           0.1593
                Pseudo loss:              0.55154
                Total gradient norm:      0.05399
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.000
                
Iteration (35800/50001) took 0.133 seconds.
                Mean final reward:        10.0000
                Mean return:              6.0938
                Policy entropy:           0.1471
                Pseudo loss:              0.37767
                Total gradient norm:      0.08332
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.906
                
Iteration (35900/50001) took 0.147 seconds.
                Mean final reward:        10.0000
                Mean return:              5.6875
                Policy entropy:           0.1498
                Pseudo loss:              0.42628
                Total gradient norm:      0.08807
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.312
                
Iteration (36000/50001) took 0.131 seconds.
                Mean final reward:        10.0000
                Mean return:              6.0625
                Policy entropy:           0.1115
                Pseudo loss:              0.17823
                Total gradient norm:      0.03280
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.938
                
Iteration (36100/50001) took 0.170 seconds.
                Mean final reward:        10.0000
                Mean return:              6.4375
                Policy entropy:           0.1349
                Pseudo loss:              0.43148
                Total gradient norm:      0.04458
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.562
                
Iteration (36200/50001) took 0.150 seconds.
                Mean final reward:        10.0000
                Mean return:              5.9375
                Policy entropy:           0.1560
                Pseudo loss:              0.40224
                Total gradient norm:      0.07285
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.062
                
Iteration (36300/50001) took 0.153 seconds.
                Mean final reward:        10.0000
                Mean return:              6.1562
                Policy entropy:           0.1537
                Pseudo loss:              0.34655
                Total gradient norm:      0.04117
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.844
                
Iteration (36400/50001) took 0.090 seconds.
                Mean final reward:        10.0000
                Mean return:              6.1875
                Policy entropy:           0.1615
                Pseudo loss:              0.43873
                Total gradient norm:      0.05450
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.812
                
Iteration (36500/50001) took 0.120 seconds.
                Mean final reward:        10.0000
                Mean return:              5.6562
                Policy entropy:           0.1317
                Pseudo loss:              0.50554
                Total gradient norm:      0.05707
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.344
                
Iteration (36600/50001) took 0.161 seconds.
                Mean final reward:        10.0000
                Mean return:              6.4375
                Policy entropy:           0.1439
                Pseudo loss:              0.40305
                Total gradient norm:      0.07570
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.562
                
Iteration (36700/50001) took 0.181 seconds.
                Mean final reward:        10.0000
                Mean return:              6.0000
                Policy entropy:           0.1465
                Pseudo loss:              0.38681
                Total gradient norm:      0.04289
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.000
                
Iteration (36800/50001) took 0.140 seconds.
                Mean final reward:        10.0000
                Mean return:              6.5625
                Policy entropy:           0.1110
                Pseudo loss:              0.29326
                Total gradient norm:      0.06248
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.438
                
Iteration (36900/50001) took 0.174 seconds.
                Mean final reward:        10.0000
                Mean return:              5.4062
                Policy entropy:           0.1920
                Pseudo loss:              0.53079
                Total gradient norm:      0.06936
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.594
                
Iteration (37000/50001) took 0.070 seconds.
                Mean final reward:        10.0000
                Mean return:              6.2500
                Policy entropy:           0.1474
                Pseudo loss:              0.53222
                Total gradient norm:      0.08111
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.750
                
Iteration (37100/50001) took 0.113 seconds.
                Mean final reward:        10.0000
                Mean return:              6.2812
                Policy entropy:           0.1388
                Pseudo loss:              0.51060
                Total gradient norm:      0.08045
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.719
                
Iteration (37200/50001) took 0.144 seconds.
                Mean final reward:        10.0000
                Mean return:              6.0938
                Policy entropy:           0.1326
                Pseudo loss:              0.39319
                Total gradient norm:      0.07487
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.906
                
Iteration (37300/50001) took 0.058 seconds.
                Mean final reward:        10.0000
                Mean return:              5.5000
                Policy entropy:           0.1614
                Pseudo loss:              0.50360
                Total gradient norm:      0.05934
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.500
                
Iteration (37400/50001) took 0.071 seconds.
                Mean final reward:        10.0000
                Mean return:              6.4062
                Policy entropy:           0.1538
                Pseudo loss:              0.33185
                Total gradient norm:      0.07692
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.594
                
Iteration (37500/50001) took 0.093 seconds.
                Mean final reward:        10.0000
                Mean return:              5.1562
                Policy entropy:           0.1617
                Pseudo loss:              0.61010
                Total gradient norm:      0.11884
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.844
                
Iteration (37600/50001) took 0.099 seconds.
                Mean final reward:        10.0000
                Mean return:              6.3125
                Policy entropy:           0.1490
                Pseudo loss:              0.42182
                Total gradient norm:      0.08479
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.688
                
Iteration (37700/50001) took 0.124 seconds.
                Mean final reward:        10.0000
                Mean return:              6.4688
                Policy entropy:           0.1403
                Pseudo loss:              0.38907
                Total gradient norm:      0.11440
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.531
                
Iteration (37800/50001) took 0.074 seconds.
                Mean final reward:        10.0000
                Mean return:              5.0938
                Policy entropy:           0.1740
                Pseudo loss:              0.55233
                Total gradient norm:      0.05476
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.906
                
Iteration (37900/50001) took 0.162 seconds.
                Mean final reward:        10.0000
                Mean return:              5.5000
                Policy entropy:           0.1768
                Pseudo loss:              0.67896
                Total gradient norm:      0.06606
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.500
                
Iteration (38000/50001) took 0.060 seconds.
                Mean final reward:        10.0000
                Mean return:              5.9688
                Policy entropy:           0.1311
                Pseudo loss:              0.31235
                Total gradient norm:      0.04092
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.031
                
Iteration (38100/50001) took 0.189 seconds.
                Mean final reward:        10.0000
                Mean return:              6.4375
                Policy entropy:           0.1340
                Pseudo loss:              0.36695
                Total gradient norm:      0.03568
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.562
                
Iteration (38200/50001) took 0.143 seconds.
                Mean final reward:        10.0000
                Mean return:              6.0312
                Policy entropy:           0.1180
                Pseudo loss:              0.24875
                Total gradient norm:      0.04508
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.969
                
Iteration (38300/50001) took 0.075 seconds.
                Mean final reward:        10.0000
                Mean return:              5.6250
                Policy entropy:           0.1676
                Pseudo loss:              0.52927
                Total gradient norm:      0.05829
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.375
                
Iteration (38400/50001) took 0.061 seconds.
                Mean final reward:        10.0000
                Mean return:              6.1250
                Policy entropy:           0.1562
                Pseudo loss:              0.40119
                Total gradient norm:      0.03790
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.875
                
Iteration (38500/50001) took 0.060 seconds.
                Mean final reward:        10.0000
                Mean return:              5.2812
                Policy entropy:           0.1873
                Pseudo loss:              0.48099
                Total gradient norm:      0.13307
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.719
                
Iteration (38600/50001) took 0.102 seconds.
                Mean final reward:        10.0000
                Mean return:              6.3125
                Policy entropy:           0.1552
                Pseudo loss:              0.40845
                Total gradient norm:      0.05861
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.688
                
Iteration (38700/50001) took 0.100 seconds.
                Mean final reward:        10.0000
                Mean return:              5.9062
                Policy entropy:           0.1552
                Pseudo loss:              0.37612
                Total gradient norm:      0.05203
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.094
                
Iteration (38800/50001) took 0.055 seconds.
                Mean final reward:        10.0000
                Mean return:              6.5625
                Policy entropy:           0.0929
                Pseudo loss:              0.18184
                Total gradient norm:      0.04401
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.438
                
Iteration (38900/50001) took 0.054 seconds.
                Mean final reward:        10.0000
                Mean return:              5.7812
                Policy entropy:           0.1643
                Pseudo loss:              0.77359
                Total gradient norm:      0.11706
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.219
                
Iteration (39000/50001) took 0.280 seconds.
                Mean final reward:        10.0000
                Mean return:              5.8750
                Policy entropy:           0.1675
                Pseudo loss:              0.57134
                Total gradient norm:      0.06033
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.125
                
Iteration (39100/50001) took 0.067 seconds.
                Mean final reward:        10.0000
                Mean return:              5.7812
                Policy entropy:           0.1321
                Pseudo loss:              0.35250
                Total gradient norm:      0.06492
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.219
                
Iteration (39200/50001) took 0.091 seconds.
                Mean final reward:        10.0000
                Mean return:              6.0312
                Policy entropy:           0.1487
                Pseudo loss:              0.29668
                Total gradient norm:      0.03970
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.969
                
Iteration (39300/50001) took 0.081 seconds.
                Mean final reward:        10.0000
                Mean return:              5.5312
                Policy entropy:           0.1583
                Pseudo loss:              0.44640
                Total gradient norm:      0.06045
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.469
                
Iteration (39400/50001) took 0.061 seconds.
                Mean final reward:        10.0000
                Mean return:              5.7812
                Policy entropy:           0.1459
                Pseudo loss:              0.39223
                Total gradient norm:      0.04192
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.219
                
Iteration (39500/50001) took 0.081 seconds.
                Mean final reward:        10.0000
                Mean return:              6.2188
                Policy entropy:           0.1375
                Pseudo loss:              0.48521
                Total gradient norm:      0.06384
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.781
                
Iteration (39600/50001) took 0.129 seconds.
                Mean final reward:        10.0000
                Mean return:              6.0938
                Policy entropy:           0.1163
                Pseudo loss:              0.24816
                Total gradient norm:      0.04371
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.906
                
Iteration (39700/50001) took 0.062 seconds.
                Mean final reward:        10.0000
                Mean return:              5.7812
                Policy entropy:           0.1376
                Pseudo loss:              0.48143
                Total gradient norm:      0.06206
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.219
                
Iteration (39800/50001) took 0.096 seconds.
                Mean final reward:        10.0000
                Mean return:              5.6250
                Policy entropy:           0.1444
                Pseudo loss:              0.57164
                Total gradient norm:      0.06202
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.226
                
Iteration (39900/50001) took 0.055 seconds.
                Mean final reward:        10.0000
                Mean return:              6.4375
                Policy entropy:           0.1600
                Pseudo loss:              0.34118
                Total gradient norm:      0.03357
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.562
                
Iteration (40000/50001) took 0.150 seconds.
                Mean final reward:        10.0000
                Mean return:              5.4375
                Policy entropy:           0.1579
                Pseudo loss:              0.34457
                Total gradient norm:      0.11106
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.562
                
Iteration (40100/50001) took 0.157 seconds.
                Mean final reward:        10.0000
                Mean return:              6.0312
                Policy entropy:           0.1451
                Pseudo loss:              0.52857
                Total gradient norm:      0.09165
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.969
                
Iteration (40200/50001) took 0.104 seconds.
                Mean final reward:        10.0000
                Mean return:              5.9062
                Policy entropy:           0.1519
                Pseudo loss:              0.32469
                Total gradient norm:      0.02661
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.094
                
Iteration (40300/50001) took 0.083 seconds.
                Mean final reward:        10.0000
                Mean return:              5.4375
                Policy entropy:           0.1735
                Pseudo loss:              0.52647
                Total gradient norm:      0.09836
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.562
                
Iteration (40400/50001) took 0.101 seconds.
                Mean final reward:        10.0000
                Mean return:              5.6250
                Policy entropy:           0.1526
                Pseudo loss:              0.40322
                Total gradient norm:      0.05327
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.375
                
Iteration (40500/50001) took 0.145 seconds.
                Mean final reward:        10.0000
                Mean return:              5.5000
                Policy entropy:           0.1734
                Pseudo loss:              0.52707
                Total gradient norm:      0.10625
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.500
                
Iteration (40600/50001) took 0.127 seconds.
                Mean final reward:        10.0000
                Mean return:              6.0000
                Policy entropy:           0.1372
                Pseudo loss:              0.33877
                Total gradient norm:      0.05409
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.000
                
Iteration (40700/50001) took 0.136 seconds.
                Mean final reward:        10.0000
                Mean return:              6.0000
                Policy entropy:           0.1180
                Pseudo loss:              0.36868
                Total gradient norm:      0.07312
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.000
                
Iteration (40800/50001) took 0.068 seconds.
                Mean final reward:        10.0000
                Mean return:              5.9062
                Policy entropy:           0.1524
                Pseudo loss:              0.49362
                Total gradient norm:      0.07312
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.094
                
Iteration (40900/50001) took 0.098 seconds.
                Mean final reward:        10.0000
                Mean return:              5.4375
                Policy entropy:           0.1514
                Pseudo loss:              0.48343
                Total gradient norm:      0.07152
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.562
                
Iteration (41000/50001) took 0.055 seconds.
                Mean final reward:        10.0000
                Mean return:              6.5000
                Policy entropy:           0.1286
                Pseudo loss:              0.30213
                Total gradient norm:      0.06100
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.500
                
Iteration (41100/50001) took 0.069 seconds.
                Mean final reward:        10.0000
                Mean return:              5.9688
                Policy entropy:           0.1453
                Pseudo loss:              0.43972
                Total gradient norm:      0.08232
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.031
                
Iteration (41200/50001) took 0.108 seconds.
                Mean final reward:        10.0000
                Mean return:              6.1562
                Policy entropy:           0.1168
                Pseudo loss:              0.58376
                Total gradient norm:      0.05367
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.844
                
Iteration (41300/50001) took 0.085 seconds.
                Mean final reward:        10.0000
                Mean return:              6.5938
                Policy entropy:           0.1257
                Pseudo loss:              0.23370
                Total gradient norm:      0.05758
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.406
                
Iteration (41400/50001) took 0.075 seconds.
                Mean final reward:        10.0000
                Mean return:              5.5312
                Policy entropy:           0.1540
                Pseudo loss:              0.79343
                Total gradient norm:      0.12790
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.469
                
Iteration (41500/50001) took 0.078 seconds.
                Mean final reward:        10.0000
                Mean return:              6.1875
                Policy entropy:           0.1532
                Pseudo loss:              0.49456
                Total gradient norm:      0.04885
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.812
                
Iteration (41600/50001) took 0.065 seconds.
                Mean final reward:        10.0000
                Mean return:              6.1875
                Policy entropy:           0.1288
                Pseudo loss:              0.47152
                Total gradient norm:      0.07400
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.812
                
Iteration (41700/50001) took 0.106 seconds.
                Mean final reward:        10.0000
                Mean return:              6.1875
                Policy entropy:           0.1322
                Pseudo loss:              0.25980
                Total gradient norm:      0.07223
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.812
                
Iteration (41800/50001) took 0.148 seconds.
                Mean final reward:        10.0000
                Mean return:              5.7188
                Policy entropy:           0.1632
                Pseudo loss:              0.45632
                Total gradient norm:      0.06702
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.281
                
Iteration (41900/50001) took 0.082 seconds.
                Mean final reward:        10.0000
                Mean return:              5.4688
                Policy entropy:           0.1455
                Pseudo loss:              0.29534
                Total gradient norm:      0.05645
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.531
                
Iteration (42000/50001) took 0.066 seconds.
                Mean final reward:        10.0000
                Mean return:              6.0312
                Policy entropy:           0.1456
                Pseudo loss:              0.53288
                Total gradient norm:      0.06008
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.969
                
Iteration (42100/50001) took 0.066 seconds.
                Mean final reward:        10.0000
                Mean return:              5.9375
                Policy entropy:           0.1250
                Pseudo loss:              0.52831
                Total gradient norm:      0.07000
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.062
                
Iteration (42200/50001) took 0.087 seconds.
                Mean final reward:        10.0000
                Mean return:              6.0000
                Policy entropy:           0.1377
                Pseudo loss:              0.44326
                Total gradient norm:      0.07117
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.000
                
Iteration (42300/50001) took 0.079 seconds.
                Mean final reward:        10.0000
                Mean return:              5.6875
                Policy entropy:           0.1583
                Pseudo loss:              0.35051
                Total gradient norm:      0.08618
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.312
                
Iteration (42400/50001) took 0.114 seconds.
                Mean final reward:        10.0000
                Mean return:              5.9375
                Policy entropy:           0.1482
                Pseudo loss:              0.50285
                Total gradient norm:      0.04204
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.062
                
Iteration (42500/50001) took 0.099 seconds.
                Mean final reward:        10.0000
                Mean return:              5.6562
                Policy entropy:           0.1625
                Pseudo loss:              0.51623
                Total gradient norm:      0.07118
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.344
                
Iteration (42600/50001) took 0.169 seconds.
                Mean final reward:        10.0000
                Mean return:              5.7812
                Policy entropy:           0.1582
                Pseudo loss:              0.42790
                Total gradient norm:      0.04735
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.219
                
Iteration (42700/50001) took 0.108 seconds.
                Mean final reward:        10.0000
                Mean return:              6.0938
                Policy entropy:           0.1377
                Pseudo loss:              0.22743
                Total gradient norm:      0.05422
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.906
                
Iteration (42800/50001) took 0.164 seconds.
                Mean final reward:        10.0000
                Mean return:              5.3125
                Policy entropy:           0.1675
                Pseudo loss:              0.41863
                Total gradient norm:      0.03670
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.688
                
Iteration (42900/50001) took 0.081 seconds.
                Mean final reward:        10.0000
                Mean return:              5.9062
                Policy entropy:           0.1384
                Pseudo loss:              0.41002
                Total gradient norm:      0.03659
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.094
                
Iteration (43000/50001) took 0.060 seconds.
                Mean final reward:        10.0000
                Mean return:              6.3125
                Policy entropy:           0.1318
                Pseudo loss:              0.30662
                Total gradient norm:      0.09002
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.688
                
Iteration (43100/50001) took 0.073 seconds.
                Mean final reward:        10.0000
                Mean return:              6.5312
                Policy entropy:           0.1132
                Pseudo loss:              0.21567
                Total gradient norm:      0.05637
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.469
                
Iteration (43200/50001) took 0.072 seconds.
                Mean final reward:        10.0000
                Mean return:              6.6250
                Policy entropy:           0.1363
                Pseudo loss:              0.27282
                Total gradient norm:      0.05970
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.375
                
Iteration (43300/50001) took 0.073 seconds.
                Mean final reward:        10.0000
                Mean return:              6.2500
                Policy entropy:           0.1763
                Pseudo loss:              0.64920
                Total gradient norm:      0.09669
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.750
                
Iteration (43400/50001) took 0.072 seconds.
                Mean final reward:        10.0000
                Mean return:              5.4375
                Policy entropy:           0.1621
                Pseudo loss:              0.40903
                Total gradient norm:      0.05163
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.562
                
Iteration (43500/50001) took 0.088 seconds.
                Mean final reward:        10.0000
                Mean return:              5.1875
                Policy entropy:           0.1615
                Pseudo loss:              0.43402
                Total gradient norm:      0.09317
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.812
                
Iteration (43600/50001) took 0.074 seconds.
                Mean final reward:        10.0000
                Mean return:              6.1562
                Policy entropy:           0.1623
                Pseudo loss:              0.42124
                Total gradient norm:      0.04295
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.844
                
Iteration (43700/50001) took 0.066 seconds.
                Mean final reward:        10.0000
                Mean return:              6.1562
                Policy entropy:           0.1541
                Pseudo loss:              0.33094
                Total gradient norm:      0.10496
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.844
                
Iteration (43800/50001) took 0.071 seconds.
                Mean final reward:        10.0000
                Mean return:              5.8125
                Policy entropy:           0.1578
                Pseudo loss:              0.46641
                Total gradient norm:      0.04297
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.188
                
Iteration (43900/50001) took 0.069 seconds.
                Mean final reward:        10.0000
                Mean return:              5.6562
                Policy entropy:           0.1609
                Pseudo loss:              0.57458
                Total gradient norm:      0.07225
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.344
                
Iteration (44000/50001) took 0.078 seconds.
                Mean final reward:        10.0000
                Mean return:              5.7812
                Policy entropy:           0.1483
                Pseudo loss:              0.38443
                Total gradient norm:      0.04847
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.219
                
Iteration (44100/50001) took 0.072 seconds.
                Mean final reward:        10.0000
                Mean return:              5.5938
                Policy entropy:           0.1591
                Pseudo loss:              0.41247
                Total gradient norm:      0.05722
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.406
                
Iteration (44200/50001) took 0.074 seconds.
                Mean final reward:        10.0000
                Mean return:              5.9375
                Policy entropy:           0.1379
                Pseudo loss:              0.38832
                Total gradient norm:      0.02530
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.062
                
Iteration (44300/50001) took 0.081 seconds.
                Mean final reward:        10.0000
                Mean return:              5.2500
                Policy entropy:           0.1692
                Pseudo loss:              0.80023
                Total gradient norm:      0.07068
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.750
                
Iteration (44400/50001) took 0.048 seconds.
                Mean final reward:        10.0000
                Mean return:              6.7812
                Policy entropy:           0.0998
                Pseudo loss:              0.22746
                Total gradient norm:      0.05430
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.219
                
Iteration (44500/50001) took 0.055 seconds.
                Mean final reward:        10.0000
                Mean return:              5.5312
                Policy entropy:           0.1892
                Pseudo loss:              0.69245
                Total gradient norm:      0.08555
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.469
                
Iteration (44600/50001) took 0.034 seconds.
                Mean final reward:        10.0000
                Mean return:              6.0625
                Policy entropy:           0.1413
                Pseudo loss:              0.39229
                Total gradient norm:      0.06274
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.938
                
Iteration (44700/50001) took 0.052 seconds.
                Mean final reward:        10.0000
                Mean return:              5.7188
                Policy entropy:           0.1522
                Pseudo loss:              0.48940
                Total gradient norm:      0.12914
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.281
                
Iteration (44800/50001) took 0.044 seconds.
                Mean final reward:        10.0000
                Mean return:              6.6562
                Policy entropy:           0.1078
                Pseudo loss:              0.37205
                Total gradient norm:      0.06131
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.344
                
Iteration (44900/50001) took 0.039 seconds.
                Mean final reward:        10.0000
                Mean return:              5.6875
                Policy entropy:           0.1501
                Pseudo loss:              0.41873
                Total gradient norm:      0.06723
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.312
                
Iteration (45000/50001) took 0.054 seconds.
                Mean final reward:        10.0000
                Mean return:              5.5312
                Policy entropy:           0.1567
                Pseudo loss:              0.55165
                Total gradient norm:      0.07125
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.469
                
Iteration (45100/50001) took 0.047 seconds.
                Mean final reward:        10.0000
                Mean return:              6.4062
                Policy entropy:           0.1272
                Pseudo loss:              0.43283
                Total gradient norm:      0.04844
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.594
                
Iteration (45200/50001) took 0.048 seconds.
                Mean final reward:        10.0000
                Mean return:              6.0625
                Policy entropy:           0.1273
                Pseudo loss:              0.26740
                Total gradient norm:      0.07769
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.938
                
Iteration (45300/50001) took 0.034 seconds.
                Mean final reward:        10.0000
                Mean return:              6.0000
                Policy entropy:           0.1649
                Pseudo loss:              0.40273
                Total gradient norm:      0.04970
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.000
                
Iteration (45400/50001) took 0.037 seconds.
                Mean final reward:        10.0000
                Mean return:              6.0000
                Policy entropy:           0.1518
                Pseudo loss:              0.39986
                Total gradient norm:      0.07572
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.000
                
Iteration (45500/50001) took 0.041 seconds.
                Mean final reward:        10.0000
                Mean return:              5.4375
                Policy entropy:           0.1646
                Pseudo loss:              0.36015
                Total gradient norm:      0.08560
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.562
                
Iteration (45600/50001) took 0.032 seconds.
                Mean final reward:        10.0000
                Mean return:              5.8750
                Policy entropy:           0.1621
                Pseudo loss:              0.68196
                Total gradient norm:      0.08872
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.125
                
Iteration (45700/50001) took 0.031 seconds.
                Mean final reward:        10.0000
                Mean return:              6.4688
                Policy entropy:           0.1055
                Pseudo loss:              0.38940
                Total gradient norm:      0.06350
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.531
                
Iteration (45800/50001) took 0.035 seconds.
                Mean final reward:        10.0000
                Mean return:              6.0000
                Policy entropy:           0.1522
                Pseudo loss:              0.31145
                Total gradient norm:      0.06577
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.000
                
Iteration (45900/50001) took 0.034 seconds.
                Mean final reward:        10.0000
                Mean return:              5.8750
                Policy entropy:           0.1401
                Pseudo loss:              0.30931
                Total gradient norm:      0.03890
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.125
                
Iteration (46000/50001) took 0.032 seconds.
                Mean final reward:        10.0000
                Mean return:              6.6562
                Policy entropy:           0.1481
                Pseudo loss:              0.29449
                Total gradient norm:      0.04539
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.344
                
Iteration (46100/50001) took 0.035 seconds.
                Mean final reward:        10.0000
                Mean return:              5.9688
                Policy entropy:           0.1439
                Pseudo loss:              0.35557
                Total gradient norm:      0.05099
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.031
                
Iteration (46200/50001) took 0.058 seconds.
                Mean final reward:        10.0000
                Mean return:              5.8438
                Policy entropy:           0.1761
                Pseudo loss:              0.67087
                Total gradient norm:      0.09090
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.156
                
Iteration (46300/50001) took 0.050 seconds.
                Mean final reward:        10.0000
                Mean return:              5.9375
                Policy entropy:           0.1445
                Pseudo loss:              0.36826
                Total gradient norm:      0.04274
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.062
                
Iteration (46400/50001) took 0.039 seconds.
                Mean final reward:        10.0000
                Mean return:              5.8125
                Policy entropy:           0.1438
                Pseudo loss:              0.54543
                Total gradient norm:      0.07461
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.188
                
Iteration (46500/50001) took 0.033 seconds.
                Mean final reward:        10.0000
                Mean return:              6.1562
                Policy entropy:           0.1223
                Pseudo loss:              0.28921
                Total gradient norm:      0.03891
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.844
                
Iteration (46600/50001) took 0.033 seconds.
                Mean final reward:        10.0000
                Mean return:              6.0938
                Policy entropy:           0.1438
                Pseudo loss:              0.47266
                Total gradient norm:      0.05922
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.906
                
Iteration (46700/50001) took 0.035 seconds.
                Mean final reward:        10.0000
                Mean return:              5.5312
                Policy entropy:           0.1795
                Pseudo loss:              0.52247
                Total gradient norm:      0.12328
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.469
                
Iteration (46800/50001) took 0.028 seconds.
                Mean final reward:        10.0000
                Mean return:              6.7188
                Policy entropy:           0.1083
                Pseudo loss:              0.31599
                Total gradient norm:      0.10709
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.281
                
Iteration (46900/50001) took 0.046 seconds.
                Mean final reward:        10.0000
                Mean return:              6.6562
                Policy entropy:           0.1067
                Pseudo loss:              0.22953
                Total gradient norm:      0.04706
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.344
                
Iteration (47000/50001) took 0.053 seconds.
                Mean final reward:        10.0000
                Mean return:              5.5625
                Policy entropy:           0.1689
                Pseudo loss:              0.41883
                Total gradient norm:      0.09050
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.438
                
Iteration (47100/50001) took 0.032 seconds.
                Mean final reward:        10.0000
                Mean return:              6.4062
                Policy entropy:           0.1200
                Pseudo loss:              0.29248
                Total gradient norm:      0.04132
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.594
                
Iteration (47200/50001) took 0.054 seconds.
                Mean final reward:        10.0000
                Mean return:              5.4375
                Policy entropy:           0.1606
                Pseudo loss:              0.51093
                Total gradient norm:      0.03987
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.562
                
Iteration (47300/50001) took 0.033 seconds.
                Mean final reward:        10.0000
                Mean return:              6.0312
                Policy entropy:           0.1533
                Pseudo loss:              0.35836
                Total gradient norm:      0.03722
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.969
                
Iteration (47400/50001) took 0.036 seconds.
                Mean final reward:        10.0000
                Mean return:              5.6562
                Policy entropy:           0.1511
                Pseudo loss:              0.38444
                Total gradient norm:      0.06554
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.344
                
Iteration (47500/50001) took 0.037 seconds.
                Mean final reward:        10.0000
                Mean return:              5.7188
                Policy entropy:           0.1283
                Pseudo loss:              0.44396
                Total gradient norm:      0.07223
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.281
                
Iteration (47600/50001) took 0.034 seconds.
                Mean final reward:        10.0000
                Mean return:              5.9375
                Policy entropy:           0.1148
                Pseudo loss:              0.22799
                Total gradient norm:      0.03509
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.062
                
Iteration (47700/50001) took 0.052 seconds.
                Mean final reward:        10.0000
                Mean return:              5.9375
                Policy entropy:           0.1466
                Pseudo loss:              0.41490
                Total gradient norm:      0.04883
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.062
                
Iteration (47800/50001) took 0.032 seconds.
                Mean final reward:        10.0000
                Mean return:              6.2188
                Policy entropy:           0.1484
                Pseudo loss:              0.46833
                Total gradient norm:      0.09695
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.781
                
Iteration (47900/50001) took 0.035 seconds.
                Mean final reward:        10.0000
                Mean return:              5.6250
                Policy entropy:           0.1655
                Pseudo loss:              0.36738
                Total gradient norm:      0.07147
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.375
                
Iteration (48000/50001) took 0.046 seconds.
                Mean final reward:        10.0000
                Mean return:              6.1875
                Policy entropy:           0.1253
                Pseudo loss:              0.40723
                Total gradient norm:      0.05195
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.812
                
Iteration (48100/50001) took 0.035 seconds.
                Mean final reward:        10.0000
                Mean return:              6.0625
                Policy entropy:           0.1452
                Pseudo loss:              0.44613
                Total gradient norm:      0.05109
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.938
                
Iteration (48200/50001) took 0.051 seconds.
                Mean final reward:        10.0000
                Mean return:              5.7812
                Policy entropy:           0.1446
                Pseudo loss:              0.55749
                Total gradient norm:      0.08337
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.219
                
Iteration (48300/50001) took 0.034 seconds.
                Mean final reward:        10.0000
                Mean return:              5.8125
                Policy entropy:           0.1586
                Pseudo loss:              0.47741
                Total gradient norm:      0.05425
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.188
                
Iteration (48400/50001) took 0.036 seconds.
                Mean final reward:        10.0000
                Mean return:              5.5625
                Policy entropy:           0.1598
                Pseudo loss:              0.30301
                Total gradient norm:      0.05955
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.438
                
Iteration (48500/50001) took 0.049 seconds.
                Mean final reward:        10.0000
                Mean return:              6.3125
                Policy entropy:           0.0939
                Pseudo loss:              0.25026
                Total gradient norm:      0.07058
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.688
                
Iteration (48600/50001) took 0.035 seconds.
                Mean final reward:        10.0000
                Mean return:              5.5938
                Policy entropy:           0.1684
                Pseudo loss:              0.71651
                Total gradient norm:      0.06673
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.406
                
Iteration (48700/50001) took 0.051 seconds.
                Mean final reward:        10.0000
                Mean return:              5.8750
                Policy entropy:           0.1253
                Pseudo loss:              0.26400
                Total gradient norm:      0.05766
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.125
                
Iteration (48800/50001) took 0.034 seconds.
                Mean final reward:        10.0000
                Mean return:              5.8750
                Policy entropy:           0.1528
                Pseudo loss:              0.38879
                Total gradient norm:      0.03332
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.125
                
Iteration (48900/50001) took 0.032 seconds.
                Mean final reward:        10.0000
                Mean return:              6.0312
                Policy entropy:           0.1560
                Pseudo loss:              0.54645
                Total gradient norm:      0.05951
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.969
                
Iteration (49000/50001) took 0.035 seconds.
                Mean final reward:        10.0000
                Mean return:              5.8438
                Policy entropy:           0.1468
                Pseudo loss:              0.42154
                Total gradient norm:      0.04917
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.156
                
Iteration (49100/50001) took 0.035 seconds.
                Mean final reward:        9.6562
                Mean return:              5.3125
                Policy entropy:           0.1558
                Pseudo loss:              0.25541
                Total gradient norm:      0.05882
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.194
                
Iteration (49200/50001) took 0.050 seconds.
                Mean final reward:        10.0000
                Mean return:              5.8750
                Policy entropy:           0.1492
                Pseudo loss:              0.34652
                Total gradient norm:      0.08118
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.125
                
Iteration (49300/50001) took 0.033 seconds.
                Mean final reward:        10.0000
                Mean return:              6.0312
                Policy entropy:           0.1438
                Pseudo loss:              0.42621
                Total gradient norm:      0.04408
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.969
                
Iteration (49400/50001) took 0.030 seconds.
                Mean final reward:        10.0000
                Mean return:              6.5625
                Policy entropy:           0.1159
                Pseudo loss:              0.44609
                Total gradient norm:      0.04995
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.438
                
Iteration (49500/50001) took 0.030 seconds.
                Mean final reward:        10.0000
                Mean return:              6.7500
                Policy entropy:           0.1186
                Pseudo loss:              0.28852
                Total gradient norm:      0.05264
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.250
                
Iteration (49600/50001) took 0.038 seconds.
                Mean final reward:        10.0000
                Mean return:              5.9688
                Policy entropy:           0.1547
                Pseudo loss:              0.47890
                Total gradient norm:      0.08438
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.031
                
Iteration (49700/50001) took 0.031 seconds.
                Mean final reward:        10.0000
                Mean return:              6.1562
                Policy entropy:           0.1408
                Pseudo loss:              0.34602
                Total gradient norm:      0.05888
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.844
                
Iteration (49800/50001) took 0.049 seconds.
                Mean final reward:        10.0000
                Mean return:              6.0312
                Policy entropy:           0.1370
                Pseudo loss:              0.35595
                Total gradient norm:      0.04490
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.969
                
Iteration (49900/50001) took 0.035 seconds.
                Mean final reward:        10.0000
                Mean return:              6.3125
                Policy entropy:           0.1128
                Pseudo loss:              0.33481
                Total gradient norm:      0.03873
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.688
                
Iteration (50000/50001) took 0.033 seconds.
                Mean final reward:        10.0000
                Mean return:              5.9375
                Policy entropy:           0.1496
                Pseudo loss:              0.50896
                Total gradient norm:      0.04950
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.062
                
Training took 5347.970 seconds.
