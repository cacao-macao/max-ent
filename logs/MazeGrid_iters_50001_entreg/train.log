Using device: cpu

Using optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)

Iteration (0/50001) took 0.072 seconds.
                Mean final reward:        5.8438
                Mean return:              -2.7500
                Policy entropy:           1.5815
                Pseudo loss:              31.91257
                Total gradient norm:      3.14105
                Solved trajectories:      8 / 32
                Avg steps to solve:       2.375
                
Iteration (100/50001) took 0.055 seconds.
                Mean final reward:        3.9062
                Mean return:              -5.9375
                Policy entropy:           1.5808
                Pseudo loss:              34.36516
                Total gradient norm:      3.34025
                Solved trajectories:      6 / 32
                Avg steps to solve:       5.833
                
Iteration (200/50001) took 0.047 seconds.
                Mean final reward:        5.9062
                Mean return:              -2.2188
                Policy entropy:           1.5758
                Pseudo loss:              28.28246
                Total gradient norm:      2.88830
                Solved trajectories:      11 / 32
                Avg steps to solve:       3.636
                
Iteration (300/50001) took 0.050 seconds.
                Mean final reward:        6.7500
                Mean return:              -2.0312
                Policy entropy:           1.5742
                Pseudo loss:              28.13851
                Total gradient norm:      2.76767
                Solved trajectories:      8 / 32
                Avg steps to solve:       3.125
                
Iteration (400/50001) took 0.039 seconds.
                Mean final reward:        10.4688
                Mean return:              4.0625
                Policy entropy:           1.5651
                Pseudo loss:              22.43939
                Total gradient norm:      2.34496
                Solved trajectories:      17 / 32
                Avg steps to solve:       3.353
                
Iteration (500/50001) took 0.046 seconds.
                Mean final reward:        6.8750
                Mean return:              -1.0312
                Policy entropy:           1.5600
                Pseudo loss:              24.29469
                Total gradient norm:      2.28012
                Solved trajectories:      12 / 32
                Avg steps to solve:       3.750
                
Iteration (600/50001) took 0.044 seconds.
                Mean final reward:        9.1562
                Mean return:              1.5625
                Policy entropy:           1.5542
                Pseudo loss:              26.16755
                Total gradient norm:      2.28110
                Solved trajectories:      15 / 32
                Avg steps to solve:       4.733
                
Iteration (700/50001) took 0.051 seconds.
                Mean final reward:        6.5312
                Mean return:              -2.4062
                Policy entropy:           1.5433
                Pseudo loss:              27.92088
                Total gradient norm:      2.38585
                Solved trajectories:      10 / 32
                Avg steps to solve:       5.400
                
Iteration (800/50001) took 0.060 seconds.
                Mean final reward:        7.2188
                Mean return:              -1.1562
                Policy entropy:           1.5349
                Pseudo loss:              24.63560
                Total gradient norm:      2.08705
                Solved trajectories:      13 / 32
                Avg steps to solve:       5.538
                
Iteration (900/50001) took 0.043 seconds.
                Mean final reward:        8.8125
                Mean return:              1.5000
                Policy entropy:           1.5298
                Pseudo loss:              20.16099
                Total gradient norm:      1.85092
                Solved trajectories:      13 / 32
                Avg steps to solve:       2.923
                
Iteration (1000/50001) took 0.077 seconds.
                Mean final reward:        9.3750
                Mean return:              1.3125
                Policy entropy:           1.5163
                Pseudo loss:              22.93708
                Total gradient norm:      2.31037
                Solved trajectories:      12 / 32
                Avg steps to solve:       4.167
                
Iteration (1100/50001) took 0.049 seconds.
                Mean final reward:        7.8438
                Mean return:              -0.4688
                Policy entropy:           1.5134
                Pseudo loss:              23.99021
                Total gradient norm:      1.86794
                Solved trajectories:      13 / 32
                Avg steps to solve:       5.385
                
Iteration (1200/50001) took 0.042 seconds.
                Mean final reward:        8.8750
                Mean return:              1.9062
                Policy entropy:           1.5044
                Pseudo loss:              19.62292
                Total gradient norm:      1.63924
                Solved trajectories:      16 / 32
                Avg steps to solve:       3.938
                
Iteration (1300/50001) took 0.049 seconds.
                Mean final reward:        10.3438
                Mean return:              1.9688
                Policy entropy:           1.5030
                Pseudo loss:              25.62033
                Total gradient norm:      2.27277
                Solved trajectories:      13 / 32
                Avg steps to solve:       5.538
                
Iteration (1400/50001) took 0.047 seconds.
                Mean final reward:        10.7500
                Mean return:              2.8125
                Policy entropy:           1.4850
                Pseudo loss:              22.97053
                Total gradient norm:      1.60085
                Solved trajectories:      16 / 32
                Avg steps to solve:       5.875
                
Iteration (1500/50001) took 0.045 seconds.
                Mean final reward:        11.0938
                Mean return:              3.7812
                Policy entropy:           1.4763
                Pseudo loss:              20.97977
                Total gradient norm:      1.79956
                Solved trajectories:      16 / 32
                Avg steps to solve:       4.625
                
Iteration (1600/50001) took 0.046 seconds.
                Mean final reward:        12.9688
                Mean return:              5.4688
                Policy entropy:           1.4671
                Pseudo loss:              22.07030
                Total gradient norm:      1.91085
                Solved trajectories:      15 / 32
                Avg steps to solve:       4.533
                
Iteration (1700/50001) took 0.044 seconds.
                Mean final reward:        9.8438
                Mean return:              2.6250
                Policy entropy:           1.4533
                Pseudo loss:              18.46115
                Total gradient norm:      1.62847
                Solved trajectories:      17 / 32
                Avg steps to solve:       4.882
                
Iteration (1800/50001) took 0.044 seconds.
                Mean final reward:        13.5938
                Mean return:              6.2812
                Policy entropy:           1.4408
                Pseudo loss:              21.05278
                Total gradient norm:      1.61035
                Solved trajectories:      17 / 32
                Avg steps to solve:       5.059
                
Iteration (1900/50001) took 0.047 seconds.
                Mean final reward:        14.9062
                Mean return:              7.1250
                Policy entropy:           1.4273
                Pseudo loss:              25.35211
                Total gradient norm:      1.93099
                Solved trajectories:      17 / 32
                Avg steps to solve:       5.941
                
Iteration (2000/50001) took 0.045 seconds.
                Mean final reward:        11.0312
                Mean return:              3.6875
                Policy entropy:           1.4377
                Pseudo loss:              17.86928
                Total gradient norm:      1.72889
                Solved trajectories:      15 / 32
                Avg steps to solve:       4.200
                
Iteration (2100/50001) took 0.049 seconds.
                Mean final reward:        10.4062
                Mean return:              2.2812
                Policy entropy:           1.4351
                Pseudo loss:              21.12731
                Total gradient norm:      1.52143
                Solved trajectories:      15 / 32
                Avg steps to solve:       5.867
                
Iteration (2200/50001) took 0.053 seconds.
                Mean final reward:        10.6875
                Mean return:              2.0938
                Policy entropy:           1.4310
                Pseudo loss:              21.68501
                Total gradient norm:      1.60237
                Solved trajectories:      13 / 32
                Avg steps to solve:       6.077
                
Iteration (2300/50001) took 0.053 seconds.
                Mean final reward:        10.0625
                Mean return:              2.9688
                Policy entropy:           1.4233
                Pseudo loss:              15.43317
                Total gradient norm:      1.22019
                Solved trajectories:      14 / 32
                Avg steps to solve:       3.071
                
Iteration (2400/50001) took 0.047 seconds.
                Mean final reward:        12.6875
                Mean return:              4.8438
                Policy entropy:           1.4006
                Pseudo loss:              21.95592
                Total gradient norm:      1.87685
                Solved trajectories:      17 / 32
                Avg steps to solve:       6.059
                
Iteration (2500/50001) took 0.055 seconds.
                Mean final reward:        12.6875
                Mean return:              5.1875
                Policy entropy:           1.3934
                Pseudo loss:              18.63521
                Total gradient norm:      1.41643
                Solved trajectories:      18 / 32
                Avg steps to solve:       5.778
                
Iteration (2600/50001) took 0.039 seconds.
                Mean final reward:        17.1875
                Mean return:              10.8125
                Policy entropy:           1.3546
                Pseudo loss:              17.79288
                Total gradient norm:      1.24596
                Solved trajectories:      20 / 32
                Avg steps to solve:       4.600
                
Iteration (2700/50001) took 0.044 seconds.
                Mean final reward:        10.1250
                Mean return:              2.8438
                Policy entropy:           1.3838
                Pseudo loss:              15.06439
                Total gradient norm:      1.09330
                Solved trajectories:      15 / 32
                Avg steps to solve:       4.067
                
Iteration (2800/50001) took 0.044 seconds.
                Mean final reward:        13.2500
                Mean return:              6.0312
                Policy entropy:           1.3704
                Pseudo loss:              16.55787
                Total gradient norm:      1.25809
                Solved trajectories:      15 / 32
                Avg steps to solve:       3.933
                
Iteration (2900/50001) took 0.038 seconds.
                Mean final reward:        17.5312
                Mean return:              11.3125
                Policy entropy:           1.3158
                Pseudo loss:              17.30293
                Total gradient norm:      1.28049
                Solved trajectories:      23 / 32
                Avg steps to solve:       5.348
                
Iteration (3000/50001) took 0.044 seconds.
                Mean final reward:        15.1875
                Mean return:              8.0312
                Policy entropy:           1.3327
                Pseudo loss:              17.92828
                Total gradient norm:      1.19049
                Solved trajectories:      18 / 32
                Avg steps to solve:       5.167
                
Iteration (3100/50001) took 0.045 seconds.
                Mean final reward:        14.9062
                Mean return:              7.6562
                Policy entropy:           1.3381
                Pseudo loss:              17.80197
                Total gradient norm:      1.22336
                Solved trajectories:      18 / 32
                Avg steps to solve:       5.333
                
Iteration (3200/50001) took 0.039 seconds.
                Mean final reward:        15.1875
                Mean return:              9.0000
                Policy entropy:           1.3379
                Pseudo loss:              12.51195
                Total gradient norm:      0.99634
                Solved trajectories:      18 / 32
                Avg steps to solve:       3.444
                
Iteration (3300/50001) took 0.045 seconds.
                Mean final reward:        11.7188
                Mean return:              4.4062
                Policy entropy:           1.3544
                Pseudo loss:              16.29619
                Total gradient norm:      1.31746
                Solved trajectories:      17 / 32
                Avg steps to solve:       5.059
                
Iteration (3400/50001) took 0.045 seconds.
                Mean final reward:        14.3438
                Mean return:              8.4688
                Policy entropy:           1.3017
                Pseudo loss:              13.33499
                Total gradient norm:      1.07304
                Solved trajectories:      21 / 32
                Avg steps to solve:       4.190
                
Iteration (3500/50001) took 0.041 seconds.
                Mean final reward:        17.1875
                Mean return:              10.7812
                Policy entropy:           1.2742
                Pseudo loss:              17.82518
                Total gradient norm:      1.19628
                Solved trajectories:      20 / 32
                Avg steps to solve:       4.650
                
Iteration (3600/50001) took 0.060 seconds.
                Mean final reward:        19.8125
                Mean return:              14.2500
                Policy entropy:           1.2217
                Pseudo loss:              17.51351
                Total gradient norm:      1.21248
                Solved trajectories:      25 / 32
                Avg steps to solve:       5.040
                
Iteration (3700/50001) took 0.042 seconds.
                Mean final reward:        13.0312
                Mean return:              6.3125
                Policy entropy:           1.3064
                Pseudo loss:              14.41204
                Total gradient norm:      1.17252
                Solved trajectories:      18 / 32
                Avg steps to solve:       4.389
                
Iteration (3800/50001) took 0.037 seconds.
                Mean final reward:        17.2500
                Mean return:              11.6875
                Policy entropy:           1.2476
                Pseudo loss:              14.10772
                Total gradient norm:      1.29376
                Solved trajectories:      22 / 32
                Avg steps to solve:       4.091
                
Iteration (3900/50001) took 0.041 seconds.
                Mean final reward:        16.9062
                Mean return:              10.5000
                Policy entropy:           1.2545
                Pseudo loss:              16.50118
                Total gradient norm:      1.02375
                Solved trajectories:      23 / 32
                Avg steps to solve:       5.609
                
Iteration (4000/50001) took 0.044 seconds.
                Mean final reward:        17.1875
                Mean return:              10.5625
                Policy entropy:           1.2611
                Pseudo loss:              15.87533
                Total gradient norm:      0.90688
                Solved trajectories:      21 / 32
                Avg steps to solve:       5.333
                
Iteration (4100/50001) took 0.033 seconds.
                Mean final reward:        23.0000
                Mean return:              18.3438
                Policy entropy:           1.1495
                Pseudo loss:              12.21859
                Total gradient norm:      1.00483
                Solved trajectories:      27 / 32
                Avg steps to solve:       4.481
                
Iteration (4200/50001) took 0.040 seconds.
                Mean final reward:        16.2812
                Mean return:              9.8438
                Policy entropy:           1.2521
                Pseudo loss:              14.46843
                Total gradient norm:      1.00288
                Solved trajectories:      22 / 32
                Avg steps to solve:       5.364
                
Iteration (4300/50001) took 0.041 seconds.
                Mean final reward:        13.6562
                Mean return:              7.1562
                Policy entropy:           1.2728
                Pseudo loss:              12.37635
                Total gradient norm:      0.92807
                Solved trajectories:      18 / 32
                Avg steps to solve:       4.000
                
Iteration (4400/50001) took 0.038 seconds.
                Mean final reward:        17.1875
                Mean return:              11.3750
                Policy entropy:           1.2376
                Pseudo loss:              11.68772
                Total gradient norm:      1.04626
                Solved trajectories:      21 / 32
                Avg steps to solve:       4.095
                
Iteration (4500/50001) took 0.040 seconds.
                Mean final reward:        19.1875
                Mean return:              13.0312
                Policy entropy:           1.1732
                Pseudo loss:              17.82417
                Total gradient norm:      1.31800
                Solved trajectories:      25 / 32
                Avg steps to solve:       5.800
                
Iteration (4600/50001) took 0.036 seconds.
                Mean final reward:        20.1562
                Mean return:              14.8750
                Policy entropy:           1.1448
                Pseudo loss:              13.50319
                Total gradient norm:      1.12014
                Solved trajectories:      26 / 32
                Avg steps to solve:       4.962
                
Iteration (4700/50001) took 0.043 seconds.
                Mean final reward:        17.5312
                Mean return:              11.2812
                Policy entropy:           1.2179
                Pseudo loss:              14.26278
                Total gradient norm:      1.02411
                Solved trajectories:      23 / 32
                Avg steps to solve:       5.391
                
Iteration (4800/50001) took 0.035 seconds.
                Mean final reward:        20.7812
                Mean return:              15.8125
                Policy entropy:           1.1306
                Pseudo loss:              13.31576
                Total gradient norm:      0.99105
                Solved trajectories:      27 / 32
                Avg steps to solve:       4.852
                
Iteration (4900/50001) took 0.039 seconds.
                Mean final reward:        19.4062
                Mean return:              13.6562
                Policy entropy:           1.1814
                Pseudo loss:              12.30648
                Total gradient norm:      1.07069
                Solved trajectories:      23 / 32
                Avg steps to solve:       4.696
                
Iteration (5000/50001) took 0.038 seconds.
                Mean final reward:        23.3438
                Mean return:              17.5312
                Policy entropy:           1.0948
                Pseudo loss:              15.81127
                Total gradient norm:      1.09269
                Solved trajectories:      28 / 32
                Avg steps to solve:       6.071
                
Iteration (5100/50001) took 0.035 seconds.
                Mean final reward:        18.1562
                Mean return:              12.9375
                Policy entropy:           1.1600
                Pseudo loss:              10.52551
                Total gradient norm:      0.82639
                Solved trajectories:      21 / 32
                Avg steps to solve:       3.190
                
Iteration (5200/50001) took 0.036 seconds.
                Mean final reward:        19.4688
                Mean return:              14.1250
                Policy entropy:           1.1389
                Pseudo loss:              11.66615
                Total gradient norm:      0.75699
                Solved trajectories:      25 / 32
                Avg steps to solve:       4.760
                
Iteration (5300/50001) took 0.039 seconds.
                Mean final reward:        21.0625
                Mean return:              15.9688
                Policy entropy:           1.1072
                Pseudo loss:              11.65958
                Total gradient norm:      0.99722
                Solved trajectories:      26 / 32
                Avg steps to solve:       4.731
                
Iteration (5400/50001) took 0.039 seconds.
                Mean final reward:        15.9375
                Mean return:              9.9375
                Policy entropy:           1.1769
                Pseudo loss:              11.46657
                Total gradient norm:      0.85832
                Solved trajectories:      21 / 32
                Avg steps to solve:       4.381
                
Iteration (5500/50001) took 0.037 seconds.
                Mean final reward:        19.1875
                Mean return:              13.5000
                Policy entropy:           1.1228
                Pseudo loss:              13.52558
                Total gradient norm:      1.12577
                Solved trajectories:      25 / 32
                Avg steps to solve:       5.200
                
Iteration (5600/50001) took 0.034 seconds.
                Mean final reward:        18.2188
                Mean return:              13.4062
                Policy entropy:           1.1616
                Pseudo loss:              8.22948
                Total gradient norm:      0.61796
                Solved trajectories:      24 / 32
                Avg steps to solve:       3.750
                
Iteration (5700/50001) took 0.034 seconds.
                Mean final reward:        21.4062
                Mean return:              16.4688
                Policy entropy:           1.0664
                Pseudo loss:              11.25345
                Total gradient norm:      0.96067
                Solved trajectories:      25 / 32
                Avg steps to solve:       4.240
                
Iteration (5800/50001) took 0.036 seconds.
                Mean final reward:        20.7188
                Mean return:              15.4062
                Policy entropy:           1.1250
                Pseudo loss:              10.88560
                Total gradient norm:      0.76723
                Solved trajectories:      25 / 32
                Avg steps to solve:       4.720
                
Iteration (5900/50001) took 0.034 seconds.
                Mean final reward:        21.0625
                Mean return:              16.1875
                Policy entropy:           1.0688
                Pseudo loss:              9.86380
                Total gradient norm:      0.89755
                Solved trajectories:      25 / 32
                Avg steps to solve:       4.160
                
Iteration (6000/50001) took 0.032 seconds.
                Mean final reward:        20.1562
                Mean return:              15.7500
                Policy entropy:           1.0298
                Pseudo loss:              8.62594
                Total gradient norm:      0.96620
                Solved trajectories:      27 / 32
                Avg steps to solve:       4.185
                
Iteration (6100/50001) took 0.040 seconds.
                Mean final reward:        18.1562
                Mean return:              12.0938
                Policy entropy:           1.1449
                Pseudo loss:              11.09463
                Total gradient norm:      0.84123
                Solved trajectories:      23 / 32
                Avg steps to solve:       5.130
                
Iteration (6200/50001) took 0.032 seconds.
                Mean final reward:        20.8438
                Mean return:              16.3125
                Policy entropy:           0.9999
                Pseudo loss:              10.39768
                Total gradient norm:      0.88463
                Solved trajectories:      28 / 32
                Avg steps to solve:       4.607
                
Iteration (6300/50001) took 0.034 seconds.
                Mean final reward:        23.3438
                Mean return:              18.5938
                Policy entropy:           1.0239
                Pseudo loss:              12.17604
                Total gradient norm:      1.17832
                Solved trajectories:      28 / 32
                Avg steps to solve:       4.857
                
Iteration (6400/50001) took 0.035 seconds.
                Mean final reward:        18.2188
                Mean return:              13.0625
                Policy entropy:           1.0915
                Pseudo loss:              10.24250
                Total gradient norm:      0.77486
                Solved trajectories:      24 / 32
                Avg steps to solve:       4.208
                
Iteration (6500/50001) took 0.034 seconds.
                Mean final reward:        24.2500
                Mean return:              19.2812
                Policy entropy:           0.9974
                Pseudo loss:              11.42041
                Total gradient norm:      0.65090
                Solved trajectories:      27 / 32
                Avg steps to solve:       4.852
                
Iteration (6600/50001) took 0.032 seconds.
                Mean final reward:        19.1875
                Mean return:              14.5312
                Policy entropy:           1.0451
                Pseudo loss:              8.08496
                Total gradient norm:      0.83810
                Solved trajectories:      26 / 32
                Avg steps to solve:       4.192
                
Iteration (6700/50001) took 0.035 seconds.
                Mean final reward:        23.2812
                Mean return:              18.3750
                Policy entropy:           1.0501
                Pseudo loss:              11.68997
                Total gradient norm:      1.11234
                Solved trajectories:      26 / 32
                Avg steps to solve:       4.500
                
Iteration (6800/50001) took 0.034 seconds.
                Mean final reward:        21.7500
                Mean return:              16.8750
                Policy entropy:           0.9993
                Pseudo loss:              11.88361
                Total gradient norm:      0.81546
                Solved trajectories:      27 / 32
                Avg steps to solve:       4.741
                
Iteration (6900/50001) took 0.031 seconds.
                Mean final reward:        23.0000
                Mean return:              18.7500
                Policy entropy:           1.0039
                Pseudo loss:              9.10316
                Total gradient norm:      0.83237
                Solved trajectories:      27 / 32
                Avg steps to solve:       4.000
                
Iteration (7000/50001) took 0.028 seconds.
                Mean final reward:        22.0938
                Mean return:              18.6250
                Policy entropy:           0.9233
                Pseudo loss:              6.87384
                Total gradient norm:      0.72609
                Solved trajectories:      29 / 32
                Avg steps to solve:       3.690
                
Iteration (7100/50001) took 0.028 seconds.
                Mean final reward:        21.4688
                Mean return:              17.7188
                Policy entropy:           0.9636
                Pseudo loss:              6.90336
                Total gradient norm:      0.57783
                Solved trajectories:      29 / 32
                Avg steps to solve:       4.000
                
Iteration (7200/50001) took 0.035 seconds.
                Mean final reward:        22.4375
                Mean return:              18.0000
                Policy entropy:           0.9753
                Pseudo loss:              10.67523
                Total gradient norm:      0.84408
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.000
                
Iteration (7300/50001) took 0.033 seconds.
                Mean final reward:        22.4375
                Mean return:              17.6875
                Policy entropy:           0.9545
                Pseudo loss:              12.32224
                Total gradient norm:      0.91688
                Solved trajectories:      29 / 32
                Avg steps to solve:       5.103
                
Iteration (7400/50001) took 0.034 seconds.
                Mean final reward:        19.1875
                Mean return:              14.3750
                Policy entropy:           1.0337
                Pseudo loss:              7.65984
                Total gradient norm:      0.60888
                Solved trajectories:      26 / 32
                Avg steps to solve:       4.385
                
Iteration (7500/50001) took 0.038 seconds.
                Mean final reward:        20.7812
                Mean return:              15.5625
                Policy entropy:           1.0423
                Pseudo loss:              8.90629
                Total gradient norm:      0.81633
                Solved trajectories:      27 / 32
                Avg steps to solve:       5.148
                
Iteration (7600/50001) took 0.029 seconds.
                Mean final reward:        20.8438
                Mean return:              16.7812
                Policy entropy:           0.9435
                Pseudo loss:              7.67742
                Total gradient norm:      0.75082
                Solved trajectories:      29 / 32
                Avg steps to solve:       4.345
                
Iteration (7700/50001) took 0.031 seconds.
                Mean final reward:        21.1875
                Mean return:              16.7500
                Policy entropy:           0.9461
                Pseudo loss:              8.03082
                Total gradient norm:      0.67762
                Solved trajectories:      29 / 32
                Avg steps to solve:       4.759
                
Iteration (7800/50001) took 0.037 seconds.
                Mean final reward:        21.0625
                Mean return:              15.3750
                Policy entropy:           1.0032
                Pseudo loss:              10.25819
                Total gradient norm:      0.67229
                Solved trajectories:      24 / 32
                Avg steps to solve:       4.917
                
Iteration (7900/50001) took 0.029 seconds.
                Mean final reward:        22.1562
                Mean return:              18.2812
                Policy entropy:           0.8938
                Pseudo loss:              8.69634
                Total gradient norm:      0.71266
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.400
                
Iteration (8000/50001) took 0.037 seconds.
                Mean final reward:        21.1250
                Mean return:              16.4688
                Policy entropy:           0.9639
                Pseudo loss:              9.02803
                Total gradient norm:      0.71312
                Solved trajectories:      26 / 32
                Avg steps to solve:       4.192
                
Iteration (8100/50001) took 0.036 seconds.
                Mean final reward:        23.3438
                Mean return:              18.3125
                Policy entropy:           0.9459
                Pseudo loss:              12.01748
                Total gradient norm:      0.79941
                Solved trajectories:      26 / 32
                Avg steps to solve:       4.654
                
Iteration (8200/50001) took 0.038 seconds.
                Mean final reward:        23.5625
                Mean return:              18.9375
                Policy entropy:           0.9114
                Pseudo loss:              7.77763
                Total gradient norm:      0.68450
                Solved trajectories:      25 / 32
                Avg steps to solve:       3.840
                
Iteration (8300/50001) took 0.049 seconds.
                Mean final reward:        19.8125
                Mean return:              14.6875
                Policy entropy:           0.9238
                Pseudo loss:              8.99921
                Total gradient norm:      0.68137
                Solved trajectories:      26 / 32
                Avg steps to solve:       4.769
                
Iteration (8400/50001) took 0.049 seconds.
                Mean final reward:        20.4375
                Mean return:              13.8750
                Policy entropy:           0.9807
                Pseudo loss:              13.18514
                Total gradient norm:      0.99608
                Solved trajectories:      25 / 32
                Avg steps to solve:       6.320
                
Iteration (8500/50001) took 0.050 seconds.
                Mean final reward:        23.6875
                Mean return:              19.7812
                Policy entropy:           0.8168
                Pseudo loss:              8.04283
                Total gradient norm:      0.50685
                Solved trajectories:      29 / 32
                Avg steps to solve:       4.172
                
Iteration (8600/50001) took 0.029 seconds.
                Mean final reward:        21.8125
                Mean return:              17.8125
                Policy entropy:           0.8823
                Pseudo loss:              6.46338
                Total gradient norm:      0.65736
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.533
                
Iteration (8700/50001) took 0.035 seconds.
                Mean final reward:        20.2812
                Mean return:              15.2500
                Policy entropy:           0.9224
                Pseudo loss:              11.65084
                Total gradient norm:      0.71349
                Solved trajectories:      29 / 32
                Avg steps to solve:       5.414
                
Iteration (8800/50001) took 0.034 seconds.
                Mean final reward:        21.7500
                Mean return:              16.9062
                Policy entropy:           0.9202
                Pseudo loss:              9.02200
                Total gradient norm:      0.75415
                Solved trajectories:      28 / 32
                Avg steps to solve:       4.964
                
Iteration (8900/50001) took 0.035 seconds.
                Mean final reward:        20.7188
                Mean return:              15.9062
                Policy entropy:           0.9239
                Pseudo loss:              5.48855
                Total gradient norm:      0.61160
                Solved trajectories:      24 / 32
                Avg steps to solve:       3.750
                
Iteration (9000/50001) took 0.034 seconds.
                Mean final reward:        22.4375
                Mean return:              17.8750
                Policy entropy:           0.8755
                Pseudo loss:              6.94502
                Total gradient norm:      0.55697
                Solved trajectories:      28 / 32
                Avg steps to solve:       4.643
                
Iteration (9100/50001) took 0.030 seconds.
                Mean final reward:        21.4688
                Mean return:              17.2500
                Policy entropy:           0.8393
                Pseudo loss:              6.41130
                Total gradient norm:      0.55810
                Solved trajectories:      29 / 32
                Avg steps to solve:       4.517
                
Iteration (9200/50001) took 0.029 seconds.
                Mean final reward:        19.9375
                Mean return:              16.0625
                Policy entropy:           0.8108
                Pseudo loss:              6.67685
                Total gradient norm:      0.52409
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.400
                
Iteration (9300/50001) took 0.030 seconds.
                Mean final reward:        24.9375
                Mean return:              20.7500
                Policy entropy:           0.7674
                Pseudo loss:              8.16111
                Total gradient norm:      0.71546
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.733
                
Iteration (9400/50001) took 0.031 seconds.
                Mean final reward:        26.8125
                Mean return:              22.5000
                Policy entropy:           0.8065
                Pseudo loss:              7.70182
                Total gradient norm:      0.54551
                Solved trajectories:      29 / 32
                Avg steps to solve:       4.621
                
Iteration (9500/50001) took 0.035 seconds.
                Mean final reward:        23.0625
                Mean return:              18.9688
                Policy entropy:           0.8105
                Pseudo loss:              7.56421
                Total gradient norm:      0.64405
                Solved trajectories:      29 / 32
                Avg steps to solve:       4.379
                
Iteration (9600/50001) took 0.031 seconds.
                Mean final reward:        25.0000
                Mean return:              20.6875
                Policy entropy:           0.8016
                Pseudo loss:              10.06982
                Total gradient norm:      0.65592
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.867
                
Iteration (9700/50001) took 0.031 seconds.
                Mean final reward:        22.1562
                Mean return:              17.8125
                Policy entropy:           0.8472
                Pseudo loss:              8.99641
                Total gradient norm:      0.79068
                Solved trajectories:      29 / 32
                Avg steps to solve:       4.655
                
Iteration (9800/50001) took 0.058 seconds.
                Mean final reward:        22.0312
                Mean return:              16.8125
                Policy entropy:           0.8888
                Pseudo loss:              8.38928
                Total gradient norm:      0.54142
                Solved trajectories:      27 / 32
                Avg steps to solve:       5.148
                
Iteration (9900/50001) took 0.033 seconds.
                Mean final reward:        18.9688
                Mean return:              14.2500
                Policy entropy:           0.8685
                Pseudo loss:              7.67044
                Total gradient norm:      0.70260
                Solved trajectories:      29 / 32
                Avg steps to solve:       5.069
                
Iteration (10000/50001) took 0.041 seconds.
                Mean final reward:        20.5000
                Mean return:              15.1562
                Policy entropy:           0.8423
                Pseudo loss:              9.41240
                Total gradient norm:      0.84426
                Solved trajectories:      28 / 32
                Avg steps to solve:       5.536
                
Iteration (10100/50001) took 0.032 seconds.
                Mean final reward:        27.5000
                Mean return:              23.6562
                Policy entropy:           0.7182
                Pseudo loss:              9.10430
                Total gradient norm:      0.59733
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.844
                
Iteration (10200/50001) took 0.038 seconds.
                Mean final reward:        18.3438
                Mean return:              13.6562
                Policy entropy:           0.8139
                Pseudo loss:              7.47561
                Total gradient norm:      0.56338
                Solved trajectories:      29 / 32
                Avg steps to solve:       5.034
                
Iteration (10300/50001) took 0.031 seconds.
                Mean final reward:        24.0312
                Mean return:              19.8750
                Policy entropy:           0.7871
                Pseudo loss:              7.16941
                Total gradient norm:      0.52872
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.700
                
Iteration (10400/50001) took 0.040 seconds.
                Mean final reward:        25.0000
                Mean return:              21.6562
                Policy entropy:           0.6987
                Pseudo loss:              6.09445
                Total gradient norm:      0.40182
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.344
                
Iteration (10500/50001) took 0.035 seconds.
                Mean final reward:        20.8438
                Mean return:              15.7500
                Policy entropy:           0.8260
                Pseudo loss:              8.57456
                Total gradient norm:      0.71224
                Solved trajectories:      28 / 32
                Avg steps to solve:       5.250
                
Iteration (10600/50001) took 0.036 seconds.
                Mean final reward:        26.2500
                Mean return:              22.6250
                Policy entropy:           0.7365
                Pseudo loss:              6.89355
                Total gradient norm:      0.44206
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.625
                
Iteration (10700/50001) took 0.058 seconds.
                Mean final reward:        26.5312
                Mean return:              21.5625
                Policy entropy:           0.7306
                Pseudo loss:              10.52350
                Total gradient norm:      0.75639
                Solved trajectories:      29 / 32
                Avg steps to solve:       5.345
                
Iteration (10800/50001) took 0.032 seconds.
                Mean final reward:        22.4375
                Mean return:              17.5938
                Policy entropy:           0.8319
                Pseudo loss:              7.91988
                Total gradient norm:      0.83675
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.433
                
Iteration (10900/50001) took 0.030 seconds.
                Mean final reward:        22.4375
                Mean return:              18.2188
                Policy entropy:           0.7930
                Pseudo loss:              7.26009
                Total gradient norm:      0.73498
                Solved trajectories:      28 / 32
                Avg steps to solve:       4.250
                
Iteration (11000/50001) took 0.025 seconds.
                Mean final reward:        24.3750
                Mean return:              21.2188
                Policy entropy:           0.7116
                Pseudo loss:              5.89248
                Total gradient norm:      0.44894
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.156
                
Iteration (11100/50001) took 0.025 seconds.
                Mean final reward:        24.3750
                Mean return:              21.3750
                Policy entropy:           0.6361
                Pseudo loss:              5.72782
                Total gradient norm:      0.47573
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.000
                
Iteration (11200/50001) took 0.026 seconds.
                Mean final reward:        25.6250
                Mean return:              22.0938
                Policy entropy:           0.6707
                Pseudo loss:              6.20425
                Total gradient norm:      0.67701
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.531
                
Iteration (11300/50001) took 0.032 seconds.
                Mean final reward:        22.7812
                Mean return:              18.0938
                Policy entropy:           0.7216
                Pseudo loss:              7.48354
                Total gradient norm:      0.54085
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.484
                
Iteration (11400/50001) took 0.028 seconds.
                Mean final reward:        25.0000
                Mean return:              21.1562
                Policy entropy:           0.7018
                Pseudo loss:              7.29039
                Total gradient norm:      0.40618
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.367
                
Iteration (11500/50001) took 0.034 seconds.
                Mean final reward:        22.1562
                Mean return:              17.9375
                Policy entropy:           0.7142
                Pseudo loss:              6.90785
                Total gradient norm:      0.50331
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.767
                
Iteration (11600/50001) took 0.028 seconds.
                Mean final reward:        22.4375
                Mean return:              18.5938
                Policy entropy:           0.6984
                Pseudo loss:              5.51086
                Total gradient norm:      0.46237
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.367
                
Iteration (11700/50001) took 0.036 seconds.
                Mean final reward:        25.2812
                Mean return:              19.9062
                Policy entropy:           0.6758
                Pseudo loss:              9.37878
                Total gradient norm:      0.70559
                Solved trajectories:      29 / 32
                Avg steps to solve:       5.793
                
Iteration (11800/50001) took 0.032 seconds.
                Mean final reward:        25.9062
                Mean return:              21.3750
                Policy entropy:           0.7148
                Pseudo loss:              8.84921
                Total gradient norm:      0.71592
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.100
                
Iteration (11900/50001) took 0.029 seconds.
                Mean final reward:        22.7812
                Mean return:              18.5312
                Policy entropy:           0.6945
                Pseudo loss:              6.25637
                Total gradient norm:      0.49814
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.032
                
Iteration (12000/50001) took 0.031 seconds.
                Mean final reward:        27.1562
                Mean return:              22.6875
                Policy entropy:           0.6716
                Pseudo loss:              7.75467
                Total gradient norm:      0.46524
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.258
                
Iteration (12100/50001) took 0.030 seconds.
                Mean final reward:        26.5312
                Mean return:              22.3125
                Policy entropy:           0.6584
                Pseudo loss:              7.87782
                Total gradient norm:      0.61158
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.000
                
Iteration (12200/50001) took 0.030 seconds.
                Mean final reward:        25.2812
                Mean return:              21.0000
                Policy entropy:           0.6794
                Pseudo loss:              7.19444
                Total gradient norm:      0.59797
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.833
                
Iteration (12300/50001) took 0.033 seconds.
                Mean final reward:        27.5000
                Mean return:              22.6875
                Policy entropy:           0.6517
                Pseudo loss:              9.98382
                Total gradient norm:      0.57355
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.613
                
Iteration (12400/50001) took 0.029 seconds.
                Mean final reward:        22.7812
                Mean return:              18.5625
                Policy entropy:           0.6500
                Pseudo loss:              5.96393
                Total gradient norm:      0.48449
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.000
                
Iteration (12500/50001) took 0.029 seconds.
                Mean final reward:        23.6875
                Mean return:              19.5312
                Policy entropy:           0.6327
                Pseudo loss:              5.99534
                Total gradient norm:      0.52483
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.700
                
Iteration (12600/50001) took 0.028 seconds.
                Mean final reward:        25.2812
                Mean return:              21.2812
                Policy entropy:           0.6635
                Pseudo loss:              6.23312
                Total gradient norm:      0.53614
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.774
                
Iteration (12700/50001) took 0.030 seconds.
                Mean final reward:        22.7812
                Mean return:              18.5625
                Policy entropy:           0.6854
                Pseudo loss:              5.99797
                Total gradient norm:      0.41503
                Solved trajectories:      29 / 32
                Avg steps to solve:       4.517
                
Iteration (12800/50001) took 0.027 seconds.
                Mean final reward:        21.2500
                Mean return:              17.5938
                Policy entropy:           0.7037
                Pseudo loss:              5.98828
                Total gradient norm:      0.41600
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.656
                
Iteration (12900/50001) took 0.034 seconds.
                Mean final reward:        26.5312
                Mean return:              21.5312
                Policy entropy:           0.6800
                Pseudo loss:              8.99291
                Total gradient norm:      0.56651
                Solved trajectories:      29 / 32
                Avg steps to solve:       5.379
                
Iteration (13000/50001) took 0.029 seconds.
                Mean final reward:        21.1875
                Mean return:              16.9688
                Policy entropy:           0.6707
                Pseudo loss:              6.54453
                Total gradient norm:      0.53247
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.767
                
Iteration (13100/50001) took 0.026 seconds.
                Mean final reward:        23.1250
                Mean return:              19.5938
                Policy entropy:           0.5897
                Pseudo loss:              5.38222
                Total gradient norm:      0.42188
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.531
                
Iteration (13200/50001) took 0.049 seconds.
                Mean final reward:        25.6250
                Mean return:              21.2500
                Policy entropy:           0.6051
                Pseudo loss:              7.47248
                Total gradient norm:      0.49567
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.161
                
Iteration (13300/50001) took 0.030 seconds.
                Mean final reward:        27.1562
                Mean return:              22.7500
                Policy entropy:           0.5860
                Pseudo loss:              7.16160
                Total gradient norm:      0.77350
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.194
                
Iteration (13400/50001) took 0.026 seconds.
                Mean final reward:        24.6562
                Mean return:              21.0312
                Policy entropy:           0.6213
                Pseudo loss:              4.79060
                Total gradient norm:      0.50337
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.387
                
Iteration (13500/50001) took 0.027 seconds.
                Mean final reward:        25.9062
                Mean return:              22.2500
                Policy entropy:           0.6021
                Pseudo loss:              4.56799
                Total gradient norm:      0.44968
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.419
                
Iteration (13600/50001) took 0.032 seconds.
                Mean final reward:        25.9062
                Mean return:              21.0938
                Policy entropy:           0.6082
                Pseudo loss:              6.53073
                Total gradient norm:      0.42167
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.613
                
Iteration (13700/50001) took 0.029 seconds.
                Mean final reward:        22.5000
                Mean return:              18.3438
                Policy entropy:           0.6677
                Pseudo loss:              6.39524
                Total gradient norm:      0.54426
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.700
                
Iteration (13800/50001) took 0.029 seconds.
                Mean final reward:        26.2500
                Mean return:              22.1250
                Policy entropy:           0.5669
                Pseudo loss:              6.34527
                Total gradient norm:      0.39054
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.903
                
Iteration (13900/50001) took 0.027 seconds.
                Mean final reward:        24.6562
                Mean return:              20.8750
                Policy entropy:           0.6149
                Pseudo loss:              4.43113
                Total gradient norm:      0.48576
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.548
                
Iteration (14000/50001) took 0.025 seconds.
                Mean final reward:        26.8750
                Mean return:              23.5938
                Policy entropy:           0.6022
                Pseudo loss:              4.94371
                Total gradient norm:      0.41524
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.281
                
Iteration (14100/50001) took 0.029 seconds.
                Mean final reward:        22.1562
                Mean return:              18.0625
                Policy entropy:           0.6295
                Pseudo loss:              5.76433
                Total gradient norm:      0.44587
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.871
                
Iteration (14200/50001) took 0.026 seconds.
                Mean final reward:        26.2500
                Mean return:              22.9062
                Policy entropy:           0.5836
                Pseudo loss:              4.60887
                Total gradient norm:      0.48466
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.344
                
Iteration (14300/50001) took 0.027 seconds.
                Mean final reward:        24.0312
                Mean return:              20.2812
                Policy entropy:           0.5788
                Pseudo loss:              5.60119
                Total gradient norm:      0.58077
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.516
                
Iteration (14400/50001) took 0.030 seconds.
                Mean final reward:        26.2500
                Mean return:              21.8750
                Policy entropy:           0.6399
                Pseudo loss:              7.36194
                Total gradient norm:      0.51783
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.933
                
Iteration (14500/50001) took 0.026 seconds.
                Mean final reward:        25.6250
                Mean return:              22.2500
                Policy entropy:           0.5857
                Pseudo loss:              5.02385
                Total gradient norm:      0.45424
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.375
                
Iteration (14600/50001) took 0.026 seconds.
                Mean final reward:        25.0000
                Mean return:              21.8125
                Policy entropy:           0.5272
                Pseudo loss:              3.85997
                Total gradient norm:      0.34029
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.188
                
Iteration (14700/50001) took 0.030 seconds.
                Mean final reward:        26.2500
                Mean return:              22.2812
                Policy entropy:           0.5411
                Pseudo loss:              6.97182
                Total gradient norm:      0.62021
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.500
                
Iteration (14800/50001) took 0.027 seconds.
                Mean final reward:        26.2500
                Mean return:              22.6562
                Policy entropy:           0.5264
                Pseudo loss:              5.25827
                Total gradient norm:      0.39969
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.355
                
Iteration (14900/50001) took 0.027 seconds.
                Mean final reward:        27.5000
                Mean return:              24.2500
                Policy entropy:           0.5626
                Pseudo loss:              5.80573
                Total gradient norm:      0.50190
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.250
                
Iteration (15000/50001) took 0.028 seconds.
                Mean final reward:        25.9062
                Mean return:              22.3125
                Policy entropy:           0.5221
                Pseudo loss:              4.62529
                Total gradient norm:      0.39422
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.100
                
Iteration (15100/50001) took 0.021 seconds.
                Mean final reward:        26.8750
                Mean return:              24.1562
                Policy entropy:           0.5315
                Pseudo loss:              3.25555
                Total gradient norm:      0.33846
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.719
                
Iteration (15200/50001) took 0.033 seconds.
                Mean final reward:        24.6562
                Mean return:              20.0000
                Policy entropy:           0.6025
                Pseudo loss:              5.38061
                Total gradient norm:      0.38642
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.233
                
Iteration (15300/50001) took 0.026 seconds.
                Mean final reward:        23.7500
                Mean return:              20.1250
                Policy entropy:           0.5876
                Pseudo loss:              5.89635
                Total gradient norm:      0.44066
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.625
                
Iteration (15400/50001) took 0.027 seconds.
                Mean final reward:        26.2500
                Mean return:              22.9688
                Policy entropy:           0.5200
                Pseudo loss:              4.42162
                Total gradient norm:      0.38608
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.281
                
Iteration (15500/50001) took 0.032 seconds.
                Mean final reward:        26.2500
                Mean return:              22.3125
                Policy entropy:           0.5364
                Pseudo loss:              5.92691
                Total gradient norm:      0.39430
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.938
                
Iteration (15600/50001) took 0.031 seconds.
                Mean final reward:        26.8750
                Mean return:              23.3438
                Policy entropy:           0.5310
                Pseudo loss:              5.01468
                Total gradient norm:      0.38338
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.290
                
Iteration (15700/50001) took 0.033 seconds.
                Mean final reward:        24.9375
                Mean return:              20.3438
                Policy entropy:           0.5136
                Pseudo loss:              4.55570
                Total gradient norm:      0.41115
                Solved trajectories:      29 / 32
                Avg steps to solve:       4.931
                
Iteration (15800/50001) took 0.030 seconds.
                Mean final reward:        26.5312
                Mean return:              22.3438
                Policy entropy:           0.5611
                Pseudo loss:              5.39735
                Total gradient norm:      0.40308
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.968
                
Iteration (15900/50001) took 0.030 seconds.
                Mean final reward:        23.6875
                Mean return:              19.7188
                Policy entropy:           0.5864
                Pseudo loss:              4.61317
                Total gradient norm:      0.38709
                Solved trajectories:      29 / 32
                Avg steps to solve:       4.241
                
Iteration (16000/50001) took 0.028 seconds.
                Mean final reward:        24.6562
                Mean return:              20.9688
                Policy entropy:           0.5749
                Pseudo loss:              4.38645
                Total gradient norm:      0.44726
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.452
                
Iteration (16100/50001) took 0.024 seconds.
                Mean final reward:        25.6250
                Mean return:              22.5000
                Policy entropy:           0.5988
                Pseudo loss:              5.30312
                Total gradient norm:      0.36559
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.125
                
Iteration (16200/50001) took 0.027 seconds.
                Mean final reward:        24.3750
                Mean return:              21.0938
                Policy entropy:           0.5547
                Pseudo loss:              4.52053
                Total gradient norm:      0.49796
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.281
                
Iteration (16300/50001) took 0.025 seconds.
                Mean final reward:        28.1250
                Mean return:              24.9062
                Policy entropy:           0.5432
                Pseudo loss:              5.76080
                Total gradient norm:      0.44709
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.219
                
Iteration (16400/50001) took 0.029 seconds.
                Mean final reward:        22.7812
                Mean return:              18.6875
                Policy entropy:           0.5461
                Pseudo loss:              4.43665
                Total gradient norm:      0.33676
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.871
                
Iteration (16500/50001) took 0.023 seconds.
                Mean final reward:        26.2500
                Mean return:              23.3125
                Policy entropy:           0.4868
                Pseudo loss:              3.44150
                Total gradient norm:      0.33431
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.938
                
Iteration (16600/50001) took 0.024 seconds.
                Mean final reward:        22.7812
                Mean return:              19.7812
                Policy entropy:           0.5647
                Pseudo loss:              3.10748
                Total gradient norm:      0.38539
                Solved trajectories:      31 / 32
                Avg steps to solve:       3.742
                
Iteration (16700/50001) took 0.030 seconds.
                Mean final reward:        26.8750
                Mean return:              22.9375
                Policy entropy:           0.5761
                Pseudo loss:              6.52406
                Total gradient norm:      0.53271
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.938
                
Iteration (16800/50001) took 0.029 seconds.
                Mean final reward:        23.6875
                Mean return:              19.6875
                Policy entropy:           0.5416
                Pseudo loss:              3.49777
                Total gradient norm:      0.39184
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.533
                
Iteration (16900/50001) took 0.026 seconds.
                Mean final reward:        25.9062
                Mean return:              22.6562
                Policy entropy:           0.4808
                Pseudo loss:              3.62711
                Total gradient norm:      0.29980
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.000
                
Iteration (17000/50001) took 0.027 seconds.
                Mean final reward:        23.7500
                Mean return:              20.1875
                Policy entropy:           0.5177
                Pseudo loss:              4.47491
                Total gradient norm:      0.36791
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.562
                
Iteration (17100/50001) took 0.025 seconds.
                Mean final reward:        25.6250
                Mean return:              22.3750
                Policy entropy:           0.4698
                Pseudo loss:              4.14030
                Total gradient norm:      0.36831
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.250
                
Iteration (17200/50001) took 0.029 seconds.
                Mean final reward:        23.6875
                Mean return:              19.7812
                Policy entropy:           0.4967
                Pseudo loss:              3.87843
                Total gradient norm:      0.50654
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.433
                
Iteration (17300/50001) took 0.029 seconds.
                Mean final reward:        28.1250
                Mean return:              24.1250
                Policy entropy:           0.4416
                Pseudo loss:              4.35247
                Total gradient norm:      0.37105
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.000
                
Iteration (17400/50001) took 0.029 seconds.
                Mean final reward:        27.7812
                Mean return:              23.7812
                Policy entropy:           0.4841
                Pseudo loss:              4.82313
                Total gradient norm:      0.46649
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.774
                
Iteration (17500/50001) took 0.029 seconds.
                Mean final reward:        24.3750
                Mean return:              20.3438
                Policy entropy:           0.4940
                Pseudo loss:              3.95364
                Total gradient norm:      0.38259
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.031
                
Iteration (17600/50001) took 0.029 seconds.
                Mean final reward:        24.0312
                Mean return:              19.9062
                Policy entropy:           0.5285
                Pseudo loss:              5.18254
                Total gradient norm:      0.51242
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.667
                
Iteration (17700/50001) took 0.028 seconds.
                Mean final reward:        26.2500
                Mean return:              22.2812
                Policy entropy:           0.5151
                Pseudo loss:              6.56845
                Total gradient norm:      0.42460
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.742
                
Iteration (17800/50001) took 0.028 seconds.
                Mean final reward:        25.2812
                Mean return:              21.5312
                Policy entropy:           0.4827
                Pseudo loss:              2.67852
                Total gradient norm:      0.32054
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.516
                
Iteration (17900/50001) took 0.028 seconds.
                Mean final reward:        24.3750
                Mean return:              20.5312
                Policy entropy:           0.5794
                Pseudo loss:              5.22950
                Total gradient norm:      0.36696
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.844
                
Iteration (18000/50001) took 0.031 seconds.
                Mean final reward:        29.3750
                Mean return:              25.0938
                Policy entropy:           0.4576
                Pseudo loss:              5.38824
                Total gradient norm:      0.43885
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.281
                
Iteration (18100/50001) took 0.027 seconds.
                Mean final reward:        25.0000
                Mean return:              21.3125
                Policy entropy:           0.5049
                Pseudo loss:              5.02539
                Total gradient norm:      0.36803
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.688
                
Iteration (18200/50001) took 0.026 seconds.
                Mean final reward:        28.1250
                Mean return:              24.5938
                Policy entropy:           0.4800
                Pseudo loss:              4.54240
                Total gradient norm:      0.46656
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.531
                
Iteration (18300/50001) took 0.024 seconds.
                Mean final reward:        25.0000
                Mean return:              21.8750
                Policy entropy:           0.4192
                Pseudo loss:              3.10804
                Total gradient norm:      0.27667
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.125
                
Iteration (18400/50001) took 0.027 seconds.
                Mean final reward:        28.1250
                Mean return:              24.5000
                Policy entropy:           0.4183
                Pseudo loss:              3.55340
                Total gradient norm:      0.34983
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.625
                
Iteration (18500/50001) took 0.027 seconds.
                Mean final reward:        24.3750
                Mean return:              20.6875
                Policy entropy:           0.4773
                Pseudo loss:              5.04704
                Total gradient norm:      0.39306
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.688
                
Iteration (18600/50001) took 0.027 seconds.
                Mean final reward:        26.2500
                Mean return:              22.8438
                Policy entropy:           0.4069
                Pseudo loss:              4.24015
                Total gradient norm:      0.34084
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.161
                
Iteration (18700/50001) took 0.023 seconds.
                Mean final reward:        25.6250
                Mean return:              22.6250
                Policy entropy:           0.5013
                Pseudo loss:              3.67647
                Total gradient norm:      0.45967
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.000
                
Iteration (18800/50001) took 0.023 seconds.
                Mean final reward:        26.2500
                Mean return:              23.4375
                Policy entropy:           0.4655
                Pseudo loss:              3.87354
                Total gradient norm:      0.25426
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.812
                
Iteration (18900/50001) took 0.028 seconds.
                Mean final reward:        26.8750
                Mean return:              23.0938
                Policy entropy:           0.4580
                Pseudo loss:              3.88500
                Total gradient norm:      0.35051
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.781
                
Iteration (19000/50001) took 0.022 seconds.
                Mean final reward:        23.7500
                Mean return:              21.0000
                Policy entropy:           0.4811
                Pseudo loss:              2.66275
                Total gradient norm:      0.30442
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.750
                
Iteration (19100/50001) took 0.027 seconds.
                Mean final reward:        25.6250
                Mean return:              22.0938
                Policy entropy:           0.4706
                Pseudo loss:              4.89164
                Total gradient norm:      0.43727
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.531
                
Iteration (19200/50001) took 0.020 seconds.
                Mean final reward:        25.6250
                Mean return:              23.1875
                Policy entropy:           0.4703
                Pseudo loss:              2.87620
                Total gradient norm:      0.34668
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.438
                
Iteration (19300/50001) took 0.029 seconds.
                Mean final reward:        24.3750
                Mean return:              20.4062
                Policy entropy:           0.4774
                Pseudo loss:              4.51662
                Total gradient norm:      0.39483
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.742
                
Iteration (19400/50001) took 0.030 seconds.
                Mean final reward:        25.9062
                Mean return:              21.8438
                Policy entropy:           0.4252
                Pseudo loss:              3.45474
                Total gradient norm:      0.32613
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.839
                
Iteration (19500/50001) took 0.030 seconds.
                Mean final reward:        25.6250
                Mean return:              21.3125
                Policy entropy:           0.4974
                Pseudo loss:              4.69441
                Total gradient norm:      0.39565
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.312
                
Iteration (19600/50001) took 0.066 seconds.
                Mean final reward:        23.7500
                Mean return:              20.5312
                Policy entropy:           0.5012
                Pseudo loss:              3.24404
                Total gradient norm:      0.37267
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.219
                
Iteration (19700/50001) took 0.028 seconds.
                Mean final reward:        27.5000
                Mean return:              23.6875
                Policy entropy:           0.4161
                Pseudo loss:              3.25498
                Total gradient norm:      0.39151
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.812
                
Iteration (19800/50001) took 0.027 seconds.
                Mean final reward:        23.7500
                Mean return:              20.0000
                Policy entropy:           0.4620
                Pseudo loss:              5.00234
                Total gradient norm:      0.37134
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.516
                
Iteration (19900/50001) took 0.023 seconds.
                Mean final reward:        26.2500
                Mean return:              23.2500
                Policy entropy:           0.4143
                Pseudo loss:              3.39950
                Total gradient norm:      0.33086
                Solved trajectories:      31 / 32
                Avg steps to solve:       3.742
                
Iteration (20000/50001) took 0.027 seconds.
                Mean final reward:        23.7500
                Mean return:              20.8125
                Policy entropy:           0.4617
                Pseudo loss:              3.87432
                Total gradient norm:      0.36013
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.938
                
Iteration (20100/50001) took 0.034 seconds.
                Mean final reward:        24.0312
                Mean return:              20.2500
                Policy entropy:           0.4125
                Pseudo loss:              3.54567
                Total gradient norm:      0.36118
                Solved trajectories:      29 / 32
                Avg steps to solve:       4.034
                
Iteration (20200/50001) took 0.028 seconds.
                Mean final reward:        26.2500
                Mean return:              22.8438
                Policy entropy:           0.4034
                Pseudo loss:              3.17194
                Total gradient norm:      0.33245
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.406
                
Iteration (20300/50001) took 0.031 seconds.
                Mean final reward:        25.2812
                Mean return:              20.8125
                Policy entropy:           0.5296
                Pseudo loss:              4.84240
                Total gradient norm:      0.31105
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.258
                
Iteration (20400/50001) took 0.025 seconds.
                Mean final reward:        27.5000
                Mean return:              24.0625
                Policy entropy:           0.4036
                Pseudo loss:              2.22678
                Total gradient norm:      0.37717
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.438
                
Iteration (20500/50001) took 0.028 seconds.
                Mean final reward:        24.6562
                Mean return:              20.9688
                Policy entropy:           0.4616
                Pseudo loss:              3.35380
                Total gradient norm:      0.28598
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.200
                
Iteration (20600/50001) took 0.026 seconds.
                Mean final reward:        25.0000
                Mean return:              21.5312
                Policy entropy:           0.4003
                Pseudo loss:              3.74048
                Total gradient norm:      0.46739
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.469
                
Iteration (20700/50001) took 0.030 seconds.
                Mean final reward:        26.8750
                Mean return:              22.6875
                Policy entropy:           0.4291
                Pseudo loss:              4.64145
                Total gradient norm:      0.41256
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.968
                
Iteration (20800/50001) took 0.025 seconds.
                Mean final reward:        26.2500
                Mean return:              23.1875
                Policy entropy:           0.4645
                Pseudo loss:              3.97136
                Total gradient norm:      0.41607
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.062
                
Iteration (20900/50001) took 0.025 seconds.
                Mean final reward:        26.2500
                Mean return:              23.0000
                Policy entropy:           0.3853
                Pseudo loss:              3.27309
                Total gradient norm:      0.32106
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.250
                
Iteration (21000/50001) took 0.028 seconds.
                Mean final reward:        25.0000
                Mean return:              21.2500
                Policy entropy:           0.4606
                Pseudo loss:              5.05419
                Total gradient norm:      0.43349
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.750
                
Iteration (21100/50001) took 0.026 seconds.
                Mean final reward:        28.1250
                Mean return:              24.4688
                Policy entropy:           0.3639
                Pseudo loss:              4.69571
                Total gradient norm:      0.40898
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.419
                
Iteration (21200/50001) took 0.024 seconds.
                Mean final reward:        26.2500
                Mean return:              23.0938
                Policy entropy:           0.3835
                Pseudo loss:              3.24917
                Total gradient norm:      0.39853
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.156
                
Iteration (21300/50001) took 0.024 seconds.
                Mean final reward:        26.2500
                Mean return:              23.1875
                Policy entropy:           0.3516
                Pseudo loss:              2.29813
                Total gradient norm:      0.24832
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.062
                
Iteration (21400/50001) took 0.024 seconds.
                Mean final reward:        25.2812
                Mean return:              22.1250
                Policy entropy:           0.4081
                Pseudo loss:              2.29313
                Total gradient norm:      0.27414
                Solved trajectories:      31 / 32
                Avg steps to solve:       3.903
                
Iteration (21500/50001) took 0.025 seconds.
                Mean final reward:        27.5000
                Mean return:              24.8438
                Policy entropy:           0.3718
                Pseudo loss:              2.53183
                Total gradient norm:      0.43335
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.656
                
Iteration (21600/50001) took 0.027 seconds.
                Mean final reward:        26.2500
                Mean return:              22.8750
                Policy entropy:           0.4525
                Pseudo loss:              4.50540
                Total gradient norm:      0.37378
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.375
                
Iteration (21700/50001) took 0.031 seconds.
                Mean final reward:        21.8125
                Mean return:              17.5938
                Policy entropy:           0.4522
                Pseudo loss:              3.54406
                Total gradient norm:      0.46221
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.767
                
Iteration (21800/50001) took 0.026 seconds.
                Mean final reward:        25.6250
                Mean return:              22.2500
                Policy entropy:           0.4087
                Pseudo loss:              3.85783
                Total gradient norm:      0.38399
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.129
                
Iteration (21900/50001) took 0.030 seconds.
                Mean final reward:        26.2500
                Mean return:              22.3750
                Policy entropy:           0.4319
                Pseudo loss:              4.67558
                Total gradient norm:      0.30625
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.875
                
Iteration (22000/50001) took 0.043 seconds.
                Mean final reward:        26.8750
                Mean return:              23.1562
                Policy entropy:           0.3854
                Pseudo loss:              3.34181
                Total gradient norm:      0.46342
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.719
                
Iteration (22100/50001) took 0.030 seconds.
                Mean final reward:        27.5000
                Mean return:              23.1250
                Policy entropy:           0.3695
                Pseudo loss:              5.67104
                Total gradient norm:      0.56768
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.161
                
Iteration (22200/50001) took 0.032 seconds.
                Mean final reward:        26.5312
                Mean return:              22.2188
                Policy entropy:           0.4171
                Pseudo loss:              4.86938
                Total gradient norm:      0.53248
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.097
                
Iteration (22300/50001) took 0.025 seconds.
                Mean final reward:        24.3750
                Mean return:              21.1875
                Policy entropy:           0.3873
                Pseudo loss:              2.42098
                Total gradient norm:      0.29276
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.188
                
Iteration (22400/50001) took 0.029 seconds.
                Mean final reward:        25.0000
                Mean return:              21.3125
                Policy entropy:           0.4218
                Pseudo loss:              3.13881
                Total gradient norm:      0.35035
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.688
                
Iteration (22500/50001) took 0.023 seconds.
                Mean final reward:        24.3750
                Mean return:              21.5938
                Policy entropy:           0.3986
                Pseudo loss:              2.98498
                Total gradient norm:      0.23888
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.781
                
Iteration (22600/50001) took 0.025 seconds.
                Mean final reward:        27.5000
                Mean return:              24.6250
                Policy entropy:           0.3896
                Pseudo loss:              2.37545
                Total gradient norm:      0.36065
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.875
                
Iteration (22700/50001) took 0.030 seconds.
                Mean final reward:        27.5000
                Mean return:              23.4062
                Policy entropy:           0.4060
                Pseudo loss:              5.74917
                Total gradient norm:      0.40776
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.094
                
Iteration (22800/50001) took 0.028 seconds.
                Mean final reward:        25.0000
                Mean return:              21.1562
                Policy entropy:           0.3553
                Pseudo loss:              4.01584
                Total gradient norm:      0.31173
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.844
                
Iteration (22900/50001) took 0.026 seconds.
                Mean final reward:        27.5000
                Mean return:              24.0000
                Policy entropy:           0.3628
                Pseudo loss:              4.12133
                Total gradient norm:      0.29036
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.258
                
Iteration (23000/50001) took 0.028 seconds.
                Mean final reward:        29.3750
                Mean return:              25.5938
                Policy entropy:           0.4273
                Pseudo loss:              5.12961
                Total gradient norm:      0.36348
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.548
                
Iteration (23100/50001) took 0.023 seconds.
                Mean final reward:        23.1250
                Mean return:              20.2188
                Policy entropy:           0.3981
                Pseudo loss:              2.97476
                Total gradient norm:      0.26805
                Solved trajectories:      31 / 32
                Avg steps to solve:       3.645
                
Iteration (23200/50001) took 0.029 seconds.
                Mean final reward:        27.5000
                Mean return:              23.8750
                Policy entropy:           0.3697
                Pseudo loss:              4.87090
                Total gradient norm:      0.43119
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.387
                
Iteration (23300/50001) took 0.026 seconds.
                Mean final reward:        23.7500
                Mean return:              20.5000
                Policy entropy:           0.4254
                Pseudo loss:              4.27245
                Total gradient norm:      0.31416
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.250
                
Iteration (23400/50001) took 0.032 seconds.
                Mean final reward:        27.5000
                Mean return:              23.0000
                Policy entropy:           0.4341
                Pseudo loss:              4.92623
                Total gradient norm:      0.35181
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.500
                
Iteration (23500/50001) took 0.030 seconds.
                Mean final reward:        26.8750
                Mean return:              22.5938
                Policy entropy:           0.3537
                Pseudo loss:              4.06642
                Total gradient norm:      0.35396
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.065
                
Iteration (23600/50001) took 0.034 seconds.
                Mean final reward:        29.3750
                Mean return:              25.4688
                Policy entropy:           0.3585
                Pseudo loss:              4.09091
                Total gradient norm:      0.32558
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.906
                
Iteration (23700/50001) took 0.026 seconds.
                Mean final reward:        27.5000
                Mean return:              23.8750
                Policy entropy:           0.3996
                Pseudo loss:              2.56883
                Total gradient norm:      0.30758
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.625
                
Iteration (23800/50001) took 0.028 seconds.
                Mean final reward:        28.7500
                Mean return:              24.9375
                Policy entropy:           0.3767
                Pseudo loss:              3.71677
                Total gradient norm:      0.47868
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.812
                
Iteration (23900/50001) took 0.024 seconds.
                Mean final reward:        26.2500
                Mean return:              23.1250
                Policy entropy:           0.3997
                Pseudo loss:              2.87780
                Total gradient norm:      0.34247
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.125
                
Iteration (24000/50001) took 0.026 seconds.
                Mean final reward:        28.1250
                Mean return:              24.5938
                Policy entropy:           0.3407
                Pseudo loss:              3.21827
                Total gradient norm:      0.27562
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.531
                
Iteration (24100/50001) took 0.025 seconds.
                Mean final reward:        25.0000
                Mean return:              21.8438
                Policy entropy:           0.3201
                Pseudo loss:              1.75474
                Total gradient norm:      0.25790
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.156
                
Iteration (24200/50001) took 0.024 seconds.
                Mean final reward:        26.8750
                Mean return:              23.7812
                Policy entropy:           0.4038
                Pseudo loss:              3.72210
                Total gradient norm:      0.32872
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.094
                
Iteration (24300/50001) took 0.028 seconds.
                Mean final reward:        26.8750
                Mean return:              23.1250
                Policy entropy:           0.3498
                Pseudo loss:              3.54427
                Total gradient norm:      0.30964
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.750
                
Iteration (24400/50001) took 0.029 seconds.
                Mean final reward:        26.8750
                Mean return:              23.0000
                Policy entropy:           0.3815
                Pseudo loss:              4.87971
                Total gradient norm:      0.38030
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.875
                
Iteration (24500/50001) took 0.027 seconds.
                Mean final reward:        25.6250
                Mean return:              21.9375
                Policy entropy:           0.3678
                Pseudo loss:              2.93983
                Total gradient norm:      0.25898
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.688
                
Iteration (24600/50001) took 0.028 seconds.
                Mean final reward:        24.3750
                Mean return:              20.5312
                Policy entropy:           0.3482
                Pseudo loss:              2.29020
                Total gradient norm:      0.27805
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.844
                
Iteration (24700/50001) took 0.024 seconds.
                Mean final reward:        23.7500
                Mean return:              20.6875
                Policy entropy:           0.3433
                Pseudo loss:              2.43313
                Total gradient norm:      0.21604
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.062
                
Iteration (24800/50001) took 0.027 seconds.
                Mean final reward:        24.3750
                Mean return:              20.7188
                Policy entropy:           0.3485
                Pseudo loss:              4.29881
                Total gradient norm:      0.39283
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.656
                
Iteration (24900/50001) took 0.024 seconds.
                Mean final reward:        25.0000
                Mean return:              21.8750
                Policy entropy:           0.3408
                Pseudo loss:              3.19385
                Total gradient norm:      0.35341
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.125
                
Iteration (25000/50001) took 0.025 seconds.
                Mean final reward:        26.2500
                Mean return:              23.0000
                Policy entropy:           0.3548
                Pseudo loss:              3.30337
                Total gradient norm:      0.36309
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.000
                
Iteration (25100/50001) took 0.028 seconds.
                Mean final reward:        25.9062
                Mean return:              22.0000
                Policy entropy:           0.4245
                Pseudo loss:              3.50414
                Total gradient norm:      0.28445
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.433
                
Iteration (25200/50001) took 0.026 seconds.
                Mean final reward:        28.1250
                Mean return:              24.6562
                Policy entropy:           0.3654
                Pseudo loss:              3.60318
                Total gradient norm:      0.31134
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.469
                
Iteration (25300/50001) took 0.018 seconds.
                Mean final reward:        26.2500
                Mean return:              24.2500
                Policy entropy:           0.3434
                Pseudo loss:              1.03829
                Total gradient norm:      0.18153
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.000
                
Iteration (25400/50001) took 0.027 seconds.
                Mean final reward:        22.7812
                Mean return:              19.0938
                Policy entropy:           0.3762
                Pseudo loss:              1.91961
                Total gradient norm:      0.32932
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.452
                
Iteration (25500/50001) took 0.039 seconds.
                Mean final reward:        26.8750
                Mean return:              23.6562
                Policy entropy:           0.3209
                Pseudo loss:              2.56086
                Total gradient norm:      0.30184
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.219
                
Iteration (25600/50001) took 0.027 seconds.
                Mean final reward:        27.5000
                Mean return:              23.7500
                Policy entropy:           0.3655
                Pseudo loss:              4.05201
                Total gradient norm:      0.36792
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.750
                
Iteration (25700/50001) took 0.022 seconds.
                Mean final reward:        24.3750
                Mean return:              21.7812
                Policy entropy:           0.2921
                Pseudo loss:              2.38125
                Total gradient norm:      0.31576
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.594
                
Iteration (25800/50001) took 0.023 seconds.
                Mean final reward:        27.5000
                Mean return:              24.4375
                Policy entropy:           0.3309
                Pseudo loss:              2.81753
                Total gradient norm:      0.26060
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.062
                
Iteration (25900/50001) took 0.028 seconds.
                Mean final reward:        28.7500
                Mean return:              24.8438
                Policy entropy:           0.3893
                Pseudo loss:              5.29659
                Total gradient norm:      0.39972
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.906
                
Iteration (26000/50001) took 0.026 seconds.
                Mean final reward:        26.8750
                Mean return:              23.4688
                Policy entropy:           0.4350
                Pseudo loss:              4.07122
                Total gradient norm:      0.25858
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.406
                
Iteration (26100/50001) took 0.027 seconds.
                Mean final reward:        27.5000
                Mean return:              24.0000
                Policy entropy:           0.3139
                Pseudo loss:              3.08999
                Total gradient norm:      0.35280
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.500
                
Iteration (26200/50001) took 0.021 seconds.
                Mean final reward:        24.3750
                Mean return:              22.0312
                Policy entropy:           0.3124
                Pseudo loss:              1.83045
                Total gradient norm:      0.21252
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.344
                
Iteration (26300/50001) took 0.025 seconds.
                Mean final reward:        25.0000
                Mean return:              21.6562
                Policy entropy:           0.3513
                Pseudo loss:              2.92260
                Total gradient norm:      0.24282
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.097
                
Iteration (26400/50001) took 0.021 seconds.
                Mean final reward:        25.0000
                Mean return:              22.4375
                Policy entropy:           0.3540
                Pseudo loss:              1.90465
                Total gradient norm:      0.26777
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.562
                
Iteration (26500/50001) took 0.027 seconds.
                Mean final reward:        27.5000
                Mean return:              23.7812
                Policy entropy:           0.3230
                Pseudo loss:              3.21917
                Total gradient norm:      0.37088
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.719
                
Iteration (26600/50001) took 0.029 seconds.
                Mean final reward:        26.8750
                Mean return:              22.8750
                Policy entropy:           0.3486
                Pseudo loss:              3.83935
                Total gradient norm:      0.31098
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.000
                
Iteration (26700/50001) took 0.026 seconds.
                Mean final reward:        28.7500
                Mean return:              25.3438
                Policy entropy:           0.2898
                Pseudo loss:              2.66629
                Total gradient norm:      0.30702
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.406
                
Iteration (26800/50001) took 0.026 seconds.
                Mean final reward:        28.1250
                Mean return:              24.7188
                Policy entropy:           0.3486
                Pseudo loss:              3.10445
                Total gradient norm:      0.30513
                Solved trajectories:      30 / 32
                Avg steps to solve:       3.900
                
Iteration (26900/50001) took 0.026 seconds.
                Mean final reward:        26.8750
                Mean return:              23.6250
                Policy entropy:           0.3186
                Pseudo loss:              2.62411
                Total gradient norm:      0.26211
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.250
                
Iteration (27000/50001) took 0.031 seconds.
                Mean final reward:        28.4062
                Mean return:              24.3750
                Policy entropy:           0.3618
                Pseudo loss:              3.30748
                Total gradient norm:      0.31804
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.806
                
Iteration (27100/50001) took 0.040 seconds.
                Mean final reward:        24.3750
                Mean return:              21.1250
                Policy entropy:           0.3640
                Pseudo loss:              3.44364
                Total gradient norm:      0.35581
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.250
                
Iteration (27200/50001) took 0.029 seconds.
                Mean final reward:        29.3750
                Mean return:              25.7188
                Policy entropy:           0.2848
                Pseudo loss:              2.12695
                Total gradient norm:      0.27799
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.419
                
Iteration (27300/50001) took 0.029 seconds.
                Mean final reward:        26.8750
                Mean return:              23.2812
                Policy entropy:           0.3432
                Pseudo loss:              3.42108
                Total gradient norm:      0.28491
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.594
                
Iteration (27400/50001) took 0.026 seconds.
                Mean final reward:        24.3750
                Mean return:              21.0000
                Policy entropy:           0.3236
                Pseudo loss:              1.96218
                Total gradient norm:      0.26420
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.129
                
Iteration (27500/50001) took 0.028 seconds.
                Mean final reward:        28.1250
                Mean return:              24.2500
                Policy entropy:           0.3218
                Pseudo loss:              3.74300
                Total gradient norm:      0.33731
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.400
                
Iteration (27600/50001) took 0.026 seconds.
                Mean final reward:        25.6250
                Mean return:              22.2188
                Policy entropy:           0.3744
                Pseudo loss:              4.08835
                Total gradient norm:      0.35370
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.406
                
Iteration (27700/50001) took 0.022 seconds.
                Mean final reward:        25.0000
                Mean return:              22.3125
                Policy entropy:           0.2941
                Pseudo loss:              2.61277
                Total gradient norm:      0.37344
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.688
                
Iteration (27800/50001) took 0.027 seconds.
                Mean final reward:        26.8750
                Mean return:              23.2188
                Policy entropy:           0.3244
                Pseudo loss:              4.09956
                Total gradient norm:      0.37607
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.656
                
Iteration (27900/50001) took 0.026 seconds.
                Mean final reward:        25.0000
                Mean return:              21.4062
                Policy entropy:           0.3128
                Pseudo loss:              2.84835
                Total gradient norm:      0.21197
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.594
                
Iteration (28000/50001) took 0.028 seconds.
                Mean final reward:        26.8750
                Mean return:              23.0938
                Policy entropy:           0.3094
                Pseudo loss:              3.90222
                Total gradient norm:      0.35658
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.781
                
Iteration (28100/50001) took 0.027 seconds.
                Mean final reward:        28.1250
                Mean return:              24.4688
                Policy entropy:           0.2811
                Pseudo loss:              2.40488
                Total gradient norm:      0.26075
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.656
                
Iteration (28200/50001) took 0.031 seconds.
                Mean final reward:        28.1250
                Mean return:              23.7500
                Policy entropy:           0.2883
                Pseudo loss:              3.02752
                Total gradient norm:      0.33765
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.375
                
Iteration (28300/50001) took 0.026 seconds.
                Mean final reward:        28.7500
                Mean return:              25.2812
                Policy entropy:           0.2031
                Pseudo loss:              1.72082
                Total gradient norm:      0.22491
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.469
                
Iteration (28400/50001) took 0.031 seconds.
                Mean final reward:        28.1250
                Mean return:              23.7500
                Policy entropy:           0.3361
                Pseudo loss:              3.98119
                Total gradient norm:      0.47622
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.375
                
Iteration (28500/50001) took 0.026 seconds.
                Mean final reward:        24.3750
                Mean return:              21.0000
                Policy entropy:           0.3776
                Pseudo loss:              3.49665
                Total gradient norm:      0.29190
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.375
                
Iteration (28600/50001) took 0.027 seconds.
                Mean final reward:        27.5000
                Mean return:              23.7812
                Policy entropy:           0.2991
                Pseudo loss:              3.87760
                Total gradient norm:      0.40109
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.719
                
Iteration (28700/50001) took 0.026 seconds.
                Mean final reward:        26.2500
                Mean return:              23.0625
                Policy entropy:           0.3345
                Pseudo loss:              2.40325
                Total gradient norm:      0.28339
                Solved trajectories:      31 / 32
                Avg steps to solve:       3.935
                
Iteration (28800/50001) took 0.026 seconds.
                Mean final reward:        25.0000
                Mean return:              21.6562
                Policy entropy:           0.3305
                Pseudo loss:              2.18865
                Total gradient norm:      0.40153
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.344
                
Iteration (28900/50001) took 0.024 seconds.
                Mean final reward:        27.5000
                Mean return:              24.3750
                Policy entropy:           0.3145
                Pseudo loss:              2.93459
                Total gradient norm:      0.37634
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.125
                
Iteration (29000/50001) took 0.027 seconds.
                Mean final reward:        28.1250
                Mean return:              24.4062
                Policy entropy:           0.2512
                Pseudo loss:              2.05133
                Total gradient norm:      0.33451
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.719
                
Iteration (29100/50001) took 0.028 seconds.
                Mean final reward:        26.2500
                Mean return:              22.7812
                Policy entropy:           0.3103
                Pseudo loss:              3.45843
                Total gradient norm:      0.43482
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.469
                
Iteration (29200/50001) took 0.029 seconds.
                Mean final reward:        26.2500
                Mean return:              22.3438
                Policy entropy:           0.3169
                Pseudo loss:              4.25627
                Total gradient norm:      0.35514
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.906
                
Iteration (29300/50001) took 0.023 seconds.
                Mean final reward:        26.2500
                Mean return:              23.2500
                Policy entropy:           0.2798
                Pseudo loss:              1.51346
                Total gradient norm:      0.18944
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.000
                
Iteration (29400/50001) took 0.026 seconds.
                Mean final reward:        24.6562
                Mean return:              21.1250
                Policy entropy:           0.2484
                Pseudo loss:              2.81777
                Total gradient norm:      0.39005
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.290
                
Iteration (29500/50001) took 0.024 seconds.
                Mean final reward:        23.7500
                Mean return:              20.8750
                Policy entropy:           0.3678
                Pseudo loss:              2.26485
                Total gradient norm:      0.22654
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.875
                
Iteration (29600/50001) took 0.025 seconds.
                Mean final reward:        25.6250
                Mean return:              22.3438
                Policy entropy:           0.2497
                Pseudo loss:              1.50819
                Total gradient norm:      0.20806
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.281
                
Iteration (29700/50001) took 0.021 seconds.
                Mean final reward:        23.1250
                Mean return:              20.7500
                Policy entropy:           0.3791
                Pseudo loss:              2.21850
                Total gradient norm:      0.16056
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.375
                
Iteration (29800/50001) took 0.025 seconds.
                Mean final reward:        25.6250
                Mean return:              22.3438
                Policy entropy:           0.2781
                Pseudo loss:              1.79701
                Total gradient norm:      0.17872
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.281
                
Iteration (29900/50001) took 0.027 seconds.
                Mean final reward:        23.7500
                Mean return:              20.1562
                Policy entropy:           0.2517
                Pseudo loss:              2.03004
                Total gradient norm:      0.27340
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.594
                
Iteration (30000/50001) took 0.022 seconds.
                Mean final reward:        23.7500
                Mean return:              21.0938
                Policy entropy:           0.2789
                Pseudo loss:              1.79626
                Total gradient norm:      0.24442
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.656
                
Iteration (30100/50001) took 0.024 seconds.
                Mean final reward:        26.2500
                Mean return:              23.0625
                Policy entropy:           0.2705
                Pseudo loss:              1.56493
                Total gradient norm:      0.23445
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.188
                
Iteration (30200/50001) took 0.024 seconds.
                Mean final reward:        25.6250
                Mean return:              22.6875
                Policy entropy:           0.2790
                Pseudo loss:              1.67087
                Total gradient norm:      0.19261
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.938
                
Iteration (30300/50001) took 0.023 seconds.
                Mean final reward:        26.2500
                Mean return:              23.3438
                Policy entropy:           0.2295
                Pseudo loss:              1.30084
                Total gradient norm:      0.21848
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.906
                
Iteration (30400/50001) took 0.029 seconds.
                Mean final reward:        28.1250
                Mean return:              24.0625
                Policy entropy:           0.2529
                Pseudo loss:              1.67209
                Total gradient norm:      0.32800
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.062
                
Iteration (30500/50001) took 0.026 seconds.
                Mean final reward:        26.2500
                Mean return:              23.0625
                Policy entropy:           0.3098
                Pseudo loss:              2.72879
                Total gradient norm:      0.21635
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.188
                
Iteration (30600/50001) took 0.027 seconds.
                Mean final reward:        26.2500
                Mean return:              22.6562
                Policy entropy:           0.2516
                Pseudo loss:              2.06512
                Total gradient norm:      0.24148
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.594
                
Iteration (30700/50001) took 0.025 seconds.
                Mean final reward:        25.6250
                Mean return:              22.3750
                Policy entropy:           0.3062
                Pseudo loss:              3.18602
                Total gradient norm:      0.39021
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.250
                
Iteration (30800/50001) took 0.029 seconds.
                Mean final reward:        26.2500
                Mean return:              22.2188
                Policy entropy:           0.2292
                Pseudo loss:              2.67010
                Total gradient norm:      0.30602
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.031
                
Iteration (30900/50001) took 0.027 seconds.
                Mean final reward:        28.1250
                Mean return:              24.4062
                Policy entropy:           0.3174
                Pseudo loss:              3.79547
                Total gradient norm:      0.30178
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.484
                
Iteration (31000/50001) took 0.033 seconds.
                Mean final reward:        25.9062
                Mean return:              21.7500
                Policy entropy:           0.3018
                Pseudo loss:              2.62891
                Total gradient norm:      0.28420
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.700
                
Iteration (31100/50001) took 0.029 seconds.
                Mean final reward:        26.8750
                Mean return:              22.7812
                Policy entropy:           0.3068
                Pseudo loss:              4.75327
                Total gradient norm:      0.46789
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.094
                
Iteration (31200/50001) took 0.026 seconds.
                Mean final reward:        26.2500
                Mean return:              22.6562
                Policy entropy:           0.3164
                Pseudo loss:              2.43905
                Total gradient norm:      0.22683
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.594
                
Iteration (31300/50001) took 0.035 seconds.
                Mean final reward:        27.5000
                Mean return:              24.0000
                Policy entropy:           0.2595
                Pseudo loss:              2.98763
                Total gradient norm:      0.33545
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.500
                
Iteration (31400/50001) took 0.029 seconds.
                Mean final reward:        27.5000
                Mean return:              23.8750
                Policy entropy:           0.3049
                Pseudo loss:              2.74474
                Total gradient norm:      0.21500
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.387
                
Iteration (31500/50001) took 0.028 seconds.
                Mean final reward:        26.8750
                Mean return:              23.0312
                Policy entropy:           0.2339
                Pseudo loss:              1.88419
                Total gradient norm:      0.21799
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.844
                
Iteration (31600/50001) took 0.030 seconds.
                Mean final reward:        27.5000
                Mean return:              23.6250
                Policy entropy:           0.2559
                Pseudo loss:              2.94335
                Total gradient norm:      0.27305
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.875
                
Iteration (31700/50001) took 0.027 seconds.
                Mean final reward:        27.5000
                Mean return:              24.0312
                Policy entropy:           0.3115
                Pseudo loss:              2.18003
                Total gradient norm:      0.25603
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.469
                
Iteration (31800/50001) took 0.027 seconds.
                Mean final reward:        26.8750
                Mean return:              23.6250
                Policy entropy:           0.2199
                Pseudo loss:              1.41093
                Total gradient norm:      0.16577
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.250
                
Iteration (31900/50001) took 0.028 seconds.
                Mean final reward:        25.6250
                Mean return:              21.7500
                Policy entropy:           0.2362
                Pseudo loss:              1.52157
                Total gradient norm:      0.22842
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.875
                
Iteration (32000/50001) took 0.025 seconds.
                Mean final reward:        26.8750
                Mean return:              23.7188
                Policy entropy:           0.2810
                Pseudo loss:              1.77349
                Total gradient norm:      0.20093
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.156
                
Iteration (32100/50001) took 0.032 seconds.
                Mean final reward:        26.8750
                Mean return:              22.2812
                Policy entropy:           0.2170
                Pseudo loss:              2.92835
                Total gradient norm:      0.21020
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.387
                
Iteration (32200/50001) took 0.026 seconds.
                Mean final reward:        26.2500
                Mean return:              22.8750
                Policy entropy:           0.2611
                Pseudo loss:              3.62678
                Total gradient norm:      0.30268
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.129
                
Iteration (32300/50001) took 0.034 seconds.
                Mean final reward:        28.7500
                Mean return:              23.7188
                Policy entropy:           0.1933
                Pseudo loss:              2.34548
                Total gradient norm:      0.17799
                Solved trajectories:      32 / 32
                Avg steps to solve:       6.031
                
Iteration (32400/50001) took 0.026 seconds.
                Mean final reward:        23.4062
                Mean return:              19.8125
                Policy entropy:           0.2421
                Pseudo loss:              1.90905
                Total gradient norm:      0.21719
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.355
                
Iteration (32500/50001) took 0.028 seconds.
                Mean final reward:        23.7500
                Mean return:              20.0938
                Policy entropy:           0.2557
                Pseudo loss:              2.28942
                Total gradient norm:      0.39618
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.419
                
Iteration (32600/50001) took 0.028 seconds.
                Mean final reward:        26.2500
                Mean return:              22.4688
                Policy entropy:           0.2375
                Pseudo loss:              2.22546
                Total gradient norm:      0.20201
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.781
                
Iteration (32700/50001) took 0.026 seconds.
                Mean final reward:        25.6250
                Mean return:              22.1562
                Policy entropy:           0.1864
                Pseudo loss:              1.47765
                Total gradient norm:      0.22654
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.469
                
Iteration (32800/50001) took 0.029 seconds.
                Mean final reward:        27.5000
                Mean return:              23.5625
                Policy entropy:           0.2896
                Pseudo loss:              3.08630
                Total gradient norm:      0.39981
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.938
                
Iteration (32900/50001) took 0.047 seconds.
                Mean final reward:        28.1250
                Mean return:              24.0625
                Policy entropy:           0.2656
                Pseudo loss:              2.31970
                Total gradient norm:      0.34490
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.062
                
Iteration (33000/50001) took 0.025 seconds.
                Mean final reward:        28.1250
                Mean return:              25.0312
                Policy entropy:           0.2505
                Pseudo loss:              1.96469
                Total gradient norm:      0.22612
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.094
                
Iteration (33100/50001) took 0.024 seconds.
                Mean final reward:        25.6250
                Mean return:              22.5000
                Policy entropy:           0.2645
                Pseudo loss:              2.17106
                Total gradient norm:      0.25520
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.125
                
Iteration (33200/50001) took 0.028 seconds.
                Mean final reward:        26.8750
                Mean return:              23.0938
                Policy entropy:           0.2442
                Pseudo loss:              2.39964
                Total gradient norm:      0.33908
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.781
                
Iteration (33300/50001) took 0.026 seconds.
                Mean final reward:        26.8750
                Mean return:              23.4062
                Policy entropy:           0.2880
                Pseudo loss:              3.33073
                Total gradient norm:      0.38102
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.469
                
Iteration (33400/50001) took 0.031 seconds.
                Mean final reward:        27.5000
                Mean return:              23.2812
                Policy entropy:           0.2239
                Pseudo loss:              2.04010
                Total gradient norm:      0.22645
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.219
                
Iteration (33500/50001) took 0.042 seconds.
                Mean final reward:        25.6250
                Mean return:              22.2812
                Policy entropy:           0.2165
                Pseudo loss:              2.60630
                Total gradient norm:      0.31175
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.344
                
Iteration (33600/50001) took 0.026 seconds.
                Mean final reward:        26.2500
                Mean return:              22.8750
                Policy entropy:           0.2172
                Pseudo loss:              2.01743
                Total gradient norm:      0.25233
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.375
                
Iteration (33700/50001) took 0.025 seconds.
                Mean final reward:        26.2500
                Mean return:              23.0938
                Policy entropy:           0.2592
                Pseudo loss:              2.93358
                Total gradient norm:      0.30962
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.156
                
Iteration (33800/50001) took 0.033 seconds.
                Mean final reward:        28.1250
                Mean return:              23.6875
                Policy entropy:           0.2475
                Pseudo loss:              4.02206
                Total gradient norm:      0.42024
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.438
                
Iteration (33900/50001) took 0.028 seconds.
                Mean final reward:        27.1562
                Mean return:              23.4062
                Policy entropy:           0.2158
                Pseudo loss:              1.34310
                Total gradient norm:      0.17804
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.516
                
Iteration (34000/50001) took 0.029 seconds.
                Mean final reward:        28.1250
                Mean return:              23.9062
                Policy entropy:           0.2322
                Pseudo loss:              1.80786
                Total gradient norm:      0.25390
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.219
                
Iteration (34100/50001) took 0.026 seconds.
                Mean final reward:        24.3750
                Mean return:              20.9688
                Policy entropy:           0.3119
                Pseudo loss:              2.51528
                Total gradient norm:      0.31692
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.406
                
Iteration (34200/50001) took 0.025 seconds.
                Mean final reward:        24.3750
                Mean return:              21.0000
                Policy entropy:           0.2183
                Pseudo loss:              1.38328
                Total gradient norm:      0.14571
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.375
                
Iteration (34300/50001) took 0.024 seconds.
                Mean final reward:        25.0000
                Mean return:              21.9375
                Policy entropy:           0.2044
                Pseudo loss:              1.36660
                Total gradient norm:      0.21587
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.062
                
Iteration (34400/50001) took 0.026 seconds.
                Mean final reward:        26.2500
                Mean return:              22.9375
                Policy entropy:           0.2190
                Pseudo loss:              1.32235
                Total gradient norm:      0.16155
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.312
                
Iteration (34500/50001) took 0.031 seconds.
                Mean final reward:        26.8750
                Mean return:              23.0312
                Policy entropy:           0.2402
                Pseudo loss:              2.65086
                Total gradient norm:      0.35278
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.844
                
Iteration (34600/50001) took 0.024 seconds.
                Mean final reward:        25.0000
                Mean return:              22.0625
                Policy entropy:           0.2531
                Pseudo loss:              2.96683
                Total gradient norm:      0.45509
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.938
                
Iteration (34700/50001) took 0.027 seconds.
                Mean final reward:        24.3750
                Mean return:              21.0000
                Policy entropy:           0.1735
                Pseudo loss:              0.83835
                Total gradient norm:      0.17445
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.375
                
Iteration (34800/50001) took 0.025 seconds.
                Mean final reward:        26.2500
                Mean return:              22.9688
                Policy entropy:           0.2454
                Pseudo loss:              2.40964
                Total gradient norm:      0.28224
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.281
                
Iteration (34900/50001) took 0.026 seconds.
                Mean final reward:        28.1250
                Mean return:              24.7500
                Policy entropy:           0.2564
                Pseudo loss:              2.09576
                Total gradient norm:      0.21032
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.375
                
Iteration (35000/50001) took 0.031 seconds.
                Mean final reward:        26.2500
                Mean return:              22.6875
                Policy entropy:           0.2693
                Pseudo loss:              2.12036
                Total gradient norm:      0.19536
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.562
                
Iteration (35100/50001) took 0.029 seconds.
                Mean final reward:        27.5000
                Mean return:              23.6250
                Policy entropy:           0.2330
                Pseudo loss:              2.37426
                Total gradient norm:      0.29056
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.645
                
Iteration (35200/50001) took 0.032 seconds.
                Mean final reward:        27.5000
                Mean return:              24.3125
                Policy entropy:           0.2609
                Pseudo loss:              2.02222
                Total gradient norm:      0.21353
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.188
                
Iteration (35300/50001) took 0.029 seconds.
                Mean final reward:        27.5000
                Mean return:              24.2812
                Policy entropy:           0.2443
                Pseudo loss:              2.72054
                Total gradient norm:      0.33799
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.219
                
Iteration (35400/50001) took 0.030 seconds.
                Mean final reward:        28.7500
                Mean return:              24.5625
                Policy entropy:           0.1941
                Pseudo loss:              2.08692
                Total gradient norm:      0.30960
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.188
                
Iteration (35500/50001) took 0.025 seconds.
                Mean final reward:        25.6250
                Mean return:              22.5938
                Policy entropy:           0.2171
                Pseudo loss:              1.82817
                Total gradient norm:      0.27543
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.031
                
Iteration (35600/50001) took 0.025 seconds.
                Mean final reward:        25.0000
                Mean return:              21.6562
                Policy entropy:           0.2607
                Pseudo loss:              3.00643
                Total gradient norm:      0.27279
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.097
                
Iteration (35700/50001) took 0.030 seconds.
                Mean final reward:        26.2500
                Mean return:              22.0938
                Policy entropy:           0.1760
                Pseudo loss:              2.85960
                Total gradient norm:      0.24361
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.935
                
Iteration (35800/50001) took 0.030 seconds.
                Mean final reward:        28.7500
                Mean return:              24.4062
                Policy entropy:           0.2251
                Pseudo loss:              2.40152
                Total gradient norm:      0.31558
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.344
                
Iteration (35900/50001) took 0.032 seconds.
                Mean final reward:        25.6250
                Mean return:              21.4688
                Policy entropy:           0.2497
                Pseudo loss:              2.86572
                Total gradient norm:      0.27121
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.156
                
Iteration (36000/50001) took 0.023 seconds.
                Mean final reward:        28.7500
                Mean return:              26.1875
                Policy entropy:           0.1695
                Pseudo loss:              1.15919
                Total gradient norm:      0.21057
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.562
                
Iteration (36100/50001) took 0.030 seconds.
                Mean final reward:        25.0000
                Mean return:              20.5938
                Policy entropy:           0.2182
                Pseudo loss:              3.11041
                Total gradient norm:      0.30430
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.194
                
Iteration (36200/50001) took 0.027 seconds.
                Mean final reward:        28.7500
                Mean return:              25.2812
                Policy entropy:           0.2135
                Pseudo loss:              1.50518
                Total gradient norm:      0.15078
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.469
                
Iteration (36300/50001) took 0.025 seconds.
                Mean final reward:        26.2500
                Mean return:              22.9688
                Policy entropy:           0.2550
                Pseudo loss:              2.43351
                Total gradient norm:      0.38882
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.281
                
Iteration (36400/50001) took 0.024 seconds.
                Mean final reward:        26.8750
                Mean return:              23.7500
                Policy entropy:           0.2975
                Pseudo loss:              2.57412
                Total gradient norm:      0.28968
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.125
                
Iteration (36500/50001) took 0.023 seconds.
                Mean final reward:        26.2500
                Mean return:              23.5312
                Policy entropy:           0.2367
                Pseudo loss:              2.42641
                Total gradient norm:      0.41772
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.719
                
Iteration (36600/50001) took 0.026 seconds.
                Mean final reward:        26.2500
                Mean return:              22.6562
                Policy entropy:           0.2326
                Pseudo loss:              3.16136
                Total gradient norm:      0.28266
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.594
                
Iteration (36700/50001) took 0.028 seconds.
                Mean final reward:        27.5000
                Mean return:              23.6562
                Policy entropy:           0.3013
                Pseudo loss:              2.71380
                Total gradient norm:      0.33656
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.844
                
Iteration (36800/50001) took 0.030 seconds.
                Mean final reward:        27.5000
                Mean return:              23.5000
                Policy entropy:           0.1856
                Pseudo loss:              1.99527
                Total gradient norm:      0.19762
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.774
                
Iteration (36900/50001) took 0.027 seconds.
                Mean final reward:        26.2500
                Mean return:              22.6250
                Policy entropy:           0.2075
                Pseudo loss:              2.27184
                Total gradient norm:      0.27939
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.387
                
Iteration (37000/50001) took 0.028 seconds.
                Mean final reward:        26.2500
                Mean return:              22.4062
                Policy entropy:           0.2393
                Pseudo loss:              3.04902
                Total gradient norm:      0.34886
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.844
                
Iteration (37100/50001) took 0.024 seconds.
                Mean final reward:        25.6250
                Mean return:              22.5000
                Policy entropy:           0.2338
                Pseudo loss:              1.70403
                Total gradient norm:      0.19286
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.125
                
Iteration (37200/50001) took 0.026 seconds.
                Mean final reward:        25.0000
                Mean return:              21.6562
                Policy entropy:           0.2117
                Pseudo loss:              1.32303
                Total gradient norm:      0.13019
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.344
                
Iteration (37300/50001) took 0.027 seconds.
                Mean final reward:        28.1250
                Mean return:              24.3750
                Policy entropy:           0.2984
                Pseudo loss:              4.59078
                Total gradient norm:      0.39811
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.750
                
Iteration (37400/50001) took 0.037 seconds.
                Mean final reward:        28.4062
                Mean return:              23.7500
                Policy entropy:           0.1968
                Pseudo loss:              1.43899
                Total gradient norm:      0.21070
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.452
                
Iteration (37500/50001) took 0.028 seconds.
                Mean final reward:        26.2500
                Mean return:              22.6562
                Policy entropy:           0.2578
                Pseudo loss:              2.82611
                Total gradient norm:      0.33395
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.594
                
Iteration (37600/50001) took 0.027 seconds.
                Mean final reward:        25.0000
                Mean return:              21.4375
                Policy entropy:           0.2013
                Pseudo loss:              1.32051
                Total gradient norm:      0.21422
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.562
                
Iteration (37700/50001) took 0.025 seconds.
                Mean final reward:        28.7500
                Mean return:              25.4375
                Policy entropy:           0.2444
                Pseudo loss:              1.51735
                Total gradient norm:      0.16565
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.312
                
Iteration (37800/50001) took 0.023 seconds.
                Mean final reward:        26.2500
                Mean return:              23.3438
                Policy entropy:           0.2208
                Pseudo loss:              1.11194
                Total gradient norm:      0.16912
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.906
                
Iteration (37900/50001) took 0.033 seconds.
                Mean final reward:        29.3750
                Mean return:              25.0938
                Policy entropy:           0.1675
                Pseudo loss:              2.01960
                Total gradient norm:      0.27471
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.281
                
Iteration (38000/50001) took 0.022 seconds.
                Mean final reward:        24.3750
                Mean return:              21.8438
                Policy entropy:           0.1971
                Pseudo loss:              1.14383
                Total gradient norm:      0.25533
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.531
                
Iteration (38100/50001) took 0.028 seconds.
                Mean final reward:        26.8750
                Mean return:              23.0000
                Policy entropy:           0.2770
                Pseudo loss:              2.23542
                Total gradient norm:      0.23229
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.875
                
Iteration (38200/50001) took 0.028 seconds.
                Mean final reward:        26.1875
                Mean return:              22.5312
                Policy entropy:           0.1355
                Pseudo loss:              0.21093
                Total gradient norm:      0.10814
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.167
                
Iteration (38300/50001) took 0.026 seconds.
                Mean final reward:        26.8750
                Mean return:              23.4688
                Policy entropy:           0.1684
                Pseudo loss:              0.91839
                Total gradient norm:      0.17386
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.406
                
Iteration (38400/50001) took 0.026 seconds.
                Mean final reward:        27.5000
                Mean return:              24.0312
                Policy entropy:           0.2203
                Pseudo loss:              1.84383
                Total gradient norm:      0.29223
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.469
                
Iteration (38500/50001) took 0.031 seconds.
                Mean final reward:        28.7500
                Mean return:              24.6562
                Policy entropy:           0.2085
                Pseudo loss:              2.85712
                Total gradient norm:      0.25298
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.633
                
Iteration (38600/50001) took 0.033 seconds.
                Mean final reward:        28.1250
                Mean return:              24.2188
                Policy entropy:           0.2099
                Pseudo loss:              2.69793
                Total gradient norm:      0.23906
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.906
                
Iteration (38700/50001) took 0.042 seconds.
                Mean final reward:        26.2500
                Mean return:              22.8438
                Policy entropy:           0.1789
                Pseudo loss:              1.78300
                Total gradient norm:      0.24477
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.406
                
Iteration (38800/50001) took 0.029 seconds.
                Mean final reward:        25.6250
                Mean return:              21.8750
                Policy entropy:           0.2359
                Pseudo loss:              2.86874
                Total gradient norm:      0.36769
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.516
                
Iteration (38900/50001) took 0.029 seconds.
                Mean final reward:        28.7500
                Mean return:              24.9375
                Policy entropy:           0.2108
                Pseudo loss:              2.22732
                Total gradient norm:      0.32275
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.581
                
Iteration (39000/50001) took 0.031 seconds.
                Mean final reward:        30.0000
                Mean return:              25.5625
                Policy entropy:           0.1888
                Pseudo loss:              2.53036
                Total gradient norm:      0.30573
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.226
                
Iteration (39100/50001) took 0.026 seconds.
                Mean final reward:        26.8750
                Mean return:              23.7500
                Policy entropy:           0.1823
                Pseudo loss:              2.49417
                Total gradient norm:      0.25214
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.125
                
Iteration (39200/50001) took 0.029 seconds.
                Mean final reward:        28.1250
                Mean return:              24.1562
                Policy entropy:           0.1586
                Pseudo loss:              1.02878
                Total gradient norm:      0.22886
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.969
                
Iteration (39300/50001) took 0.029 seconds.
                Mean final reward:        26.2500
                Mean return:              22.3438
                Policy entropy:           0.2009
                Pseudo loss:              1.54702
                Total gradient norm:      0.17628
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.906
                
Iteration (39400/50001) took 0.030 seconds.
                Mean final reward:        23.7500
                Mean return:              20.1875
                Policy entropy:           0.1948
                Pseudo loss:              1.92908
                Total gradient norm:      0.22893
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.562
                
Iteration (39500/50001) took 0.029 seconds.
                Mean final reward:        25.0000
                Mean return:              21.2188
                Policy entropy:           0.1897
                Pseudo loss:              2.14835
                Total gradient norm:      0.24765
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.781
                
Iteration (39600/50001) took 0.030 seconds.
                Mean final reward:        28.1250
                Mean return:              24.9062
                Policy entropy:           0.1895
                Pseudo loss:              0.88186
                Total gradient norm:      0.13608
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.219
                
Iteration (39700/50001) took 0.024 seconds.
                Mean final reward:        27.5000
                Mean return:              24.4688
                Policy entropy:           0.2047
                Pseudo loss:              1.69963
                Total gradient norm:      0.15662
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.031
                
Iteration (39800/50001) took 0.025 seconds.
                Mean final reward:        30.0000
                Mean return:              26.4375
                Policy entropy:           0.2432
                Pseudo loss:              2.04277
                Total gradient norm:      0.29824
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.323
                
Iteration (39900/50001) took 0.025 seconds.
                Mean final reward:        27.5000
                Mean return:              24.2188
                Policy entropy:           0.1915
                Pseudo loss:              1.44351
                Total gradient norm:      0.29142
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.281
                
Iteration (40000/50001) took 0.037 seconds.
                Mean final reward:        29.3750
                Mean return:              23.8750
                Policy entropy:           0.1653
                Pseudo loss:              2.77956
                Total gradient norm:      0.28231
                Solved trajectories:      32 / 32
                Avg steps to solve:       6.500
                
Iteration (40100/50001) took 0.028 seconds.
                Mean final reward:        28.7500
                Mean return:              24.7188
                Policy entropy:           0.2004
                Pseudo loss:              1.92261
                Total gradient norm:      0.25814
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.031
                
Iteration (40200/50001) took 0.024 seconds.
                Mean final reward:        26.2500
                Mean return:              22.9375
                Policy entropy:           0.1585
                Pseudo loss:              1.07655
                Total gradient norm:      0.22670
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.312
                
Iteration (40300/50001) took 0.020 seconds.
                Mean final reward:        26.2500
                Mean return:              23.6562
                Policy entropy:           0.2547
                Pseudo loss:              2.27891
                Total gradient norm:      0.34792
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.594
                
Iteration (40400/50001) took 0.028 seconds.
                Mean final reward:        28.7500
                Mean return:              24.5312
                Policy entropy:           0.2601
                Pseudo loss:              2.95058
                Total gradient norm:      0.28217
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.219
                
Iteration (40500/50001) took 0.032 seconds.
                Mean final reward:        27.1562
                Mean return:              22.9062
                Policy entropy:           0.1899
                Pseudo loss:              1.07448
                Total gradient norm:      0.27574
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.800
                
Iteration (40600/50001) took 0.028 seconds.
                Mean final reward:        26.2500
                Mean return:              22.1562
                Policy entropy:           0.1544
                Pseudo loss:              2.23752
                Total gradient norm:      0.28718
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.094
                
Iteration (40700/50001) took 0.023 seconds.
                Mean final reward:        27.5000
                Mean return:              24.4062
                Policy entropy:           0.1985
                Pseudo loss:              1.60035
                Total gradient norm:      0.20692
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.094
                
Iteration (40800/50001) took 0.028 seconds.
                Mean final reward:        28.7500
                Mean return:              25.0938
                Policy entropy:           0.1718
                Pseudo loss:              1.47050
                Total gradient norm:      0.15479
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.656
                
Iteration (40900/50001) took 0.026 seconds.
                Mean final reward:        28.1250
                Mean return:              24.2188
                Policy entropy:           0.2045
                Pseudo loss:              2.20360
                Total gradient norm:      0.27545
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.906
                
Iteration (41000/50001) took 0.026 seconds.
                Mean final reward:        28.1250
                Mean return:              24.8125
                Policy entropy:           0.1855
                Pseudo loss:              1.34024
                Total gradient norm:      0.26020
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.312
                
Iteration (41100/50001) took 0.021 seconds.
                Mean final reward:        26.8750
                Mean return:              24.2500
                Policy entropy:           0.2296
                Pseudo loss:              1.75916
                Total gradient norm:      0.31926
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.625
                
Iteration (41200/50001) took 0.022 seconds.
                Mean final reward:        23.1250
                Mean return:              20.3750
                Policy entropy:           0.1730
                Pseudo loss:              1.32817
                Total gradient norm:      0.14613
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.750
                
Iteration (41300/50001) took 0.027 seconds.
                Mean final reward:        26.2500
                Mean return:              22.9375
                Policy entropy:           0.1765
                Pseudo loss:              2.17984
                Total gradient norm:      0.32711
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.312
                
Iteration (41400/50001) took 0.026 seconds.
                Mean final reward:        23.7500
                Mean return:              20.5625
                Policy entropy:           0.1953
                Pseudo loss:              1.68968
                Total gradient norm:      0.24183
                Solved trajectories:      31 / 32
                Avg steps to solve:       3.935
                
Iteration (41500/50001) took 0.024 seconds.
                Mean final reward:        28.1250
                Mean return:              24.8125
                Policy entropy:           0.2245
                Pseudo loss:              1.97223
                Total gradient norm:      0.23097
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.312
                
Iteration (41600/50001) took 0.028 seconds.
                Mean final reward:        28.7500
                Mean return:              24.5938
                Policy entropy:           0.2160
                Pseudo loss:              2.09048
                Total gradient norm:      0.25469
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.156
                
Iteration (41700/50001) took 0.034 seconds.
                Mean final reward:        29.3750
                Mean return:              24.4062
                Policy entropy:           0.1824
                Pseudo loss:              1.77908
                Total gradient norm:      0.25395
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.969
                
Iteration (41800/50001) took 0.027 seconds.
                Mean final reward:        26.2500
                Mean return:              23.0312
                Policy entropy:           0.1767
                Pseudo loss:              1.20796
                Total gradient norm:      0.15673
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.219
                
Iteration (41900/50001) took 0.026 seconds.
                Mean final reward:        28.1250
                Mean return:              24.5625
                Policy entropy:           0.1651
                Pseudo loss:              1.51678
                Total gradient norm:      0.21220
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.562
                
Iteration (42000/50001) took 0.034 seconds.
                Mean final reward:        29.3750
                Mean return:              25.4375
                Policy entropy:           0.1285
                Pseudo loss:              1.70979
                Total gradient norm:      0.18349
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.938
                
Iteration (42100/50001) took 0.027 seconds.
                Mean final reward:        28.7500
                Mean return:              24.9688
                Policy entropy:           0.1457
                Pseudo loss:              0.75541
                Total gradient norm:      0.18503
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.781
                
Iteration (42200/50001) took 0.027 seconds.
                Mean final reward:        27.7812
                Mean return:              24.0625
                Policy entropy:           0.2160
                Pseudo loss:              2.08867
                Total gradient norm:      0.40544
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.484
                
Iteration (42300/50001) took 0.026 seconds.
                Mean final reward:        26.2500
                Mean return:              22.6875
                Policy entropy:           0.1462
                Pseudo loss:              1.26011
                Total gradient norm:      0.18180
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.562
                
Iteration (42400/50001) took 0.025 seconds.
                Mean final reward:        28.1250
                Mean return:              24.5938
                Policy entropy:           0.1004
                Pseudo loss:              1.77661
                Total gradient norm:      0.18202
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.531
                
Iteration (42500/50001) took 0.039 seconds.
                Mean final reward:        28.1250
                Mean return:              23.5625
                Policy entropy:           0.1380
                Pseudo loss:              1.53264
                Total gradient norm:      0.17680
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.355
                
Iteration (42600/50001) took 0.028 seconds.
                Mean final reward:        30.0000
                Mean return:              26.0938
                Policy entropy:           0.2087
                Pseudo loss:              2.29189
                Total gradient norm:      0.22206
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.906
                
Iteration (42700/50001) took 0.024 seconds.
                Mean final reward:        26.2500
                Mean return:              23.0000
                Policy entropy:           0.0930
                Pseudo loss:              0.86528
                Total gradient norm:      0.14852
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.000
                
Iteration (42800/50001) took 0.040 seconds.
                Mean final reward:        26.8750
                Mean return:              22.3125
                Policy entropy:           0.1707
                Pseudo loss:              1.80981
                Total gradient norm:      0.19919
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.355
                
Iteration (42900/50001) took 0.032 seconds.
                Mean final reward:        28.1250
                Mean return:              23.3750
                Policy entropy:           0.1806
                Pseudo loss:              2.00818
                Total gradient norm:      0.20617
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.750
                
Iteration (43000/50001) took 0.028 seconds.
                Mean final reward:        25.9062
                Mean return:              21.8750
                Policy entropy:           0.2281
                Pseudo loss:              2.62499
                Total gradient norm:      0.30098
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.806
                
Iteration (43100/50001) took 0.026 seconds.
                Mean final reward:        29.3750
                Mean return:              25.8125
                Policy entropy:           0.1539
                Pseudo loss:              1.87622
                Total gradient norm:      0.29191
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.323
                
Iteration (43200/50001) took 0.027 seconds.
                Mean final reward:        28.1250
                Mean return:              24.3750
                Policy entropy:           0.1725
                Pseudo loss:              1.57494
                Total gradient norm:      0.29294
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.750
                
Iteration (43300/50001) took 0.026 seconds.
                Mean final reward:        25.2812
                Mean return:              21.6875
                Policy entropy:           0.1572
                Pseudo loss:              0.55431
                Total gradient norm:      0.20746
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.355
                
Iteration (43400/50001) took 0.029 seconds.
                Mean final reward:        28.1250
                Mean return:              23.7500
                Policy entropy:           0.1598
                Pseudo loss:              1.78303
                Total gradient norm:      0.38011
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.375
                
Iteration (43500/50001) took 0.027 seconds.
                Mean final reward:        27.5000
                Mean return:              23.6562
                Policy entropy:           0.1923
                Pseudo loss:              1.57134
                Total gradient norm:      0.19804
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.613
                
Iteration (43600/50001) took 0.027 seconds.
                Mean final reward:        28.7500
                Mean return:              25.1875
                Policy entropy:           0.1896
                Pseudo loss:              2.21348
                Total gradient norm:      0.23837
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.323
                
Iteration (43700/50001) took 0.029 seconds.
                Mean final reward:        28.7500
                Mean return:              24.4688
                Policy entropy:           0.1423
                Pseudo loss:              1.61864
                Total gradient norm:      0.19819
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.281
                
Iteration (43800/50001) took 0.028 seconds.
                Mean final reward:        28.1250
                Mean return:              24.2812
                Policy entropy:           0.1514
                Pseudo loss:              1.17303
                Total gradient norm:      0.13036
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.844
                
Iteration (43900/50001) took 0.028 seconds.
                Mean final reward:        26.8750
                Mean return:              23.4375
                Policy entropy:           0.1654
                Pseudo loss:              1.98243
                Total gradient norm:      0.23604
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.194
                
Iteration (44000/50001) took 0.032 seconds.
                Mean final reward:        28.7500
                Mean return:              24.4062
                Policy entropy:           0.1007
                Pseudo loss:              0.95055
                Total gradient norm:      0.12420
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.129
                
Iteration (44100/50001) took 0.023 seconds.
                Mean final reward:        26.8750
                Mean return:              23.6562
                Policy entropy:           0.1118
                Pseudo loss:              1.45835
                Total gradient norm:      0.19947
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.219
                
Iteration (44200/50001) took 0.029 seconds.
                Mean final reward:        28.1250
                Mean return:              24.0625
                Policy entropy:           0.1491
                Pseudo loss:              1.11218
                Total gradient norm:      0.23088
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.062
                
Iteration (44300/50001) took 0.024 seconds.
                Mean final reward:        26.8750
                Mean return:              23.6875
                Policy entropy:           0.1483
                Pseudo loss:              1.77031
                Total gradient norm:      0.25059
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.188
                
Iteration (44400/50001) took 0.027 seconds.
                Mean final reward:        28.1250
                Mean return:              24.1875
                Policy entropy:           0.1662
                Pseudo loss:              1.86800
                Total gradient norm:      0.18391
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.710
                
Iteration (44500/50001) took 0.022 seconds.
                Mean final reward:        28.7500
                Mean return:              25.9688
                Policy entropy:           0.2071
                Pseudo loss:              1.94040
                Total gradient norm:      0.21253
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.781
                
Iteration (44600/50001) took 0.030 seconds.
                Mean final reward:        28.1250
                Mean return:              23.8125
                Policy entropy:           0.1733
                Pseudo loss:              1.88943
                Total gradient norm:      0.20592
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.312
                
Iteration (44700/50001) took 0.028 seconds.
                Mean final reward:        27.5000
                Mean return:              23.5312
                Policy entropy:           0.1566
                Pseudo loss:              1.54187
                Total gradient norm:      0.17510
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.742
                
Iteration (44800/50001) took 0.027 seconds.
                Mean final reward:        27.5000
                Mean return:              23.6875
                Policy entropy:           0.1784
                Pseudo loss:              1.96701
                Total gradient norm:      0.24428
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.812
                
Iteration (44900/50001) took 0.034 seconds.
                Mean final reward:        28.1250
                Mean return:              23.0938
                Policy entropy:           0.1407
                Pseudo loss:              1.94443
                Total gradient norm:      0.19599
                Solved trajectories:      32 / 32
                Avg steps to solve:       6.031
                
Iteration (45000/50001) took 0.028 seconds.
                Mean final reward:        26.2500
                Mean return:              22.0625
                Policy entropy:           0.1339
                Pseudo loss:              1.08029
                Total gradient norm:      0.13883
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.968
                
Iteration (45100/50001) took 0.025 seconds.
                Mean final reward:        27.5000
                Mean return:              24.0938
                Policy entropy:           0.1860
                Pseudo loss:              1.83628
                Total gradient norm:      0.25341
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.161
                
Iteration (45200/50001) took 0.026 seconds.
                Mean final reward:        28.7500
                Mean return:              25.0000
                Policy entropy:           0.1473
                Pseudo loss:              1.51058
                Total gradient norm:      0.16717
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.750
                
Iteration (45300/50001) took 0.026 seconds.
                Mean final reward:        29.3750
                Mean return:              25.7500
                Policy entropy:           0.2273
                Pseudo loss:              1.93470
                Total gradient norm:      0.17202
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.625
                
Iteration (45400/50001) took 0.021 seconds.
                Mean final reward:        26.8750
                Mean return:              24.0938
                Policy entropy:           0.1591
                Pseudo loss:              1.82002
                Total gradient norm:      0.20499
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.781
                
Iteration (45500/50001) took 0.025 seconds.
                Mean final reward:        28.1250
                Mean return:              24.8125
                Policy entropy:           0.1762
                Pseudo loss:              2.09153
                Total gradient norm:      0.24860
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.312
                
Iteration (45600/50001) took 0.030 seconds.
                Mean final reward:        26.2500
                Mean return:              22.9062
                Policy entropy:           0.1409
                Pseudo loss:              2.25961
                Total gradient norm:      0.27926
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.344
                
Iteration (45700/50001) took 0.028 seconds.
                Mean final reward:        26.2500
                Mean return:              22.1562
                Policy entropy:           0.2190
                Pseudo loss:              2.54476
                Total gradient norm:      0.29610
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.094
                
Iteration (45800/50001) took 0.029 seconds.
                Mean final reward:        28.1250
                Mean return:              23.9688
                Policy entropy:           0.1564
                Pseudo loss:              1.12876
                Total gradient norm:      0.18185
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.156
                
Iteration (45900/50001) took 0.028 seconds.
                Mean final reward:        28.1250
                Mean return:              24.0625
                Policy entropy:           0.1608
                Pseudo loss:              1.74313
                Total gradient norm:      0.21987
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.062
                
Iteration (46000/50001) took 0.029 seconds.
                Mean final reward:        26.2500
                Mean return:              22.0312
                Policy entropy:           0.1199
                Pseudo loss:              1.34982
                Total gradient norm:      0.18628
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.219
                
Iteration (46100/50001) took 0.029 seconds.
                Mean final reward:        29.3750
                Mean return:              25.1562
                Policy entropy:           0.1582
                Pseudo loss:              2.03293
                Total gradient norm:      0.18600
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.000
                
Iteration (46200/50001) took 0.030 seconds.
                Mean final reward:        28.1250
                Mean return:              23.5312
                Policy entropy:           0.1607
                Pseudo loss:              2.66537
                Total gradient norm:      0.25960
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.594
                
Iteration (46300/50001) took 0.033 seconds.
                Mean final reward:        26.2500
                Mean return:              21.3750
                Policy entropy:           0.1117
                Pseudo loss:              1.52612
                Total gradient norm:      0.19774
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.875
                
Iteration (46400/50001) took 0.026 seconds.
                Mean final reward:        28.7500
                Mean return:              25.0625
                Policy entropy:           0.1126
                Pseudo loss:              1.89306
                Total gradient norm:      0.24082
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.452
                
Iteration (46500/50001) took 0.031 seconds.
                Mean final reward:        29.3750
                Mean return:              24.8750
                Policy entropy:           0.2195
                Pseudo loss:              4.55554
                Total gradient norm:      0.36008
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.500
                
Iteration (46600/50001) took 0.027 seconds.
                Mean final reward:        28.7500
                Mean return:              24.9688
                Policy entropy:           0.1402
                Pseudo loss:              1.27243
                Total gradient norm:      0.17994
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.781
                
Iteration (46700/50001) took 0.025 seconds.
                Mean final reward:        28.7500
                Mean return:              25.2188
                Policy entropy:           0.1621
                Pseudo loss:              1.61581
                Total gradient norm:      0.18202
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.531
                
Iteration (46800/50001) took 0.028 seconds.
                Mean final reward:        27.5000
                Mean return:              23.7812
                Policy entropy:           0.1640
                Pseudo loss:              2.12815
                Total gradient norm:      0.25045
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.484
                
Iteration (46900/50001) took 0.025 seconds.
                Mean final reward:        26.8750
                Mean return:              23.2812
                Policy entropy:           0.1911
                Pseudo loss:              2.28934
                Total gradient norm:      0.39894
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.594
                
Iteration (47000/50001) took 0.027 seconds.
                Mean final reward:        27.5000
                Mean return:              23.4688
                Policy entropy:           0.1215
                Pseudo loss:              0.56048
                Total gradient norm:      0.16753
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.031
                
Iteration (47100/50001) took 0.027 seconds.
                Mean final reward:        28.7500
                Mean return:              24.8125
                Policy entropy:           0.1718
                Pseudo loss:              1.73644
                Total gradient norm:      0.18899
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.938
                
Iteration (47200/50001) took 0.021 seconds.
                Mean final reward:        24.3750
                Mean return:              21.6250
                Policy entropy:           0.1405
                Pseudo loss:              0.92994
                Total gradient norm:      0.21549
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.750
                
Iteration (47300/50001) took 0.027 seconds.
                Mean final reward:        28.1250
                Mean return:              24.3750
                Policy entropy:           0.1264
                Pseudo loss:              1.26661
                Total gradient norm:      0.24023
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.750
                
Iteration (47400/50001) took 0.027 seconds.
                Mean final reward:        26.2500
                Mean return:              22.6562
                Policy entropy:           0.1468
                Pseudo loss:              0.96101
                Total gradient norm:      0.16510
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.594
                
Iteration (47500/50001) took 0.037 seconds.
                Mean final reward:        27.5000
                Mean return:              23.5312
                Policy entropy:           0.1690
                Pseudo loss:              1.15903
                Total gradient norm:      0.09600
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.969
                
Iteration (47600/50001) took 0.025 seconds.
                Mean final reward:        27.5000
                Mean return:              24.0625
                Policy entropy:           0.2164
                Pseudo loss:              1.43338
                Total gradient norm:      0.31180
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.438
                
Iteration (47700/50001) took 0.029 seconds.
                Mean final reward:        27.5000
                Mean return:              23.1562
                Policy entropy:           0.1486
                Pseudo loss:              1.41151
                Total gradient norm:      0.21902
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.344
                
Iteration (47800/50001) took 0.031 seconds.
                Mean final reward:        28.1250
                Mean return:              23.6562
                Policy entropy:           0.1114
                Pseudo loss:              0.81228
                Total gradient norm:      0.20310
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.258
                
Iteration (47900/50001) took 0.027 seconds.
                Mean final reward:        27.5000
                Mean return:              23.6562
                Policy entropy:           0.1889
                Pseudo loss:              1.78672
                Total gradient norm:      0.21229
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.844
                
Iteration (48000/50001) took 0.029 seconds.
                Mean final reward:        28.1250
                Mean return:              23.8750
                Policy entropy:           0.1342
                Pseudo loss:              1.03749
                Total gradient norm:      0.12707
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.250
                
Iteration (48100/50001) took 0.029 seconds.
                Mean final reward:        29.3750
                Mean return:              25.0625
                Policy entropy:           0.1496
                Pseudo loss:              0.94808
                Total gradient norm:      0.32764
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.312
                
Iteration (48200/50001) took 0.023 seconds.
                Mean final reward:        27.5000
                Mean return:              24.3750
                Policy entropy:           0.1550
                Pseudo loss:              1.35994
                Total gradient norm:      0.19481
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.125
                
Iteration (48300/50001) took 0.029 seconds.
                Mean final reward:        28.1250
                Mean return:              24.0000
                Policy entropy:           0.1107
                Pseudo loss:              1.03013
                Total gradient norm:      0.14408
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.903
                
Iteration (48400/50001) took 0.028 seconds.
                Mean final reward:        26.2500
                Mean return:              22.2500
                Policy entropy:           0.0876
                Pseudo loss:              0.31703
                Total gradient norm:      0.07130
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.000
                
Iteration (48500/50001) took 0.021 seconds.
                Mean final reward:        25.6250
                Mean return:              22.9688
                Policy entropy:           0.2117
                Pseudo loss:              1.09192
                Total gradient norm:      0.11093
                Solved trajectories:      32 / 32
                Avg steps to solve:       3.656
                
Iteration (48600/50001) took 0.030 seconds.
                Mean final reward:        27.7812
                Mean return:              23.7188
                Policy entropy:           0.1230
                Pseudo loss:              0.62393
                Total gradient norm:      0.12407
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.839
                
Iteration (48700/50001) took 0.027 seconds.
                Mean final reward:        27.5000
                Mean return:              23.8125
                Policy entropy:           0.1624
                Pseudo loss:              1.70690
                Total gradient norm:      0.19904
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.688
                
Iteration (48800/50001) took 0.025 seconds.
                Mean final reward:        27.5000
                Mean return:              24.1250
                Policy entropy:           0.1839
                Pseudo loss:              2.25898
                Total gradient norm:      0.23974
                Solved trajectories:      30 / 32
                Avg steps to solve:       3.867
                
Iteration (48900/50001) took 0.031 seconds.
                Mean final reward:        28.7500
                Mean return:              24.1562
                Policy entropy:           0.1107
                Pseudo loss:              1.25788
                Total gradient norm:      0.16395
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.387
                
Iteration (49000/50001) took 0.027 seconds.
                Mean final reward:        26.2500
                Mean return:              22.2812
                Policy entropy:           0.1210
                Pseudo loss:              1.08858
                Total gradient norm:      0.19190
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.969
                
Iteration (49100/50001) took 0.029 seconds.
                Mean final reward:        30.0000
                Mean return:              25.7812
                Policy entropy:           0.1694
                Pseudo loss:              2.70421
                Total gradient norm:      0.26905
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.219
                
Iteration (49200/50001) took 0.026 seconds.
                Mean final reward:        29.3750
                Mean return:              25.6875
                Policy entropy:           0.1351
                Pseudo loss:              1.27684
                Total gradient norm:      0.33823
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.688
                
Iteration (49300/50001) took 0.027 seconds.
                Mean final reward:        27.1562
                Mean return:              23.0938
                Policy entropy:           0.1559
                Pseudo loss:              0.99670
                Total gradient norm:      0.16859
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.839
                
Iteration (49400/50001) took 0.027 seconds.
                Mean final reward:        26.8750
                Mean return:              23.1562
                Policy entropy:           0.1572
                Pseudo loss:              1.03478
                Total gradient norm:      0.16625
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.719
                
Iteration (49500/50001) took 0.029 seconds.
                Mean final reward:        28.7500
                Mean return:              24.4375
                Policy entropy:           0.1404
                Pseudo loss:              1.52400
                Total gradient norm:      0.16653
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.312
                
Iteration (49600/50001) took 0.031 seconds.
                Mean final reward:        28.7500
                Mean return:              24.4688
                Policy entropy:           0.1596
                Pseudo loss:              2.60048
                Total gradient norm:      0.32147
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.281
                
Iteration (49700/50001) took 0.032 seconds.
                Mean final reward:        28.1250
                Mean return:              23.3125
                Policy entropy:           0.1398
                Pseudo loss:              1.35399
                Total gradient norm:      0.29546
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.812
                
Iteration (49800/50001) took 0.028 seconds.
                Mean final reward:        27.5000
                Mean return:              23.5000
                Policy entropy:           0.1055
                Pseudo loss:              1.87097
                Total gradient norm:      0.25331
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.000
                
Iteration (49900/50001) took 0.025 seconds.
                Mean final reward:        29.3750
                Mean return:              25.8438
                Policy entropy:           0.1196
                Pseudo loss:              0.87878
                Total gradient norm:      0.20449
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.531
                
Iteration (50000/50001) took 0.026 seconds.
                Mean final reward:        25.6250
                Mean return:              22.2188
                Policy entropy:           0.1209
                Pseudo loss:              1.98849
                Total gradient norm:      0.20444
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.406
                
Training took 1545.286 seconds.
