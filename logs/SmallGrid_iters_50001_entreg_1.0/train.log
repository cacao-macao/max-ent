Using device: cpu

Using optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 0.0
)

Iteration (0/50001) took 0.092 seconds.
                Mean final reward:        -0.3125
                Mean return:              -9.0625
                Policy entropy:           1.5783
                Pseudo loss:              4.53004
                Total gradient norm:      0.81757
                Solved trajectories:      1 / 32
                Avg steps to solve:       2.000
                
Iteration (100/50001) took 0.117 seconds.
                Mean final reward:        0.7188
                Mean return:              -7.2188
                Policy entropy:           1.5795
                Pseudo loss:              4.27038
                Total gradient norm:      0.65583
                Solved trajectories:      5 / 32
                Avg steps to solve:       3.200
                
Iteration (200/50001) took 0.164 seconds.
                Mean final reward:        0.7188
                Mean return:              -7.6875
                Policy entropy:           1.5725
                Pseudo loss:              4.98003
                Total gradient norm:      0.80922
                Solved trajectories:      5 / 32
                Avg steps to solve:       6.200
                
Iteration (300/50001) took 0.124 seconds.
                Mean final reward:        2.0938
                Mean return:              -5.0938
                Policy entropy:           1.5704
                Pseudo loss:              4.57972
                Total gradient norm:      0.79786
                Solved trajectories:      9 / 32
                Avg steps to solve:       3.556
                
Iteration (400/50001) took 0.144 seconds.
                Mean final reward:        1.7500
                Mean return:              -6.3438
                Policy entropy:           1.5641
                Pseudo loss:              5.49213
                Total gradient norm:      0.80670
                Solved trajectories:      7 / 32
                Avg steps to solve:       5.857
                
Iteration (500/50001) took 0.188 seconds.
                Mean final reward:        1.0625
                Mean return:              -7.2500
                Policy entropy:           1.5588
                Pseudo loss:              4.82255
                Total gradient norm:      0.62541
                Solved trajectories:      5 / 32
                Avg steps to solve:       5.600
                
Iteration (600/50001) took 0.135 seconds.
                Mean final reward:        1.0625
                Mean return:              -6.7500
                Policy entropy:           1.5500
                Pseudo loss:              3.57095
                Total gradient norm:      0.59493
                Solved trajectories:      6 / 32
                Avg steps to solve:       3.667
                
Iteration (700/50001) took 0.203 seconds.
                Mean final reward:        1.0625
                Mean return:              -6.5938
                Policy entropy:           1.5413
                Pseudo loss:              3.15294
                Total gradient norm:      0.50792
                Solved trajectories:      6 / 32
                Avg steps to solve:       2.833
                
Iteration (800/50001) took 0.202 seconds.
                Mean final reward:        2.4375
                Mean return:              -5.0625
                Policy entropy:           1.5398
                Pseudo loss:              4.84592
                Total gradient norm:      0.72423
                Solved trajectories:      10 / 32
                Avg steps to solve:       5.200
                
Iteration (900/50001) took 0.141 seconds.
                Mean final reward:        1.0625
                Mean return:              -7.0938
                Policy entropy:           1.5239
                Pseudo loss:              3.82256
                Total gradient norm:      0.55915
                Solved trajectories:      4 / 32
                Avg steps to solve:       3.250
                
Iteration (1000/50001) took 0.161 seconds.
                Mean final reward:        2.0938
                Mean return:              -5.7812
                Policy entropy:           1.5189
                Pseudo loss:              4.55638
                Total gradient norm:      0.59246
                Solved trajectories:      9 / 32
                Avg steps to solve:       6.000
                
Iteration (1100/50001) took 0.193 seconds.
                Mean final reward:        2.4375
                Mean return:              -5.2500
                Policy entropy:           1.5074
                Pseudo loss:              4.69671
                Total gradient norm:      0.63468
                Solved trajectories:      7 / 32
                Avg steps to solve:       4.000
                
Iteration (1200/50001) took 0.193 seconds.
                Mean final reward:        1.4062
                Mean return:              -6.3125
                Policy entropy:           1.5006
                Pseudo loss:              3.25064
                Total gradient norm:      0.51008
                Solved trajectories:      7 / 32
                Avg steps to solve:       4.143
                
Iteration (1300/50001) took 0.182 seconds.
                Mean final reward:        2.7812
                Mean return:              -4.4688
                Policy entropy:           1.4878
                Pseudo loss:              4.00295
                Total gradient norm:      0.52696
                Solved trajectories:      10 / 32
                Avg steps to solve:       4.400
                
Iteration (1400/50001) took 0.089 seconds.
                Mean final reward:        2.4375
                Mean return:              -5.3125
                Policy entropy:           1.4755
                Pseudo loss:              4.28674
                Total gradient norm:      0.44857
                Solved trajectories:      10 / 32
                Avg steps to solve:       6.000
                
Iteration (1500/50001) took 0.180 seconds.
                Mean final reward:        2.4375
                Mean return:              -5.0625
                Policy entropy:           1.4669
                Pseudo loss:              3.90910
                Total gradient norm:      0.49510
                Solved trajectories:      10 / 32
                Avg steps to solve:       5.200
                
Iteration (1600/50001) took 0.172 seconds.
                Mean final reward:        3.1250
                Mean return:              -4.2500
                Policy entropy:           1.4451
                Pseudo loss:              4.29660
                Total gradient norm:      0.55460
                Solved trajectories:      10 / 32
                Avg steps to solve:       4.800
                
Iteration (1700/50001) took 0.217 seconds.
                Mean final reward:        2.7812
                Mean return:              -4.7500
                Policy entropy:           1.4438
                Pseudo loss:              4.28585
                Total gradient norm:      0.50873
                Solved trajectories:      11 / 32
                Avg steps to solve:       5.727
                
Iteration (1800/50001) took 0.127 seconds.
                Mean final reward:        1.7500
                Mean return:              -5.6875
                Policy entropy:           1.4351
                Pseudo loss:              2.89187
                Total gradient norm:      0.40431
                Solved trajectories:      8 / 32
                Avg steps to solve:       3.750
                
Iteration (1900/50001) took 0.223 seconds.
                Mean final reward:        3.1250
                Mean return:              -3.9062
                Policy entropy:           1.4207
                Pseudo loss:              3.22781
                Total gradient norm:      0.45986
                Solved trajectories:      11 / 32
                Avg steps to solve:       4.273
                
Iteration (2000/50001) took 0.123 seconds.
                Mean final reward:        3.4688
                Mean return:              -3.5312
                Policy entropy:           1.4017
                Pseudo loss:              3.57539
                Total gradient norm:      0.42755
                Solved trajectories:      12 / 32
                Avg steps to solve:       4.667
                
Iteration (2100/50001) took 0.220 seconds.
                Mean final reward:        3.8125
                Mean return:              -2.7500
                Policy entropy:           1.3823
                Pseudo loss:              3.62360
                Total gradient norm:      0.47043
                Solved trajectories:      14 / 32
                Avg steps to solve:       4.429
                
Iteration (2200/50001) took 0.289 seconds.
                Mean final reward:        4.1562
                Mean return:              -2.8125
                Policy entropy:           1.3591
                Pseudo loss:              4.12937
                Total gradient norm:      0.51079
                Solved trajectories:      15 / 32
                Avg steps to solve:       5.667
                
Iteration (2300/50001) took 0.135 seconds.
                Mean final reward:        3.8125
                Mean return:              -3.0938
                Policy entropy:           1.3575
                Pseudo loss:              3.69431
                Total gradient norm:      0.43874
                Solved trajectories:      14 / 32
                Avg steps to solve:       5.214
                
Iteration (2400/50001) took 0.164 seconds.
                Mean final reward:        4.1562
                Mean return:              -2.3125
                Policy entropy:           1.3478
                Pseudo loss:              3.59981
                Total gradient norm:      0.41076
                Solved trajectories:      15 / 32
                Avg steps to solve:       4.600
                
Iteration (2500/50001) took 0.204 seconds.
                Mean final reward:        4.1562
                Mean return:              -2.6562
                Policy entropy:           1.3154
                Pseudo loss:              3.99819
                Total gradient norm:      0.43828
                Solved trajectories:      14 / 32
                Avg steps to solve:       5.000
                
Iteration (2600/50001) took 0.163 seconds.
                Mean final reward:        5.1875
                Mean return:              -1.1562
                Policy entropy:           1.3050
                Pseudo loss:              3.84560
                Total gradient norm:      0.43733
                Solved trajectories:      16 / 32
                Avg steps to solve:       4.688
                
Iteration (2700/50001) took 0.254 seconds.
                Mean final reward:        4.5000
                Mean return:              -2.6875
                Policy entropy:           1.3212
                Pseudo loss:              4.30822
                Total gradient norm:      0.47726
                Solved trajectories:      14 / 32
                Avg steps to solve:       5.857
                
Iteration (2800/50001) took 0.156 seconds.
                Mean final reward:        4.8438
                Mean return:              -1.7188
                Policy entropy:           1.2780
                Pseudo loss:              4.40070
                Total gradient norm:      0.47322
                Solved trajectories:      16 / 32
                Avg steps to solve:       5.125
                
Iteration (2900/50001) took 0.185 seconds.
                Mean final reward:        3.4688
                Mean return:              -3.2500
                Policy entropy:           1.3114
                Pseudo loss:              2.67289
                Total gradient norm:      0.37480
                Solved trajectories:      13 / 32
                Avg steps to solve:       4.385
                
Iteration (3000/50001) took 0.170 seconds.
                Mean final reward:        5.8750
                Mean return:              0.0938
                Policy entropy:           1.2390
                Pseudo loss:              3.63313
                Total gradient norm:      0.41513
                Solved trajectories:      20 / 32
                Avg steps to solve:       4.850
                
Iteration (3100/50001) took 0.182 seconds.
                Mean final reward:        6.2188
                Mean return:              -0.1875
                Policy entropy:           1.2148
                Pseudo loss:              4.46448
                Total gradient norm:      0.43690
                Solved trajectories:      15 / 32
                Avg steps to solve:       4.467
                
Iteration (3200/50001) took 0.103 seconds.
                Mean final reward:        3.8125
                Mean return:              -3.8750
                Policy entropy:           1.2769
                Pseudo loss:              4.41518
                Total gradient norm:      0.47833
                Solved trajectories:      13 / 32
                Avg steps to solve:       6.769
                
Iteration (3300/50001) took 0.110 seconds.
                Mean final reward:        6.9062
                Mean return:              1.4375
                Policy entropy:           1.1803
                Pseudo loss:              3.90288
                Total gradient norm:      0.43608
                Solved trajectories:      21 / 32
                Avg steps to solve:       4.619
                
Iteration (3400/50001) took 0.127 seconds.
                Mean final reward:        5.8750
                Mean return:              -0.5938
                Policy entropy:           1.2172
                Pseudo loss:              4.43170
                Total gradient norm:      0.42146
                Solved trajectories:      19 / 32
                Avg steps to solve:       5.737
                
Iteration (3500/50001) took 0.110 seconds.
                Mean final reward:        5.5312
                Mean return:              -0.7812
                Policy entropy:           1.1850
                Pseudo loss:              3.65948
                Total gradient norm:      0.38103
                Solved trajectories:      17 / 32
                Avg steps to solve:       4.941
                
Iteration (3600/50001) took 0.163 seconds.
                Mean final reward:        6.5625
                Mean return:              1.3125
                Policy entropy:           1.1333
                Pseudo loss:              3.20694
                Total gradient norm:      0.38948
                Solved trajectories:      21 / 32
                Avg steps to solve:       4.286
                
Iteration (3700/50001) took 0.157 seconds.
                Mean final reward:        7.5938
                Mean return:              3.0938
                Policy entropy:           1.0776
                Pseudo loss:              2.91753
                Total gradient norm:      0.34425
                Solved trajectories:      24 / 32
                Avg steps to solve:       4.000
                
Iteration (3800/50001) took 0.204 seconds.
                Mean final reward:        6.9062
                Mean return:              1.0625
                Policy entropy:           1.1549
                Pseudo loss:              4.45025
                Total gradient norm:      0.40693
                Solved trajectories:      19 / 32
                Avg steps to solve:       4.684
                
Iteration (3900/50001) took 0.079 seconds.
                Mean final reward:        6.5625
                Mean return:              0.7812
                Policy entropy:           1.1214
                Pseudo loss:              2.97936
                Total gradient norm:      0.40017
                Solved trajectories:      20 / 32
                Avg steps to solve:       4.850
                
Iteration (4000/50001) took 0.178 seconds.
                Mean final reward:        6.9062
                Mean return:              1.2812
                Policy entropy:           1.1086
                Pseudo loss:              4.34857
                Total gradient norm:      0.41023
                Solved trajectories:      21 / 32
                Avg steps to solve:       4.857
                
Iteration (4100/50001) took 0.144 seconds.
                Mean final reward:        5.8750
                Mean return:              -0.2188
                Policy entropy:           1.1292
                Pseudo loss:              3.66102
                Total gradient norm:      0.44851
                Solved trajectories:      20 / 32
                Avg steps to solve:       5.350
                
Iteration (4200/50001) took 0.181 seconds.
                Mean final reward:        4.1562
                Mean return:              -2.9062
                Policy entropy:           1.2080
                Pseudo loss:              3.87890
                Total gradient norm:      0.41698
                Solved trajectories:      15 / 32
                Avg steps to solve:       5.867
                
Iteration (4300/50001) took 0.178 seconds.
                Mean final reward:        5.5312
                Mean return:              -0.9688
                Policy entropy:           1.1733
                Pseudo loss:              3.17191
                Total gradient norm:      0.39268
                Solved trajectories:      15 / 32
                Avg steps to solve:       4.667
                
Iteration (4400/50001) took 0.175 seconds.
                Mean final reward:        5.5312
                Mean return:              -0.8438
                Policy entropy:           1.1491
                Pseudo loss:              3.98856
                Total gradient norm:      0.43569
                Solved trajectories:      19 / 32
                Avg steps to solve:       5.579
                
Iteration (4500/50001) took 0.157 seconds.
                Mean final reward:        7.5938
                Mean return:              2.0938
                Policy entropy:           1.0201
                Pseudo loss:              3.93752
                Total gradient norm:      0.32482
                Solved trajectories:      23 / 32
                Avg steps to solve:       5.130
                
Iteration (4600/50001) took 0.131 seconds.
                Mean final reward:        6.5625
                Mean return:              0.5625
                Policy entropy:           1.0874
                Pseudo loss:              3.83323
                Total gradient norm:      0.38896
                Solved trajectories:      21 / 32
                Avg steps to solve:       5.429
                
Iteration (4700/50001) took 0.241 seconds.
                Mean final reward:        6.2188
                Mean return:              0.1562
                Policy entropy:           1.0784
                Pseudo loss:              3.41718
                Total gradient norm:      0.35997
                Solved trajectories:      20 / 32
                Avg steps to solve:       5.300
                
Iteration (4800/50001) took 0.099 seconds.
                Mean final reward:        6.9062
                Mean return:              1.2188
                Policy entropy:           1.0612
                Pseudo loss:              3.78433
                Total gradient norm:      0.31276
                Solved trajectories:      21 / 32
                Avg steps to solve:       4.952
                
Iteration (4900/50001) took 0.143 seconds.
                Mean final reward:        8.6250
                Mean return:              3.8438
                Policy entropy:           0.9345
                Pseudo loss:              4.11149
                Total gradient norm:      0.34356
                Solved trajectories:      28 / 32
                Avg steps to solve:       5.179
                
Iteration (5000/50001) took 0.155 seconds.
                Mean final reward:        6.5625
                Mean return:              0.1250
                Policy entropy:           1.0839
                Pseudo loss:              4.12296
                Total gradient norm:      0.38662
                Solved trajectories:      20 / 32
                Avg steps to solve:       5.900
                
Iteration (5100/50001) took 0.156 seconds.
                Mean final reward:        6.5625
                Mean return:              0.7188
                Policy entropy:           1.0145
                Pseudo loss:              3.48407
                Total gradient norm:      0.39136
                Solved trajectories:      21 / 32
                Avg steps to solve:       5.190
                
Iteration (5200/50001) took 0.077 seconds.
                Mean final reward:        6.2188
                Mean return:              -0.2500
                Policy entropy:           1.1028
                Pseudo loss:              3.43673
                Total gradient norm:      0.44528
                Solved trajectories:      18 / 32
                Avg steps to solve:       5.500
                
Iteration (5300/50001) took 0.125 seconds.
                Mean final reward:        6.9062
                Mean return:              1.0000
                Policy entropy:           1.0372
                Pseudo loss:              3.50475
                Total gradient norm:      0.38226
                Solved trajectories:      23 / 32
                Avg steps to solve:       5.696
                
Iteration (5400/50001) took 0.136 seconds.
                Mean final reward:        8.6250
                Mean return:              3.6562
                Policy entropy:           0.9043
                Pseudo loss:              3.60191
                Total gradient norm:      0.37916
                Solved trajectories:      27 / 32
                Avg steps to solve:       5.222
                
Iteration (5500/50001) took 0.139 seconds.
                Mean final reward:        5.8750
                Mean return:              -0.5938
                Policy entropy:           1.0525
                Pseudo loss:              3.88298
                Total gradient norm:      0.39262
                Solved trajectories:      19 / 32
                Avg steps to solve:       5.737
                
Iteration (5600/50001) took 0.169 seconds.
                Mean final reward:        6.9062
                Mean return:              1.5938
                Policy entropy:           0.9923
                Pseudo loss:              2.79749
                Total gradient norm:      0.41166
                Solved trajectories:      22 / 32
                Avg steps to solve:       4.636
                
Iteration (5700/50001) took 0.160 seconds.
                Mean final reward:        7.2500
                Mean return:              1.7812
                Policy entropy:           0.9847
                Pseudo loss:              2.94408
                Total gradient norm:      0.31848
                Solved trajectories:      22 / 32
                Avg steps to solve:       4.864
                
Iteration (5800/50001) took 0.087 seconds.
                Mean final reward:        7.9375
                Mean return:              2.0938
                Policy entropy:           0.9351
                Pseudo loss:              4.43900
                Total gradient norm:      0.38725
                Solved trajectories:      23 / 32
                Avg steps to solve:       5.609
                
Iteration (5900/50001) took 0.136 seconds.
                Mean final reward:        9.3125
                Mean return:              4.4062
                Policy entropy:           0.8465
                Pseudo loss:              4.35280
                Total gradient norm:      0.37110
                Solved trajectories:      25 / 32
                Avg steps to solve:       4.760
                
Iteration (6000/50001) took 0.157 seconds.
                Mean final reward:        6.9062
                Mean return:              0.5000
                Policy entropy:           0.9775
                Pseudo loss:              4.83761
                Total gradient norm:      0.41230
                Solved trajectories:      22 / 32
                Avg steps to solve:       6.227
                
Iteration (6100/50001) took 0.203 seconds.
                Mean final reward:        8.2812
                Mean return:              3.1250
                Policy entropy:           0.8983
                Pseudo loss:              3.68486
                Total gradient norm:      0.40492
                Solved trajectories:      26 / 32
                Avg steps to solve:       5.269
                
Iteration (6200/50001) took 0.064 seconds.
                Mean final reward:        7.5938
                Mean return:              2.4688
                Policy entropy:           0.9017
                Pseudo loss:              3.05839
                Total gradient norm:      0.31539
                Solved trajectories:      24 / 32
                Avg steps to solve:       4.833
                
Iteration (6300/50001) took 0.122 seconds.
                Mean final reward:        7.9375
                Mean return:              3.0312
                Policy entropy:           0.8928
                Pseudo loss:              3.27074
                Total gradient norm:      0.37047
                Solved trajectories:      26 / 32
                Avg steps to solve:       4.962
                
Iteration (6400/50001) took 0.073 seconds.
                Mean final reward:        6.5625
                Mean return:              0.3125
                Policy entropy:           1.0455
                Pseudo loss:              3.49814
                Total gradient norm:      0.43270
                Solved trajectories:      19 / 32
                Avg steps to solve:       5.368
                
Iteration (6500/50001) took 0.166 seconds.
                Mean final reward:        7.5938
                Mean return:              1.9375
                Policy entropy:           0.9403
                Pseudo loss:              3.59627
                Total gradient norm:      0.31224
                Solved trajectories:      25 / 32
                Avg steps to solve:       5.720
                
Iteration (6600/50001) took 0.161 seconds.
                Mean final reward:        7.5938
                Mean return:              2.4062
                Policy entropy:           0.9124
                Pseudo loss:              3.66212
                Total gradient norm:      0.34656
                Solved trajectories:      25 / 32
                Avg steps to solve:       5.120
                
Iteration (6700/50001) took 0.154 seconds.
                Mean final reward:        8.6250
                Mean return:              3.6250
                Policy entropy:           0.8624
                Pseudo loss:              3.60050
                Total gradient norm:      0.35298
                Solved trajectories:      25 / 32
                Avg steps to solve:       4.880
                
Iteration (6800/50001) took 0.079 seconds.
                Mean final reward:        7.9375
                Mean return:              2.5312
                Policy entropy:           0.8479
                Pseudo loss:              3.46634
                Total gradient norm:      0.33643
                Solved trajectories:      26 / 32
                Avg steps to solve:       5.577
                
Iteration (6900/50001) took 0.170 seconds.
                Mean final reward:        6.9062
                Mean return:              0.8750
                Policy entropy:           0.9164
                Pseudo loss:              3.58323
                Total gradient norm:      0.37458
                Solved trajectories:      22 / 32
                Avg steps to solve:       5.682
                
Iteration (7000/50001) took 0.114 seconds.
                Mean final reward:        8.9688
                Mean return:              4.3750
                Policy entropy:           0.7896
                Pseudo loss:              3.31720
                Total gradient norm:      0.29213
                Solved trajectories:      29 / 32
                Avg steps to solve:       5.138
                
Iteration (7100/50001) took 0.189 seconds.
                Mean final reward:        6.9062
                Mean return:              1.1250
                Policy entropy:           0.9124
                Pseudo loss:              3.18344
                Total gradient norm:      0.38212
                Solved trajectories:      21 / 32
                Avg steps to solve:       5.095
                
Iteration (7200/50001) took 0.161 seconds.
                Mean final reward:        8.2812
                Mean return:              2.9062
                Policy entropy:           0.8973
                Pseudo loss:              3.61490
                Total gradient norm:      0.33671
                Solved trajectories:      27 / 32
                Avg steps to solve:       5.704
                
Iteration (7300/50001) took 0.121 seconds.
                Mean final reward:        7.2500
                Mean return:              1.5312
                Policy entropy:           0.8807
                Pseudo loss:              3.25655
                Total gradient norm:      0.40901
                Solved trajectories:      21 / 32
                Avg steps to solve:       5.000
                
Iteration (7400/50001) took 0.088 seconds.
                Mean final reward:        9.3125
                Mean return:              4.7812
                Policy entropy:           0.7751
                Pseudo loss:              3.39815
                Total gradient norm:      0.33974
                Solved trajectories:      28 / 32
                Avg steps to solve:       4.893
                
Iteration (7500/50001) took 0.152 seconds.
                Mean final reward:        8.2812
                Mean return:              3.2812
                Policy entropy:           0.8144
                Pseudo loss:              3.61037
                Total gradient norm:      0.35639
                Solved trajectories:      27 / 32
                Avg steps to solve:       5.259
                
Iteration (7600/50001) took 0.113 seconds.
                Mean final reward:        8.6250
                Mean return:              3.3438
                Policy entropy:           0.8600
                Pseudo loss:              3.95792
                Total gradient norm:      0.31698
                Solved trajectories:      24 / 32
                Avg steps to solve:       5.042
                
Iteration (7700/50001) took 0.138 seconds.
                Mean final reward:        8.6250
                Mean return:              3.3750
                Policy entropy:           0.8155
                Pseudo loss:              3.48385
                Total gradient norm:      0.31381
                Solved trajectories:      25 / 32
                Avg steps to solve:       5.200
                
Iteration (7800/50001) took 0.145 seconds.
                Mean final reward:        7.2500
                Mean return:              1.9375
                Policy entropy:           0.8949
                Pseudo loss:              3.14880
                Total gradient norm:      0.36162
                Solved trajectories:      24 / 32
                Avg steps to solve:       5.083
                
Iteration (7900/50001) took 0.098 seconds.
                Mean final reward:        7.2500
                Mean return:              1.8125
                Policy entropy:           0.9026
                Pseudo loss:              2.71231
                Total gradient norm:      0.30211
                Solved trajectories:      24 / 32
                Avg steps to solve:       5.250
                
Iteration (8000/50001) took 0.090 seconds.
                Mean final reward:        7.5938
                Mean return:              2.0625
                Policy entropy:           0.9367
                Pseudo loss:              3.27164
                Total gradient norm:      0.32795
                Solved trajectories:      24 / 32
                Avg steps to solve:       5.375
                
Iteration (8100/50001) took 0.159 seconds.
                Mean final reward:        8.9688
                Mean return:              4.4062
                Policy entropy:           0.8160
                Pseudo loss:              2.57139
                Total gradient norm:      0.32490
                Solved trajectories:      28 / 32
                Avg steps to solve:       4.929
                
Iteration (8200/50001) took 0.130 seconds.
                Mean final reward:        8.2812
                Mean return:              3.0312
                Policy entropy:           0.8157
                Pseudo loss:              3.57021
                Total gradient norm:      0.28795
                Solved trajectories:      27 / 32
                Avg steps to solve:       5.556
                
Iteration (8300/50001) took 0.140 seconds.
                Mean final reward:        9.6562
                Mean return:              4.3750
                Policy entropy:           0.7581
                Pseudo loss:              4.71343
                Total gradient norm:      0.36245
                Solved trajectories:      26 / 32
                Avg steps to solve:       5.423
                
Iteration (8400/50001) took 0.115 seconds.
                Mean final reward:        7.5938
                Mean return:              1.5938
                Policy entropy:           0.9103
                Pseudo loss:              3.68353
                Total gradient norm:      0.33604
                Solved trajectories:      23 / 32
                Avg steps to solve:       5.826
                
Iteration (8500/50001) took 0.091 seconds.
                Mean final reward:        8.9688
                Mean return:              3.8125
                Policy entropy:           0.7946
                Pseudo loss:              3.78640
                Total gradient norm:      0.39308
                Solved trajectories:      28 / 32
                Avg steps to solve:       5.607
                
Iteration (8600/50001) took 0.107 seconds.
                Mean final reward:        8.2812
                Mean return:              3.7812
                Policy entropy:           0.8193
                Pseudo loss:              2.77279
                Total gradient norm:      0.41002
                Solved trajectories:      26 / 32
                Avg steps to solve:       4.462
                
Iteration (8700/50001) took 0.156 seconds.
                Mean final reward:        7.5938
                Mean return:              1.8750
                Policy entropy:           0.8548
                Pseudo loss:              3.17775
                Total gradient norm:      0.34912
                Solved trajectories:      23 / 32
                Avg steps to solve:       5.435
                
Iteration (8800/50001) took 0.187 seconds.
                Mean final reward:        8.2812
                Mean return:              2.7500
                Policy entropy:           0.8311
                Pseudo loss:              3.76660
                Total gradient norm:      0.37642
                Solved trajectories:      23 / 32
                Avg steps to solve:       5.174
                
Iteration (8900/50001) took 0.116 seconds.
                Mean final reward:        8.9688
                Mean return:              4.0938
                Policy entropy:           0.7739
                Pseudo loss:              3.28283
                Total gradient norm:      0.32839
                Solved trajectories:      29 / 32
                Avg steps to solve:       5.448
                
Iteration (9000/50001) took 0.113 seconds.
                Mean final reward:        8.6250
                Mean return:              4.2812
                Policy entropy:           0.7542
                Pseudo loss:              2.98087
                Total gradient norm:      0.37431
                Solved trajectories:      28 / 32
                Avg steps to solve:       4.679
                
Iteration (9100/50001) took 0.179 seconds.
                Mean final reward:        8.9688
                Mean return:              3.7500
                Policy entropy:           0.7887
                Pseudo loss:              2.91537
                Total gradient norm:      0.29975
                Solved trajectories:      26 / 32
                Avg steps to solve:       5.346
                
Iteration (9200/50001) took 0.113 seconds.
                Mean final reward:        7.9375
                Mean return:              2.0625
                Policy entropy:           0.8616
                Pseudo loss:              3.82580
                Total gradient norm:      0.44381
                Solved trajectories:      23 / 32
                Avg steps to solve:       5.652
                
Iteration (9300/50001) took 0.149 seconds.
                Mean final reward:        8.6250
                Mean return:              3.5000
                Policy entropy:           0.7607
                Pseudo loss:              3.47277
                Total gradient norm:      0.29555
                Solved trajectories:      28 / 32
                Avg steps to solve:       5.571
                
Iteration (9400/50001) took 0.139 seconds.
                Mean final reward:        8.6250
                Mean return:              3.6250
                Policy entropy:           0.7380
                Pseudo loss:              3.13010
                Total gradient norm:      0.28936
                Solved trajectories:      28 / 32
                Avg steps to solve:       5.429
                
Iteration (9500/50001) took 0.115 seconds.
                Mean final reward:        7.9375
                Mean return:              3.6875
                Policy entropy:           0.8073
                Pseudo loss:              1.74993
                Total gradient norm:      0.26163
                Solved trajectories:      26 / 32
                Avg steps to solve:       4.154
                
Iteration (9600/50001) took 0.092 seconds.
                Mean final reward:        8.9688
                Mean return:              4.1250
                Policy entropy:           0.7661
                Pseudo loss:              3.66587
                Total gradient norm:      0.26765
                Solved trajectories:      28 / 32
                Avg steps to solve:       5.250
                
Iteration (9700/50001) took 0.090 seconds.
                Mean final reward:        9.3125
                Mean return:              4.2812
                Policy entropy:           0.7524
                Pseudo loss:              3.81717
                Total gradient norm:      0.45386
                Solved trajectories:      28 / 32
                Avg steps to solve:       5.464
                
Iteration (9800/50001) took 0.137 seconds.
                Mean final reward:        8.9688
                Mean return:              4.0312
                Policy entropy:           0.7473
                Pseudo loss:              3.38033
                Total gradient norm:      0.34050
                Solved trajectories:      27 / 32
                Avg steps to solve:       5.185
                
Iteration (9900/50001) took 0.090 seconds.
                Mean final reward:        9.3125
                Mean return:              4.3438
                Policy entropy:           0.7346
                Pseudo loss:              3.65361
                Total gradient norm:      0.42793
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.700
                
Iteration (10000/50001) took 0.195 seconds.
                Mean final reward:        8.6250
                Mean return:              2.9062
                Policy entropy:           0.8218
                Pseudo loss:              3.39346
                Total gradient norm:      0.28410
                Solved trajectories:      27 / 32
                Avg steps to solve:       6.111
                
Iteration (10100/50001) took 0.164 seconds.
                Mean final reward:        8.2812
                Mean return:              3.5000
                Policy entropy:           0.7402
                Pseudo loss:              2.39404
                Total gradient norm:      0.29476
                Solved trajectories:      27 / 32
                Avg steps to solve:       5.000
                
Iteration (10200/50001) took 0.174 seconds.
                Mean final reward:        8.9688
                Mean return:              4.0000
                Policy entropy:           0.7490
                Pseudo loss:              3.20316
                Total gradient norm:      0.31187
                Solved trajectories:      28 / 32
                Avg steps to solve:       5.393
                
Iteration (10300/50001) took 0.094 seconds.
                Mean final reward:        8.2812
                Mean return:              3.5000
                Policy entropy:           0.7620
                Pseudo loss:              2.73922
                Total gradient norm:      0.25218
                Solved trajectories:      27 / 32
                Avg steps to solve:       5.000
                
Iteration (10400/50001) took 0.105 seconds.
                Mean final reward:        8.9688
                Mean return:              4.0000
                Policy entropy:           0.8006
                Pseudo loss:              3.16866
                Total gradient norm:      0.35038
                Solved trajectories:      28 / 32
                Avg steps to solve:       5.393
                
Iteration (10500/50001) took 0.116 seconds.
                Mean final reward:        7.9375
                Mean return:              2.5625
                Policy entropy:           0.8220
                Pseudo loss:              3.20352
                Total gradient norm:      0.29938
                Solved trajectories:      24 / 32
                Avg steps to solve:       5.167
                
Iteration (10600/50001) took 0.185 seconds.
                Mean final reward:        8.6250
                Mean return:              3.2188
                Policy entropy:           0.7407
                Pseudo loss:              3.42979
                Total gradient norm:      0.30948
                Solved trajectories:      27 / 32
                Avg steps to solve:       5.741
                
Iteration (10700/50001) took 0.143 seconds.
                Mean final reward:        8.2812
                Mean return:              2.7188
                Policy entropy:           0.7666
                Pseudo loss:              3.43736
                Total gradient norm:      0.35031
                Solved trajectories:      23 / 32
                Avg steps to solve:       5.217
                
Iteration (10800/50001) took 0.144 seconds.
                Mean final reward:        9.3125
                Mean return:              4.9688
                Policy entropy:           0.6966
                Pseudo loss:              3.18337
                Total gradient norm:      0.29813
                Solved trajectories:      29 / 32
                Avg steps to solve:       4.862
                
Iteration (10900/50001) took 0.129 seconds.
                Mean final reward:        9.3125
                Mean return:              5.0000
                Policy entropy:           0.6742
                Pseudo loss:              2.82423
                Total gradient norm:      0.29723
                Solved trajectories:      29 / 32
                Avg steps to solve:       4.828
                
Iteration (11000/50001) took 0.148 seconds.
                Mean final reward:        8.9688
                Mean return:              3.3438
                Policy entropy:           0.7676
                Pseudo loss:              4.31284
                Total gradient norm:      0.42177
                Solved trajectories:      26 / 32
                Avg steps to solve:       5.846
                
Iteration (11100/50001) took 0.101 seconds.
                Mean final reward:        8.9688
                Mean return:              4.1562
                Policy entropy:           0.7248
                Pseudo loss:              2.99920
                Total gradient norm:      0.34019
                Solved trajectories:      28 / 32
                Avg steps to solve:       5.214
                
Iteration (11200/50001) took 0.145 seconds.
                Mean final reward:        9.3125
                Mean return:              5.3438
                Policy entropy:           0.6807
                Pseudo loss:              2.93739
                Total gradient norm:      0.32648
                Solved trajectories:      29 / 32
                Avg steps to solve:       4.448
                
Iteration (11300/50001) took 0.099 seconds.
                Mean final reward:        8.6250
                Mean return:              3.0625
                Policy entropy:           0.7380
                Pseudo loss:              3.37286
                Total gradient norm:      0.31104
                Solved trajectories:      27 / 32
                Avg steps to solve:       5.926
                
Iteration (11400/50001) took 0.158 seconds.
                Mean final reward:        7.5938
                Mean return:              1.9375
                Policy entropy:           0.7790
                Pseudo loss:              3.39674
                Total gradient norm:      0.35975
                Solved trajectories:      24 / 32
                Avg steps to solve:       5.542
                
Iteration (11500/50001) took 0.145 seconds.
                Mean final reward:        8.9688
                Mean return:              4.2188
                Policy entropy:           0.7163
                Pseudo loss:              2.90162
                Total gradient norm:      0.25014
                Solved trajectories:      28 / 32
                Avg steps to solve:       5.143
                
Iteration (11600/50001) took 0.085 seconds.
                Mean final reward:        8.9688
                Mean return:              3.5625
                Policy entropy:           0.7210
                Pseudo loss:              3.78615
                Total gradient norm:      0.35635
                Solved trajectories:      25 / 32
                Avg steps to solve:       5.400
                
Iteration (11700/50001) took 0.101 seconds.
                Mean final reward:        8.6250
                Mean return:              3.2188
                Policy entropy:           0.7362
                Pseudo loss:              3.67667
                Total gradient norm:      0.32412
                Solved trajectories:      26 / 32
                Avg steps to solve:       5.577
                
Iteration (11800/50001) took 0.105 seconds.
                Mean final reward:        8.2812
                Mean return:              2.8750
                Policy entropy:           0.7340
                Pseudo loss:              3.55749
                Total gradient norm:      0.25807
                Solved trajectories:      26 / 32
                Avg steps to solve:       5.577
                
Iteration (11900/50001) took 0.169 seconds.
                Mean final reward:        9.3125
                Mean return:              4.2500
                Policy entropy:           0.6945
                Pseudo loss:              3.55486
                Total gradient norm:      0.28715
                Solved trajectories:      29 / 32
                Avg steps to solve:       5.655
                
Iteration (12000/50001) took 0.078 seconds.
                Mean final reward:        9.3125
                Mean return:              4.6875
                Policy entropy:           0.7157
                Pseudo loss:              3.22013
                Total gradient norm:      0.36063
                Solved trajectories:      28 / 32
                Avg steps to solve:       5.000
                
Iteration (12100/50001) took 0.082 seconds.
                Mean final reward:        7.5938
                Mean return:              2.2500
                Policy entropy:           0.7577
                Pseudo loss:              2.43295
                Total gradient norm:      0.25777
                Solved trajectories:      24 / 32
                Avg steps to solve:       5.125
                
Iteration (12200/50001) took 0.111 seconds.
                Mean final reward:        10.0000
                Mean return:              4.8438
                Policy entropy:           0.6627
                Pseudo loss:              3.94379
                Total gradient norm:      0.21575
                Solved trajectories:      31 / 32
                Avg steps to solve:       6.032
                
Iteration (12300/50001) took 0.085 seconds.
                Mean final reward:        8.9688
                Mean return:              3.7812
                Policy entropy:           0.7298
                Pseudo loss:              3.45927
                Total gradient norm:      0.35647
                Solved trajectories:      27 / 32
                Avg steps to solve:       5.481
                
Iteration (12400/50001) took 0.088 seconds.
                Mean final reward:        9.6562
                Mean return:              5.4062
                Policy entropy:           0.6225
                Pseudo loss:              3.21125
                Total gradient norm:      0.33011
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.097
                
Iteration (12500/50001) took 0.167 seconds.
                Mean final reward:        8.6250
                Mean return:              4.0312
                Policy entropy:           0.7094
                Pseudo loss:              2.96338
                Total gradient norm:      0.35362
                Solved trajectories:      27 / 32
                Avg steps to solve:       4.778
                
Iteration (12600/50001) took 0.119 seconds.
                Mean final reward:        9.3125
                Mean return:              4.9375
                Policy entropy:           0.6743
                Pseudo loss:              2.55947
                Total gradient norm:      0.24954
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.067
                
Iteration (12700/50001) took 0.193 seconds.
                Mean final reward:        10.0000
                Mean return:              5.6875
                Policy entropy:           0.6289
                Pseudo loss:              3.36770
                Total gradient norm:      0.30450
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.000
                
Iteration (12800/50001) took 0.116 seconds.
                Mean final reward:        7.9375
                Mean return:              2.0625
                Policy entropy:           0.7365
                Pseudo loss:              3.92921
                Total gradient norm:      0.37785
                Solved trajectories:      23 / 32
                Avg steps to solve:       5.652
                
Iteration (12900/50001) took 0.178 seconds.
                Mean final reward:        8.9688
                Mean return:              3.6562
                Policy entropy:           0.7059
                Pseudo loss:              3.72555
                Total gradient norm:      0.43027
                Solved trajectories:      28 / 32
                Avg steps to solve:       5.786
                
Iteration (13000/50001) took 0.163 seconds.
                Mean final reward:        8.9688
                Mean return:              3.9688
                Policy entropy:           0.7128
                Pseudo loss:              3.31146
                Total gradient norm:      0.29696
                Solved trajectories:      27 / 32
                Avg steps to solve:       5.259
                
Iteration (13100/50001) took 0.172 seconds.
                Mean final reward:        9.6562
                Mean return:              5.3750
                Policy entropy:           0.6600
                Pseudo loss:              2.93573
                Total gradient norm:      0.26920
                Solved trajectories:      29 / 32
                Avg steps to solve:       4.793
                
Iteration (13200/50001) took 0.139 seconds.
                Mean final reward:        8.9688
                Mean return:              4.3438
                Policy entropy:           0.7151
                Pseudo loss:              2.97380
                Total gradient norm:      0.36766
                Solved trajectories:      28 / 32
                Avg steps to solve:       5.000
                
Iteration (13300/50001) took 0.146 seconds.
                Mean final reward:        8.2812
                Mean return:              3.7188
                Policy entropy:           0.7172
                Pseudo loss:              2.21569
                Total gradient norm:      0.30084
                Solved trajectories:      26 / 32
                Avg steps to solve:       4.538
                
Iteration (13400/50001) took 0.133 seconds.
                Mean final reward:        8.6250
                Mean return:              3.3125
                Policy entropy:           0.6896
                Pseudo loss:              3.58820
                Total gradient norm:      0.34568
                Solved trajectories:      28 / 32
                Avg steps to solve:       5.786
                
Iteration (13500/50001) took 0.178 seconds.
                Mean final reward:        9.6562
                Mean return:              5.0000
                Policy entropy:           0.6544
                Pseudo loss:              3.40710
                Total gradient norm:      0.32106
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.367
                
Iteration (13600/50001) took 0.092 seconds.
                Mean final reward:        9.3125
                Mean return:              4.9062
                Policy entropy:           0.6499
                Pseudo loss:              2.54847
                Total gradient norm:      0.22054
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.100
                
Iteration (13700/50001) took 0.161 seconds.
                Mean final reward:        8.9688
                Mean return:              4.9688
                Policy entropy:           0.6599
                Pseudo loss:              2.26519
                Total gradient norm:      0.36572
                Solved trajectories:      29 / 32
                Avg steps to solve:       4.483
                
Iteration (13800/50001) took 0.078 seconds.
                Mean final reward:        10.0000
                Mean return:              5.4375
                Policy entropy:           0.6388
                Pseudo loss:              3.38086
                Total gradient norm:      0.29324
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.419
                
Iteration (13900/50001) took 0.188 seconds.
                Mean final reward:        9.3125
                Mean return:              4.5312
                Policy entropy:           0.6461
                Pseudo loss:              3.41336
                Total gradient norm:      0.37650
                Solved trajectories:      29 / 32
                Avg steps to solve:       5.345
                
Iteration (14000/50001) took 0.260 seconds.
                Mean final reward:        10.0000
                Mean return:              4.9688
                Policy entropy:           0.6685
                Pseudo loss:              3.70107
                Total gradient norm:      0.33358
                Solved trajectories:      29 / 32
                Avg steps to solve:       5.621
                
Iteration (14100/50001) took 0.110 seconds.
                Mean final reward:        9.6562
                Mean return:              6.1875
                Policy entropy:           0.6029
                Pseudo loss:              2.14886
                Total gradient norm:      0.27510
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.290
                
Iteration (14200/50001) took 0.157 seconds.
                Mean final reward:        10.0000
                Mean return:              5.8438
                Policy entropy:           0.5937
                Pseudo loss:              3.44519
                Total gradient norm:      0.28023
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.156
                
Iteration (14300/50001) took 0.154 seconds.
                Mean final reward:        9.6562
                Mean return:              5.7812
                Policy entropy:           0.5911
                Pseudo loss:              2.90099
                Total gradient norm:      0.27835
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.533
                
Iteration (14400/50001) took 0.170 seconds.
                Mean final reward:        8.9688
                Mean return:              4.1875
                Policy entropy:           0.7105
                Pseudo loss:              3.42866
                Total gradient norm:      0.29128
                Solved trajectories:      27 / 32
                Avg steps to solve:       5.000
                
Iteration (14500/50001) took 0.139 seconds.
                Mean final reward:        9.3125
                Mean return:              4.5938
                Policy entropy:           0.6394
                Pseudo loss:              3.54019
                Total gradient norm:      0.32947
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.433
                
Iteration (14600/50001) took 0.060 seconds.
                Mean final reward:        9.6562
                Mean return:              5.0312
                Policy entropy:           0.6557
                Pseudo loss:              2.80671
                Total gradient norm:      0.36951
                Solved trajectories:      29 / 32
                Avg steps to solve:       5.172
                
Iteration (14700/50001) took 0.131 seconds.
                Mean final reward:        9.6562
                Mean return:              5.2500
                Policy entropy:           0.6447
                Pseudo loss:              3.01202
                Total gradient norm:      0.26270
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.100
                
Iteration (14800/50001) took 0.148 seconds.
                Mean final reward:        10.0000
                Mean return:              5.8125
                Policy entropy:           0.6093
                Pseudo loss:              3.21916
                Total gradient norm:      0.32324
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.032
                
Iteration (14900/50001) took 0.149 seconds.
                Mean final reward:        9.3125
                Mean return:              4.0938
                Policy entropy:           0.6816
                Pseudo loss:              3.35291
                Total gradient norm:      0.30663
                Solved trajectories:      29 / 32
                Avg steps to solve:       5.828
                
Iteration (15000/50001) took 0.132 seconds.
                Mean final reward:        8.9688
                Mean return:              4.9688
                Policy entropy:           0.6135
                Pseudo loss:              2.42344
                Total gradient norm:      0.31514
                Solved trajectories:      29 / 32
                Avg steps to solve:       4.483
                
Iteration (15100/50001) took 0.209 seconds.
                Mean final reward:        9.3125
                Mean return:              3.8750
                Policy entropy:           0.6826
                Pseudo loss:              4.01244
                Total gradient norm:      0.33871
                Solved trajectories:      26 / 32
                Avg steps to solve:       5.615
                
Iteration (15200/50001) took 0.068 seconds.
                Mean final reward:        9.6562
                Mean return:              4.6562
                Policy entropy:           0.6491
                Pseudo loss:              3.45268
                Total gradient norm:      0.29206
                Solved trajectories:      29 / 32
                Avg steps to solve:       5.586
                
Iteration (15300/50001) took 0.138 seconds.
                Mean final reward:        9.3125
                Mean return:              4.4375
                Policy entropy:           0.6737
                Pseudo loss:              3.17036
                Total gradient norm:      0.28835
                Solved trajectories:      28 / 32
                Avg steps to solve:       5.286
                
Iteration (15400/50001) took 0.176 seconds.
                Mean final reward:        8.9688
                Mean return:              3.7500
                Policy entropy:           0.6900
                Pseudo loss:              3.61106
                Total gradient norm:      0.41762
                Solved trajectories:      27 / 32
                Avg steps to solve:       5.519
                
Iteration (15500/50001) took 0.163 seconds.
                Mean final reward:        8.9688
                Mean return:              4.5000
                Policy entropy:           0.6725
                Pseudo loss:              2.55872
                Total gradient norm:      0.31141
                Solved trajectories:      29 / 32
                Avg steps to solve:       5.000
                
Iteration (15600/50001) took 0.176 seconds.
                Mean final reward:        9.3125
                Mean return:              4.4375
                Policy entropy:           0.6322
                Pseudo loss:              3.51625
                Total gradient norm:      0.25330
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.600
                
Iteration (15700/50001) took 0.123 seconds.
                Mean final reward:        8.9688
                Mean return:              4.0625
                Policy entropy:           0.6664
                Pseudo loss:              2.95712
                Total gradient norm:      0.33485
                Solved trajectories:      27 / 32
                Avg steps to solve:       5.148
                
Iteration (15800/50001) took 0.097 seconds.
                Mean final reward:        9.6562
                Mean return:              5.0625
                Policy entropy:           0.6265
                Pseudo loss:              3.06178
                Total gradient norm:      0.36142
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.300
                
Iteration (15900/50001) took 0.129 seconds.
                Mean final reward:        9.3125
                Mean return:              4.6875
                Policy entropy:           0.6617
                Pseudo loss:              2.95201
                Total gradient norm:      0.24761
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.333
                
Iteration (16000/50001) took 0.145 seconds.
                Mean final reward:        9.6562
                Mean return:              5.2500
                Policy entropy:           0.6259
                Pseudo loss:              2.90456
                Total gradient norm:      0.33322
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.100
                
Iteration (16100/50001) took 0.200 seconds.
                Mean final reward:        9.3125
                Mean return:              4.2188
                Policy entropy:           0.6556
                Pseudo loss:              4.45125
                Total gradient norm:      0.42937
                Solved trajectories:      27 / 32
                Avg steps to solve:       5.370
                
Iteration (16200/50001) took 0.144 seconds.
                Mean final reward:        9.6562
                Mean return:              5.0000
                Policy entropy:           0.6062
                Pseudo loss:              3.98101
                Total gradient norm:      0.32693
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.516
                
Iteration (16300/50001) took 0.111 seconds.
                Mean final reward:        10.0000
                Mean return:              4.6875
                Policy entropy:           0.6395
                Pseudo loss:              4.25804
                Total gradient norm:      0.32451
                Solved trajectories:      30 / 32
                Avg steps to solve:       6.067
                
Iteration (16400/50001) took 0.179 seconds.
                Mean final reward:        8.9688
                Mean return:              4.3125
                Policy entropy:           0.6610
                Pseudo loss:              2.91546
                Total gradient norm:      0.27621
                Solved trajectories:      28 / 32
                Avg steps to solve:       5.036
                
Iteration (16500/50001) took 0.157 seconds.
                Mean final reward:        9.3125
                Mean return:              4.1250
                Policy entropy:           0.6678
                Pseudo loss:              2.88913
                Total gradient norm:      0.30949
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.933
                
Iteration (16600/50001) took 0.139 seconds.
                Mean final reward:        9.6562
                Mean return:              5.1562
                Policy entropy:           0.6243
                Pseudo loss:              3.08021
                Total gradient norm:      0.30619
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.200
                
Iteration (16700/50001) took 0.106 seconds.
                Mean final reward:        9.6562
                Mean return:              5.1562
                Policy entropy:           0.6046
                Pseudo loss:              2.87402
                Total gradient norm:      0.29979
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.355
                
Iteration (16800/50001) took 0.128 seconds.
                Mean final reward:        10.0000
                Mean return:              5.8438
                Policy entropy:           0.5991
                Pseudo loss:              3.16286
                Total gradient norm:      0.25633
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.000
                
Iteration (16900/50001) took 0.117 seconds.
                Mean final reward:        9.6562
                Mean return:              5.6875
                Policy entropy:           0.5732
                Pseudo loss:              2.71716
                Total gradient norm:      0.30745
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.806
                
Iteration (17000/50001) took 0.102 seconds.
                Mean final reward:        8.6250
                Mean return:              2.7500
                Policy entropy:           0.7003
                Pseudo loss:              3.35834
                Total gradient norm:      0.27895
                Solved trajectories:      25 / 32
                Avg steps to solve:       6.000
                
Iteration (17100/50001) took 0.157 seconds.
                Mean final reward:        9.3125
                Mean return:              4.4375
                Policy entropy:           0.6481
                Pseudo loss:              2.80140
                Total gradient norm:      0.40393
                Solved trajectories:      27 / 32
                Avg steps to solve:       5.111
                
Iteration (17200/50001) took 0.163 seconds.
                Mean final reward:        8.9688
                Mean return:              4.1562
                Policy entropy:           0.6497
                Pseudo loss:              3.14194
                Total gradient norm:      0.28308
                Solved trajectories:      28 / 32
                Avg steps to solve:       5.214
                
Iteration (17300/50001) took 0.157 seconds.
                Mean final reward:        9.6562
                Mean return:              5.2500
                Policy entropy:           0.5946
                Pseudo loss:              3.05207
                Total gradient norm:      0.30971
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.100
                
Iteration (17400/50001) took 0.111 seconds.
                Mean final reward:        10.0000
                Mean return:              5.6875
                Policy entropy:           0.5993
                Pseudo loss:              3.08871
                Total gradient norm:      0.36648
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.000
                
Iteration (17500/50001) took 0.127 seconds.
                Mean final reward:        9.3125
                Mean return:              4.4062
                Policy entropy:           0.6429
                Pseudo loss:              3.55252
                Total gradient norm:      0.28790
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.633
                
Iteration (17600/50001) took 0.092 seconds.
                Mean final reward:        9.3125
                Mean return:              4.4688
                Policy entropy:           0.6438
                Pseudo loss:              3.04818
                Total gradient norm:      0.21048
                Solved trajectories:      26 / 32
                Avg steps to solve:       4.885
                
Iteration (17700/50001) took 0.180 seconds.
                Mean final reward:        8.9688
                Mean return:              4.1875
                Policy entropy:           0.6559
                Pseudo loss:              2.79296
                Total gradient norm:      0.36501
                Solved trajectories:      29 / 32
                Avg steps to solve:       5.345
                
Iteration (17800/50001) took 0.113 seconds.
                Mean final reward:        10.0000
                Mean return:              6.0938
                Policy entropy:           0.5665
                Pseudo loss:              3.24349
                Total gradient norm:      0.38851
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.742
                
Iteration (17900/50001) took 0.108 seconds.
                Mean final reward:        9.3125
                Mean return:              4.9062
                Policy entropy:           0.6156
                Pseudo loss:              2.97535
                Total gradient norm:      0.28052
                Solved trajectories:      29 / 32
                Avg steps to solve:       4.931
                
Iteration (18000/50001) took 0.138 seconds.
                Mean final reward:        9.6562
                Mean return:              5.0000
                Policy entropy:           0.6087
                Pseudo loss:              3.30977
                Total gradient norm:      0.29126
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.516
                
Iteration (18100/50001) took 0.148 seconds.
                Mean final reward:        9.6562
                Mean return:              4.7812
                Policy entropy:           0.6173
                Pseudo loss:              3.44124
                Total gradient norm:      0.30483
                Solved trajectories:      27 / 32
                Avg steps to solve:       5.111
                
Iteration (18200/50001) took 0.201 seconds.
                Mean final reward:        9.6562
                Mean return:              4.4688
                Policy entropy:           0.6384
                Pseudo loss:              3.92194
                Total gradient norm:      0.26059
                Solved trajectories:      27 / 32
                Avg steps to solve:       5.481
                
Iteration (18300/50001) took 0.142 seconds.
                Mean final reward:        9.3125
                Mean return:              4.5000
                Policy entropy:           0.6222
                Pseudo loss:              3.11499
                Total gradient norm:      0.30426
                Solved trajectories:      27 / 32
                Avg steps to solve:       5.037
                
Iteration (18400/50001) took 0.296 seconds.
                Mean final reward:        9.6562
                Mean return:              4.1250
                Policy entropy:           0.6491
                Pseudo loss:              4.45844
                Total gradient norm:      0.31824
                Solved trajectories:      30 / 32
                Avg steps to solve:       6.300
                
Iteration (18500/50001) took 0.085 seconds.
                Mean final reward:        9.6562
                Mean return:              4.8125
                Policy entropy:           0.6519
                Pseudo loss:              3.22885
                Total gradient norm:      0.31616
                Solved trajectories:      29 / 32
                Avg steps to solve:       5.414
                
Iteration (18600/50001) took 0.210 seconds.
                Mean final reward:        9.3125
                Mean return:              4.3750
                Policy entropy:           0.6554
                Pseudo loss:              3.12997
                Total gradient norm:      0.30689
                Solved trajectories:      28 / 32
                Avg steps to solve:       5.357
                
Iteration (18700/50001) took 0.096 seconds.
                Mean final reward:        9.6562
                Mean return:              4.5000
                Policy entropy:           0.6399
                Pseudo loss:              3.57339
                Total gradient norm:      0.39235
                Solved trajectories:      28 / 32
                Avg steps to solve:       5.607
                
Iteration (18800/50001) took 0.149 seconds.
                Mean final reward:        10.0000
                Mean return:              5.6562
                Policy entropy:           0.5565
                Pseudo loss:              3.37730
                Total gradient norm:      0.32994
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.194
                
Iteration (18900/50001) took 0.113 seconds.
                Mean final reward:        9.6562
                Mean return:              5.6875
                Policy entropy:           0.5825
                Pseudo loss:              2.73768
                Total gradient norm:      0.29077
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.806
                
Iteration (19000/50001) took 0.072 seconds.
                Mean final reward:        9.6562
                Mean return:              5.2812
                Policy entropy:           0.6097
                Pseudo loss:              2.76625
                Total gradient norm:      0.24040
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.067
                
Iteration (19100/50001) took 0.166 seconds.
                Mean final reward:        9.3125
                Mean return:              5.0938
                Policy entropy:           0.5828
                Pseudo loss:              2.44839
                Total gradient norm:      0.21383
                Solved trajectories:      29 / 32
                Avg steps to solve:       4.724
                
Iteration (19200/50001) took 0.170 seconds.
                Mean final reward:        9.6562
                Mean return:              5.0625
                Policy entropy:           0.6211
                Pseudo loss:              2.76928
                Total gradient norm:      0.31528
                Solved trajectories:      28 / 32
                Avg steps to solve:       4.964
                
Iteration (19300/50001) took 0.074 seconds.
                Mean final reward:        9.6562
                Mean return:              5.2188
                Policy entropy:           0.5951
                Pseudo loss:              3.16934
                Total gradient norm:      0.34989
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.133
                
Iteration (19400/50001) took 0.123 seconds.
                Mean final reward:        10.0000
                Mean return:              5.6250
                Policy entropy:           0.5969
                Pseudo loss:              2.83630
                Total gradient norm:      0.24438
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.375
                
Iteration (19500/50001) took 0.168 seconds.
                Mean final reward:        9.6562
                Mean return:              5.2188
                Policy entropy:           0.5951
                Pseudo loss:              2.83883
                Total gradient norm:      0.24809
                Solved trajectories:      29 / 32
                Avg steps to solve:       4.966
                
Iteration (19600/50001) took 0.086 seconds.
                Mean final reward:        10.0000
                Mean return:              5.2812
                Policy entropy:           0.6127
                Pseudo loss:              3.24291
                Total gradient norm:      0.22395
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.719
                
Iteration (19700/50001) took 0.106 seconds.
                Mean final reward:        10.0000
                Mean return:              5.7188
                Policy entropy:           0.5730
                Pseudo loss:              3.25078
                Total gradient norm:      0.27140
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.129
                
Iteration (19800/50001) took 0.171 seconds.
                Mean final reward:        9.3125
                Mean return:              4.4062
                Policy entropy:           0.6146
                Pseudo loss:              4.19640
                Total gradient norm:      0.46508
                Solved trajectories:      29 / 32
                Avg steps to solve:       5.483
                
Iteration (19900/50001) took 0.168 seconds.
                Mean final reward:        8.9688
                Mean return:              4.3438
                Policy entropy:           0.6217
                Pseudo loss:              2.95388
                Total gradient norm:      0.36150
                Solved trajectories:      29 / 32
                Avg steps to solve:       5.172
                
Iteration (20000/50001) took 0.121 seconds.
                Mean final reward:        9.6562
                Mean return:              5.3438
                Policy entropy:           0.5753
                Pseudo loss:              2.90509
                Total gradient norm:      0.27858
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.161
                
Iteration (20100/50001) took 0.195 seconds.
                Mean final reward:        9.6562
                Mean return:              5.1875
                Policy entropy:           0.6222
                Pseudo loss:              2.20234
                Total gradient norm:      0.30546
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.323
                
Iteration (20200/50001) took 0.182 seconds.
                Mean final reward:        10.0000
                Mean return:              6.3438
                Policy entropy:           0.5542
                Pseudo loss:              2.50590
                Total gradient norm:      0.22425
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.484
                
Iteration (20300/50001) took 0.075 seconds.
                Mean final reward:        9.3125
                Mean return:              4.3750
                Policy entropy:           0.6474
                Pseudo loss:              3.19307
                Total gradient norm:      0.30175
                Solved trajectories:      27 / 32
                Avg steps to solve:       5.185
                
Iteration (20400/50001) took 0.062 seconds.
                Mean final reward:        9.3125
                Mean return:              4.9375
                Policy entropy:           0.6067
                Pseudo loss:              2.40548
                Total gradient norm:      0.25467
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.067
                
Iteration (20500/50001) took 0.167 seconds.
                Mean final reward:        10.0000
                Mean return:              5.4688
                Policy entropy:           0.5753
                Pseudo loss:              2.91599
                Total gradient norm:      0.28160
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.531
                
Iteration (20600/50001) took 0.102 seconds.
                Mean final reward:        10.0000
                Mean return:              5.2812
                Policy entropy:           0.6151
                Pseudo loss:              3.26467
                Total gradient norm:      0.30217
                Solved trajectories:      29 / 32
                Avg steps to solve:       5.276
                
Iteration (20700/50001) took 0.072 seconds.
                Mean final reward:        10.0000
                Mean return:              6.3125
                Policy entropy:           0.5466
                Pseudo loss:              2.52618
                Total gradient norm:      0.29761
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.688
                
Iteration (20800/50001) took 0.133 seconds.
                Mean final reward:        8.9688
                Mean return:              3.8125
                Policy entropy:           0.6478
                Pseudo loss:              3.25144
                Total gradient norm:      0.35100
                Solved trajectories:      29 / 32
                Avg steps to solve:       5.759
                
Iteration (20900/50001) took 0.135 seconds.
                Mean final reward:        9.3125
                Mean return:              4.8125
                Policy entropy:           0.5864
                Pseudo loss:              3.14240
                Total gradient norm:      0.29398
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.200
                
Iteration (21000/50001) took 0.155 seconds.
                Mean final reward:        9.3125
                Mean return:              4.5312
                Policy entropy:           0.6085
                Pseudo loss:              2.82596
                Total gradient norm:      0.28072
                Solved trajectories:      29 / 32
                Avg steps to solve:       5.345
                
Iteration (21100/50001) took 0.156 seconds.
                Mean final reward:        9.6562
                Mean return:              4.7812
                Policy entropy:           0.6169
                Pseudo loss:              3.99470
                Total gradient norm:      0.38365
                Solved trajectories:      29 / 32
                Avg steps to solve:       5.448
                
Iteration (21200/50001) took 0.186 seconds.
                Mean final reward:        9.3125
                Mean return:              4.7812
                Policy entropy:           0.6054
                Pseudo loss:              3.11636
                Total gradient norm:      0.25538
                Solved trajectories:      27 / 32
                Avg steps to solve:       4.704
                
Iteration (21300/50001) took 0.147 seconds.
                Mean final reward:        9.3125
                Mean return:              4.0938
                Policy entropy:           0.6327
                Pseudo loss:              2.84583
                Total gradient norm:      0.26049
                Solved trajectories:      28 / 32
                Avg steps to solve:       5.679
                
Iteration (21400/50001) took 0.162 seconds.
                Mean final reward:        8.9688
                Mean return:              3.5000
                Policy entropy:           0.6580
                Pseudo loss:              3.64429
                Total gradient norm:      0.46619
                Solved trajectories:      27 / 32
                Avg steps to solve:       5.815
                
Iteration (21500/50001) took 0.161 seconds.
                Mean final reward:        9.6562
                Mean return:              5.1250
                Policy entropy:           0.6221
                Pseudo loss:              3.15117
                Total gradient norm:      0.24843
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.387
                
Iteration (21600/50001) took 0.078 seconds.
                Mean final reward:        9.6562
                Mean return:              5.2812
                Policy entropy:           0.5985
                Pseudo loss:              2.45230
                Total gradient norm:      0.26873
                Solved trajectories:      29 / 32
                Avg steps to solve:       4.897
                
Iteration (21700/50001) took 0.088 seconds.
                Mean final reward:        9.3125
                Mean return:              4.3438
                Policy entropy:           0.6489
                Pseudo loss:              3.20827
                Total gradient norm:      0.42079
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.700
                
Iteration (21800/50001) took 0.097 seconds.
                Mean final reward:        10.0000
                Mean return:              5.8750
                Policy entropy:           0.5820
                Pseudo loss:              2.99167
                Total gradient norm:      0.20184
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.800
                
Iteration (21900/50001) took 0.103 seconds.
                Mean final reward:        9.6562
                Mean return:              5.3438
                Policy entropy:           0.5955
                Pseudo loss:              2.91952
                Total gradient norm:      0.33799
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.161
                
Iteration (22000/50001) took 0.103 seconds.
                Mean final reward:        9.3125
                Mean return:              4.9688
                Policy entropy:           0.5918
                Pseudo loss:              2.40223
                Total gradient norm:      0.31138
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.033
                
Iteration (22100/50001) took 0.138 seconds.
                Mean final reward:        9.3125
                Mean return:              5.0312
                Policy entropy:           0.5933
                Pseudo loss:              2.61661
                Total gradient norm:      0.22242
                Solved trajectories:      29 / 32
                Avg steps to solve:       4.793
                
Iteration (22200/50001) took 0.113 seconds.
                Mean final reward:        9.3125
                Mean return:              4.4375
                Policy entropy:           0.6174
                Pseudo loss:              3.22024
                Total gradient norm:      0.38498
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.600
                
Iteration (22300/50001) took 0.090 seconds.
                Mean final reward:        9.6562
                Mean return:              4.6875
                Policy entropy:           0.6202
                Pseudo loss:              3.42308
                Total gradient norm:      0.35835
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.839
                
Iteration (22400/50001) took 0.136 seconds.
                Mean final reward:        8.9688
                Mean return:              4.2812
                Policy entropy:           0.6088
                Pseudo loss:              2.99631
                Total gradient norm:      0.27523
                Solved trajectories:      28 / 32
                Avg steps to solve:       5.071
                
Iteration (22500/50001) took 0.162 seconds.
                Mean final reward:        9.6562
                Mean return:              4.9688
                Policy entropy:           0.6022
                Pseudo loss:              2.79047
                Total gradient norm:      0.27775
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.548
                
Iteration (22600/50001) took 0.065 seconds.
                Mean final reward:        9.6562
                Mean return:              5.0625
                Policy entropy:           0.5912
                Pseudo loss:              2.94177
                Total gradient norm:      0.33404
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.300
                
Iteration (22700/50001) took 0.139 seconds.
                Mean final reward:        9.3125
                Mean return:              4.3438
                Policy entropy:           0.6324
                Pseudo loss:              3.25901
                Total gradient norm:      0.37129
                Solved trajectories:      28 / 32
                Avg steps to solve:       5.393
                
Iteration (22800/50001) took 0.153 seconds.
                Mean final reward:        9.3125
                Mean return:              4.4375
                Policy entropy:           0.6153
                Pseudo loss:              3.01485
                Total gradient norm:      0.28071
                Solved trajectories:      28 / 32
                Avg steps to solve:       5.286
                
Iteration (22900/50001) took 0.220 seconds.
                Mean final reward:        10.0000
                Mean return:              5.2188
                Policy entropy:           0.6080
                Pseudo loss:              3.39778
                Total gradient norm:      0.42082
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.645
                
Iteration (23000/50001) took 0.147 seconds.
                Mean final reward:        8.2812
                Mean return:              2.7812
                Policy entropy:           0.6499
                Pseudo loss:              3.20427
                Total gradient norm:      0.30887
                Solved trajectories:      26 / 32
                Avg steps to solve:       5.692
                
Iteration (23100/50001) took 0.133 seconds.
                Mean final reward:        8.6250
                Mean return:              3.1562
                Policy entropy:           0.6393
                Pseudo loss:              2.98040
                Total gradient norm:      0.35846
                Solved trajectories:      28 / 32
                Avg steps to solve:       5.964
                
Iteration (23200/50001) took 0.114 seconds.
                Mean final reward:        9.6562
                Mean return:              5.3125
                Policy entropy:           0.5755
                Pseudo loss:              2.69833
                Total gradient norm:      0.24304
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.033
                
Iteration (23300/50001) took 0.165 seconds.
                Mean final reward:        9.3125
                Mean return:              4.0312
                Policy entropy:           0.6273
                Pseudo loss:              3.40954
                Total gradient norm:      0.30947
                Solved trajectories:      28 / 32
                Avg steps to solve:       5.750
                
Iteration (23400/50001) took 0.149 seconds.
                Mean final reward:        9.6562
                Mean return:              5.5000
                Policy entropy:           0.5624
                Pseudo loss:              2.52741
                Total gradient norm:      0.25843
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.000
                
Iteration (23500/50001) took 0.115 seconds.
                Mean final reward:        9.6562
                Mean return:              4.8125
                Policy entropy:           0.5954
                Pseudo loss:              3.25036
                Total gradient norm:      0.29559
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.567
                
Iteration (23600/50001) took 0.090 seconds.
                Mean final reward:        9.3125
                Mean return:              4.0000
                Policy entropy:           0.6186
                Pseudo loss:              3.97171
                Total gradient norm:      0.41312
                Solved trajectories:      27 / 32
                Avg steps to solve:       5.630
                
Iteration (23700/50001) took 0.146 seconds.
                Mean final reward:        10.0000
                Mean return:              6.0000
                Policy entropy:           0.5545
                Pseudo loss:              3.08112
                Total gradient norm:      0.30685
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.839
                
Iteration (23800/50001) took 0.142 seconds.
                Mean final reward:        9.6562
                Mean return:              4.6250
                Policy entropy:           0.6169
                Pseudo loss:              3.48224
                Total gradient norm:      0.24755
                Solved trajectories:      29 / 32
                Avg steps to solve:       5.621
                
Iteration (23900/50001) took 0.080 seconds.
                Mean final reward:        9.3125
                Mean return:              4.9062
                Policy entropy:           0.5906
                Pseudo loss:              2.99205
                Total gradient norm:      0.49109
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.100
                
Iteration (24000/50001) took 0.059 seconds.
                Mean final reward:        9.3125
                Mean return:              5.1875
                Policy entropy:           0.5864
                Pseudo loss:              2.24261
                Total gradient norm:      0.30781
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.800
                
Iteration (24100/50001) took 0.221 seconds.
                Mean final reward:        9.3125
                Mean return:              4.1562
                Policy entropy:           0.6222
                Pseudo loss:              3.36154
                Total gradient norm:      0.45713
                Solved trajectories:      28 / 32
                Avg steps to solve:       5.607
                
Iteration (24200/50001) took 0.149 seconds.
                Mean final reward:        9.3125
                Mean return:              3.7188
                Policy entropy:           0.6454
                Pseudo loss:              3.42302
                Total gradient norm:      0.28451
                Solved trajectories:      28 / 32
                Avg steps to solve:       6.107
                
Iteration (24300/50001) took 0.138 seconds.
                Mean final reward:        8.6250
                Mean return:              3.8438
                Policy entropy:           0.6265
                Pseudo loss:              2.34840
                Total gradient norm:      0.28572
                Solved trajectories:      28 / 32
                Avg steps to solve:       5.179
                
Iteration (24400/50001) took 0.167 seconds.
                Mean final reward:        9.6562
                Mean return:              4.5000
                Policy entropy:           0.6259
                Pseudo loss:              3.68752
                Total gradient norm:      0.29365
                Solved trajectories:      28 / 32
                Avg steps to solve:       5.607
                
Iteration (24500/50001) took 0.124 seconds.
                Mean final reward:        9.6562
                Mean return:              5.3438
                Policy entropy:           0.5813
                Pseudo loss:              2.81218
                Total gradient norm:      0.28741
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.000
                
Iteration (24600/50001) took 0.104 seconds.
                Mean final reward:        9.6562
                Mean return:              5.1250
                Policy entropy:           0.6071
                Pseudo loss:              3.14544
                Total gradient norm:      0.27005
                Solved trajectories:      29 / 32
                Avg steps to solve:       5.069
                
Iteration (24700/50001) took 0.147 seconds.
                Mean final reward:        8.6250
                Mean return:              3.8125
                Policy entropy:           0.6151
                Pseudo loss:              3.08454
                Total gradient norm:      0.32232
                Solved trajectories:      26 / 32
                Avg steps to solve:       4.846
                
Iteration (24800/50001) took 0.152 seconds.
                Mean final reward:        10.0000
                Mean return:              5.7812
                Policy entropy:           0.5695
                Pseudo loss:              3.16834
                Total gradient norm:      0.26858
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.900
                
Iteration (24900/50001) took 0.137 seconds.
                Mean final reward:        9.6562
                Mean return:              5.1562
                Policy entropy:           0.5921
                Pseudo loss:              2.83737
                Total gradient norm:      0.20716
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.355
                
Iteration (25000/50001) took 0.172 seconds.
                Mean final reward:        9.3125
                Mean return:              4.6875
                Policy entropy:           0.5867
                Pseudo loss:              3.65687
                Total gradient norm:      0.33117
                Solved trajectories:      29 / 32
                Avg steps to solve:       5.172
                
Iteration (25100/50001) took 0.074 seconds.
                Mean final reward:        9.3125
                Mean return:              4.5312
                Policy entropy:           0.6153
                Pseudo loss:              3.19855
                Total gradient norm:      0.37854
                Solved trajectories:      25 / 32
                Avg steps to solve:       4.600
                
Iteration (25200/50001) took 0.163 seconds.
                Mean final reward:        9.6562
                Mean return:              5.5625
                Policy entropy:           0.5646
                Pseudo loss:              2.87220
                Total gradient norm:      0.26410
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.935
                
Iteration (25300/50001) took 0.173 seconds.
                Mean final reward:        9.6562
                Mean return:              4.5000
                Policy entropy:           0.5999
                Pseudo loss:              3.71798
                Total gradient norm:      0.28105
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.900
                
Iteration (25400/50001) took 0.187 seconds.
                Mean final reward:        10.0000
                Mean return:              5.3438
                Policy entropy:           0.5961
                Pseudo loss:              3.47934
                Total gradient norm:      0.33363
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.367
                
Iteration (25500/50001) took 0.078 seconds.
                Mean final reward:        10.0000
                Mean return:              5.4688
                Policy entropy:           0.5890
                Pseudo loss:              3.42569
                Total gradient norm:      0.49165
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.233
                
Iteration (25600/50001) took 0.074 seconds.
                Mean final reward:        9.3125
                Mean return:              5.1250
                Policy entropy:           0.5934
                Pseudo loss:              2.53779
                Total gradient norm:      0.30224
                Solved trajectories:      29 / 32
                Avg steps to solve:       4.690
                
Iteration (25700/50001) took 0.175 seconds.
                Mean final reward:        9.6562
                Mean return:              4.0312
                Policy entropy:           0.6171
                Pseudo loss:              4.39070
                Total gradient norm:      0.35302
                Solved trajectories:      29 / 32
                Avg steps to solve:       6.276
                
Iteration (25800/50001) took 0.166 seconds.
                Mean final reward:        9.6562
                Mean return:              4.0938
                Policy entropy:           0.6307
                Pseudo loss:              4.07574
                Total gradient norm:      0.27626
                Solved trajectories:      29 / 32
                Avg steps to solve:       6.207
                
Iteration (25900/50001) took 0.075 seconds.
                Mean final reward:        10.0000
                Mean return:              5.5625
                Policy entropy:           0.6023
                Pseudo loss:              2.95136
                Total gradient norm:      0.31790
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.133
                
Iteration (26000/50001) took 0.105 seconds.
                Mean final reward:        9.3125
                Mean return:              4.2500
                Policy entropy:           0.6307
                Pseudo loss:              3.19928
                Total gradient norm:      0.35847
                Solved trajectories:      28 / 32
                Avg steps to solve:       5.500
                
Iteration (26100/50001) took 0.068 seconds.
                Mean final reward:        9.6562
                Mean return:              5.4375
                Policy entropy:           0.5795
                Pseudo loss:              2.73728
                Total gradient norm:      0.26317
                Solved trajectories:      29 / 32
                Avg steps to solve:       4.724
                
Iteration (26200/50001) took 0.132 seconds.
                Mean final reward:        9.6562
                Mean return:              4.8750
                Policy entropy:           0.5854
                Pseudo loss:              3.32782
                Total gradient norm:      0.39324
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.500
                
Iteration (26300/50001) took 0.124 seconds.
                Mean final reward:        8.9688
                Mean return:              2.7812
                Policy entropy:           0.6639
                Pseudo loss:              3.47560
                Total gradient norm:      0.31278
                Solved trajectories:      25 / 32
                Avg steps to solve:       6.400
                
Iteration (26400/50001) took 0.091 seconds.
                Mean final reward:        9.6562
                Mean return:              5.0000
                Policy entropy:           0.6020
                Pseudo loss:              2.99927
                Total gradient norm:      0.29956
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.367
                
Iteration (26500/50001) took 0.087 seconds.
                Mean final reward:        9.3125
                Mean return:              3.7812
                Policy entropy:           0.6441
                Pseudo loss:              3.93104
                Total gradient norm:      0.37495
                Solved trajectories:      29 / 32
                Avg steps to solve:       6.172
                
Iteration (26600/50001) took 0.056 seconds.
                Mean final reward:        8.9688
                Mean return:              4.0312
                Policy entropy:           0.5946
                Pseudo loss:              3.15149
                Total gradient norm:      0.32217
                Solved trajectories:      27 / 32
                Avg steps to solve:       5.185
                
Iteration (26700/50001) took 0.104 seconds.
                Mean final reward:        10.0000
                Mean return:              5.8125
                Policy entropy:           0.5588
                Pseudo loss:              2.99525
                Total gradient norm:      0.23717
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.032
                
Iteration (26800/50001) took 0.155 seconds.
                Mean final reward:        10.0000
                Mean return:              5.5625
                Policy entropy:           0.5634
                Pseudo loss:              3.10428
                Total gradient norm:      0.27252
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.290
                
Iteration (26900/50001) took 0.068 seconds.
                Mean final reward:        9.6562
                Mean return:              5.2188
                Policy entropy:           0.5937
                Pseudo loss:              2.58240
                Total gradient norm:      0.25632
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.290
                
Iteration (27000/50001) took 0.156 seconds.
                Mean final reward:        9.6562
                Mean return:              4.8750
                Policy entropy:           0.6105
                Pseudo loss:              2.84855
                Total gradient norm:      0.38786
                Solved trajectories:      29 / 32
                Avg steps to solve:       5.345
                
Iteration (27100/50001) took 0.072 seconds.
                Mean final reward:        9.6562
                Mean return:              5.0000
                Policy entropy:           0.5991
                Pseudo loss:              2.68541
                Total gradient norm:      0.26227
                Solved trajectories:      29 / 32
                Avg steps to solve:       5.207
                
Iteration (27200/50001) took 0.136 seconds.
                Mean final reward:        9.3125
                Mean return:              4.6875
                Policy entropy:           0.5979
                Pseudo loss:              2.87061
                Total gradient norm:      0.42583
                Solved trajectories:      29 / 32
                Avg steps to solve:       5.172
                
Iteration (27300/50001) took 0.158 seconds.
                Mean final reward:        10.0000
                Mean return:              5.3750
                Policy entropy:           0.5841
                Pseudo loss:              3.29913
                Total gradient norm:      0.23140
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.484
                
Iteration (27400/50001) took 0.114 seconds.
                Mean final reward:        9.6562
                Mean return:              5.3125
                Policy entropy:           0.5607
                Pseudo loss:              2.82419
                Total gradient norm:      0.29554
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.194
                
Iteration (27500/50001) took 0.127 seconds.
                Mean final reward:        9.6562
                Mean return:              5.3438
                Policy entropy:           0.5568
                Pseudo loss:              3.03704
                Total gradient norm:      0.29595
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.161
                
Iteration (27600/50001) took 0.157 seconds.
                Mean final reward:        9.6562
                Mean return:              4.6562
                Policy entropy:           0.6056
                Pseudo loss:              3.50405
                Total gradient norm:      0.19455
                Solved trajectories:      28 / 32
                Avg steps to solve:       5.429
                
Iteration (27700/50001) took 0.059 seconds.
                Mean final reward:        10.0000
                Mean return:              5.1875
                Policy entropy:           0.5644
                Pseudo loss:              3.99708
                Total gradient norm:      0.36122
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.677
                
Iteration (27800/50001) took 0.141 seconds.
                Mean final reward:        9.6562
                Mean return:              4.4375
                Policy entropy:           0.5955
                Pseudo loss:              4.17197
                Total gradient norm:      0.37762
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.967
                
Iteration (27900/50001) took 0.066 seconds.
                Mean final reward:        10.0000
                Mean return:              6.0938
                Policy entropy:           0.5531
                Pseudo loss:              2.62596
                Total gradient norm:      0.27244
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.906
                
Iteration (28000/50001) took 0.106 seconds.
                Mean final reward:        9.6562
                Mean return:              4.7188
                Policy entropy:           0.5916
                Pseudo loss:              3.65398
                Total gradient norm:      0.40205
                Solved trajectories:      29 / 32
                Avg steps to solve:       5.517
                
Iteration (28100/50001) took 0.126 seconds.
                Mean final reward:        9.6562
                Mean return:              4.9688
                Policy entropy:           0.5760
                Pseudo loss:              3.48993
                Total gradient norm:      0.34199
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.548
                
Iteration (28200/50001) took 0.077 seconds.
                Mean final reward:        9.6562
                Mean return:              5.5938
                Policy entropy:           0.5514
                Pseudo loss:              2.41731
                Total gradient norm:      0.37695
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.903
                
Iteration (28300/50001) took 0.061 seconds.
                Mean final reward:        9.3125
                Mean return:              4.1250
                Policy entropy:           0.6147
                Pseudo loss:              3.46776
                Total gradient norm:      0.33945
                Solved trajectories:      29 / 32
                Avg steps to solve:       5.793
                
Iteration (28400/50001) took 0.073 seconds.
                Mean final reward:        8.9688
                Mean return:              4.0938
                Policy entropy:           0.5968
                Pseudo loss:              3.09627
                Total gradient norm:      0.32105
                Solved trajectories:      27 / 32
                Avg steps to solve:       5.111
                
Iteration (28500/50001) took 0.079 seconds.
                Mean final reward:        9.3125
                Mean return:              5.1562
                Policy entropy:           0.5682
                Pseudo loss:              2.45998
                Total gradient norm:      0.28589
                Solved trajectories:      28 / 32
                Avg steps to solve:       4.464
                
Iteration (28600/50001) took 0.096 seconds.
                Mean final reward:        10.0000
                Mean return:              4.8438
                Policy entropy:           0.6010
                Pseudo loss:              4.16156
                Total gradient norm:      0.32986
                Solved trajectories:      28 / 32
                Avg steps to solve:       5.607
                
Iteration (28700/50001) took 0.107 seconds.
                Mean final reward:        9.3125
                Mean return:              5.0625
                Policy entropy:           0.5951
                Pseudo loss:              2.37179
                Total gradient norm:      0.24133
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.933
                
Iteration (28800/50001) took 0.102 seconds.
                Mean final reward:        10.0000
                Mean return:              5.8125
                Policy entropy:           0.5572
                Pseudo loss:              2.74359
                Total gradient norm:      0.28982
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.032
                
Iteration (28900/50001) took 0.086 seconds.
                Mean final reward:        9.6562
                Mean return:              4.8438
                Policy entropy:           0.5999
                Pseudo loss:              3.13213
                Total gradient norm:      0.22935
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.533
                
Iteration (29000/50001) took 0.123 seconds.
                Mean final reward:        10.0000
                Mean return:              5.0938
                Policy entropy:           0.5904
                Pseudo loss:              3.79240
                Total gradient norm:      0.30247
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.906
                
Iteration (29100/50001) took 0.153 seconds.
                Mean final reward:        8.9688
                Mean return:              4.1250
                Policy entropy:           0.6022
                Pseudo loss:              3.55883
                Total gradient norm:      0.31135
                Solved trajectories:      28 / 32
                Avg steps to solve:       5.250
                
Iteration (29200/50001) took 0.154 seconds.
                Mean final reward:        10.0000
                Mean return:              5.7500
                Policy entropy:           0.5575
                Pseudo loss:              2.67285
                Total gradient norm:      0.34510
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.933
                
Iteration (29300/50001) took 0.089 seconds.
                Mean final reward:        10.0000
                Mean return:              5.4375
                Policy entropy:           0.5957
                Pseudo loss:              2.59673
                Total gradient norm:      0.30043
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.562
                
Iteration (29400/50001) took 0.116 seconds.
                Mean final reward:        9.3125
                Mean return:              5.1875
                Policy entropy:           0.5732
                Pseudo loss:              2.38610
                Total gradient norm:      0.18611
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.800
                
Iteration (29500/50001) took 0.075 seconds.
                Mean final reward:        9.6562
                Mean return:              5.2188
                Policy entropy:           0.5658
                Pseudo loss:              3.03343
                Total gradient norm:      0.26453
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.133
                
Iteration (29600/50001) took 0.124 seconds.
                Mean final reward:        10.0000
                Mean return:              5.1250
                Policy entropy:           0.5832
                Pseudo loss:              3.46968
                Total gradient norm:      0.42531
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.742
                
Iteration (29700/50001) took 0.080 seconds.
                Mean final reward:        9.6562
                Mean return:              4.9688
                Policy entropy:           0.5866
                Pseudo loss:              3.51231
                Total gradient norm:      0.33339
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.548
                
Iteration (29800/50001) took 0.117 seconds.
                Mean final reward:        9.6562
                Mean return:              5.0312
                Policy entropy:           0.5950
                Pseudo loss:              3.83733
                Total gradient norm:      0.30056
                Solved trajectories:      29 / 32
                Avg steps to solve:       5.172
                
Iteration (29900/50001) took 0.136 seconds.
                Mean final reward:        9.6562
                Mean return:              5.0625
                Policy entropy:           0.6020
                Pseudo loss:              2.79583
                Total gradient norm:      0.30568
                Solved trajectories:      29 / 32
                Avg steps to solve:       5.138
                
Iteration (30000/50001) took 0.125 seconds.
                Mean final reward:        9.6562
                Mean return:              5.5312
                Policy entropy:           0.5435
                Pseudo loss:              3.00752
                Total gradient norm:      0.28837
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.968
                
Iteration (30100/50001) took 0.126 seconds.
                Mean final reward:        10.0000
                Mean return:              6.0312
                Policy entropy:           0.5458
                Pseudo loss:              2.70040
                Total gradient norm:      0.29989
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.969
                
Iteration (30200/50001) took 0.102 seconds.
                Mean final reward:        9.6562
                Mean return:              4.9375
                Policy entropy:           0.6023
                Pseudo loss:              2.80800
                Total gradient norm:      0.55416
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.581
                
Iteration (30300/50001) took 0.094 seconds.
                Mean final reward:        9.6562
                Mean return:              4.7500
                Policy entropy:           0.5941
                Pseudo loss:              2.94697
                Total gradient norm:      0.40696
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.774
                
Iteration (30400/50001) took 0.102 seconds.
                Mean final reward:        10.0000
                Mean return:              5.0312
                Policy entropy:           0.6001
                Pseudo loss:              3.67364
                Total gradient norm:      0.27868
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.700
                
Iteration (30500/50001) took 0.083 seconds.
                Mean final reward:        9.6562
                Mean return:              5.2812
                Policy entropy:           0.5822
                Pseudo loss:              2.35193
                Total gradient norm:      0.36227
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.226
                
Iteration (30600/50001) took 0.086 seconds.
                Mean final reward:        9.3125
                Mean return:              4.4688
                Policy entropy:           0.6064
                Pseudo loss:              3.46804
                Total gradient norm:      0.36182
                Solved trajectories:      29 / 32
                Avg steps to solve:       5.414
                
Iteration (30700/50001) took 0.114 seconds.
                Mean final reward:        9.6562
                Mean return:              5.3750
                Policy entropy:           0.5586
                Pseudo loss:              3.90354
                Total gradient norm:      0.31573
                Solved trajectories:      29 / 32
                Avg steps to solve:       4.793
                
Iteration (30800/50001) took 0.147 seconds.
                Mean final reward:        10.0000
                Mean return:              5.1562
                Policy entropy:           0.5833
                Pseudo loss:              3.60851
                Total gradient norm:      0.34859
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.567
                
Iteration (30900/50001) took 0.149 seconds.
                Mean final reward:        10.0000
                Mean return:              4.8438
                Policy entropy:           0.6032
                Pseudo loss:              3.43475
                Total gradient norm:      0.32979
                Solved trajectories:      32 / 32
                Avg steps to solve:       6.156
                
Iteration (31000/50001) took 0.173 seconds.
                Mean final reward:        10.0000
                Mean return:              6.0000
                Policy entropy:           0.5470
                Pseudo loss:              2.79061
                Total gradient norm:      0.26467
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.839
                
Iteration (31100/50001) took 0.221 seconds.
                Mean final reward:        9.3125
                Mean return:              4.4062
                Policy entropy:           0.6071
                Pseudo loss:              3.12686
                Total gradient norm:      0.27390
                Solved trajectories:      28 / 32
                Avg steps to solve:       5.321
                
Iteration (31200/50001) took 0.168 seconds.
                Mean final reward:        8.9688
                Mean return:              4.0938
                Policy entropy:           0.6054
                Pseudo loss:              2.81880
                Total gradient norm:      0.28011
                Solved trajectories:      28 / 32
                Avg steps to solve:       5.286
                
Iteration (31300/50001) took 0.091 seconds.
                Mean final reward:        10.0000
                Mean return:              5.7812
                Policy entropy:           0.5833
                Pseudo loss:              2.59730
                Total gradient norm:      0.22154
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.219
                
Iteration (31400/50001) took 0.154 seconds.
                Mean final reward:        10.0000
                Mean return:              5.1250
                Policy entropy:           0.5686
                Pseudo loss:              4.23744
                Total gradient norm:      0.36360
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.742
                
Iteration (31500/50001) took 0.118 seconds.
                Mean final reward:        9.6562
                Mean return:              5.0312
                Policy entropy:           0.5962
                Pseudo loss:              2.95803
                Total gradient norm:      0.29135
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.484
                
Iteration (31600/50001) took 0.107 seconds.
                Mean final reward:        8.6250
                Mean return:              3.8125
                Policy entropy:           0.6299
                Pseudo loss:              2.64136
                Total gradient norm:      0.26013
                Solved trajectories:      26 / 32
                Avg steps to solve:       4.846
                
Iteration (31700/50001) took 0.281 seconds.
                Mean final reward:        10.0000
                Mean return:              5.7500
                Policy entropy:           0.5781
                Pseudo loss:              2.60112
                Total gradient norm:      0.25506
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.250
                
Iteration (31800/50001) took 0.152 seconds.
                Mean final reward:        9.6562
                Mean return:              4.9375
                Policy entropy:           0.5745
                Pseudo loss:              3.40156
                Total gradient norm:      0.35659
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.433
                
Iteration (31900/50001) took 0.165 seconds.
                Mean final reward:        10.0000
                Mean return:              5.2188
                Policy entropy:           0.5856
                Pseudo loss:              3.11513
                Total gradient norm:      0.41053
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.645
                
Iteration (32000/50001) took 0.189 seconds.
                Mean final reward:        10.0000
                Mean return:              5.4375
                Policy entropy:           0.5743
                Pseudo loss:              3.87400
                Total gradient norm:      0.32338
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.562
                
Iteration (32100/50001) took 0.297 seconds.
                Mean final reward:        9.3125
                Mean return:              3.8125
                Policy entropy:           0.6311
                Pseudo loss:              3.85448
                Total gradient norm:      0.30850
                Solved trajectories:      28 / 32
                Avg steps to solve:       6.000
                
Iteration (32200/50001) took 0.123 seconds.
                Mean final reward:        8.9688
                Mean return:              4.3125
                Policy entropy:           0.5987
                Pseudo loss:              2.05334
                Total gradient norm:      0.28352
                Solved trajectories:      29 / 32
                Avg steps to solve:       5.207
                
Iteration (32300/50001) took 0.165 seconds.
                Mean final reward:        9.6562
                Mean return:              5.5625
                Policy entropy:           0.5785
                Pseudo loss:              2.88227
                Total gradient norm:      0.26047
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.935
                
Iteration (32400/50001) took 0.184 seconds.
                Mean final reward:        9.6562
                Mean return:              4.9062
                Policy entropy:           0.5863
                Pseudo loss:              3.14560
                Total gradient norm:      0.32434
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.467
                
Iteration (32500/50001) took 0.151 seconds.
                Mean final reward:        9.6562
                Mean return:              4.9688
                Policy entropy:           0.5866
                Pseudo loss:              2.87252
                Total gradient norm:      0.28672
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.400
                
Iteration (32600/50001) took 0.068 seconds.
                Mean final reward:        9.6562
                Mean return:              5.6875
                Policy entropy:           0.5542
                Pseudo loss:              2.79176
                Total gradient norm:      0.29690
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.633
                
Iteration (32700/50001) took 0.193 seconds.
                Mean final reward:        8.9688
                Mean return:              3.8125
                Policy entropy:           0.5969
                Pseudo loss:              3.44476
                Total gradient norm:      0.36119
                Solved trajectories:      28 / 32
                Avg steps to solve:       5.607
                
Iteration (32800/50001) took 0.178 seconds.
                Mean final reward:        10.0000
                Mean return:              5.8125
                Policy entropy:           0.5566
                Pseudo loss:              2.75507
                Total gradient norm:      0.26828
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.032
                
Iteration (32900/50001) took 0.115 seconds.
                Mean final reward:        9.6562
                Mean return:              5.5312
                Policy entropy:           0.5610
                Pseudo loss:              2.46302
                Total gradient norm:      0.23559
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.968
                
Iteration (33000/50001) took 0.075 seconds.
                Mean final reward:        9.6562
                Mean return:              5.0938
                Policy entropy:           0.5950
                Pseudo loss:              3.24141
                Total gradient norm:      0.29897
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.267
                
Iteration (33100/50001) took 0.169 seconds.
                Mean final reward:        9.3125
                Mean return:              4.6250
                Policy entropy:           0.5951
                Pseudo loss:              3.02972
                Total gradient norm:      0.35107
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.400
                
Iteration (33200/50001) took 0.119 seconds.
                Mean final reward:        10.0000
                Mean return:              5.3125
                Policy entropy:           0.5778
                Pseudo loss:              3.57223
                Total gradient norm:      0.35954
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.400
                
Iteration (33300/50001) took 0.142 seconds.
                Mean final reward:        9.6562
                Mean return:              4.7812
                Policy entropy:           0.5928
                Pseudo loss:              3.73127
                Total gradient norm:      0.29161
                Solved trajectories:      28 / 32
                Avg steps to solve:       5.286
                
Iteration (33400/50001) took 0.108 seconds.
                Mean final reward:        9.6562
                Mean return:              5.1250
                Policy entropy:           0.5776
                Pseudo loss:              3.51584
                Total gradient norm:      0.30715
                Solved trajectories:      28 / 32
                Avg steps to solve:       4.893
                
Iteration (33500/50001) took 0.134 seconds.
                Mean final reward:        10.0000
                Mean return:              5.2812
                Policy entropy:           0.5591
                Pseudo loss:              3.37052
                Total gradient norm:      0.25352
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.719
                
Iteration (33600/50001) took 0.129 seconds.
                Mean final reward:        9.3125
                Mean return:              4.2500
                Policy entropy:           0.6071
                Pseudo loss:              3.18739
                Total gradient norm:      0.31617
                Solved trajectories:      27 / 32
                Avg steps to solve:       5.333
                
Iteration (33700/50001) took 0.118 seconds.
                Mean final reward:        9.3125
                Mean return:              4.5938
                Policy entropy:           0.5973
                Pseudo loss:              3.23873
                Total gradient norm:      0.32926
                Solved trajectories:      29 / 32
                Avg steps to solve:       5.276
                
Iteration (33800/50001) took 0.057 seconds.
                Mean final reward:        10.0000
                Mean return:              4.9062
                Policy entropy:           0.5866
                Pseudo loss:              3.67289
                Total gradient norm:      0.26173
                Solved trajectories:      29 / 32
                Avg steps to solve:       5.690
                
Iteration (33900/50001) took 0.067 seconds.
                Mean final reward:        9.6562
                Mean return:              4.2188
                Policy entropy:           0.5974
                Pseudo loss:              4.00608
                Total gradient norm:      0.36424
                Solved trajectories:      29 / 32
                Avg steps to solve:       6.069
                
Iteration (34000/50001) took 0.159 seconds.
                Mean final reward:        9.6562
                Mean return:              5.4375
                Policy entropy:           0.5940
                Pseudo loss:              2.22520
                Total gradient norm:      0.33628
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.900
                
Iteration (34100/50001) took 0.125 seconds.
                Mean final reward:        10.0000
                Mean return:              5.5000
                Policy entropy:           0.5658
                Pseudo loss:              3.01140
                Total gradient norm:      0.26615
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.500
                
Iteration (34200/50001) took 0.137 seconds.
                Mean final reward:        9.6562
                Mean return:              4.7188
                Policy entropy:           0.5774
                Pseudo loss:              3.99497
                Total gradient norm:      0.40847
                Solved trajectories:      27 / 32
                Avg steps to solve:       5.185
                
Iteration (34300/50001) took 0.162 seconds.
                Mean final reward:        10.0000
                Mean return:              4.6875
                Policy entropy:           0.6082
                Pseudo loss:              4.43728
                Total gradient norm:      0.33152
                Solved trajectories:      30 / 32
                Avg steps to solve:       6.067
                
Iteration (34400/50001) took 0.100 seconds.
                Mean final reward:        9.6562
                Mean return:              5.6250
                Policy entropy:           0.5563
                Pseudo loss:              3.02355
                Total gradient norm:      0.20307
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.700
                
Iteration (34500/50001) took 0.093 seconds.
                Mean final reward:        9.6562
                Mean return:              4.5938
                Policy entropy:           0.5913
                Pseudo loss:              3.91127
                Total gradient norm:      0.35443
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.800
                
Iteration (34600/50001) took 0.143 seconds.
                Mean final reward:        10.0000
                Mean return:              5.7188
                Policy entropy:           0.5720
                Pseudo loss:              3.21722
                Total gradient norm:      0.33810
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.129
                
Iteration (34700/50001) took 0.109 seconds.
                Mean final reward:        9.6562
                Mean return:              4.8750
                Policy entropy:           0.6016
                Pseudo loss:              2.87249
                Total gradient norm:      0.28177
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.645
                
Iteration (34800/50001) took 0.254 seconds.
                Mean final reward:        9.3125
                Mean return:              4.4062
                Policy entropy:           0.5826
                Pseudo loss:              3.23581
                Total gradient norm:      0.30705
                Solved trajectories:      28 / 32
                Avg steps to solve:       5.321
                
Iteration (34900/50001) took 0.100 seconds.
                Mean final reward:        9.6562
                Mean return:              4.7500
                Policy entropy:           0.5893
                Pseudo loss:              3.85433
                Total gradient norm:      0.50908
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.633
                
Iteration (35000/50001) took 0.177 seconds.
                Mean final reward:        9.6562
                Mean return:              5.2188
                Policy entropy:           0.5931
                Pseudo loss:              3.29886
                Total gradient norm:      0.34482
                Solved trajectories:      29 / 32
                Avg steps to solve:       4.966
                
Iteration (35100/50001) took 0.127 seconds.
                Mean final reward:        9.6562
                Mean return:              5.4062
                Policy entropy:           0.5580
                Pseudo loss:              2.95600
                Total gradient norm:      0.33855
                Solved trajectories:      29 / 32
                Avg steps to solve:       4.759
                
Iteration (35200/50001) took 0.119 seconds.
                Mean final reward:        10.0000
                Mean return:              6.0000
                Policy entropy:           0.5505
                Pseudo loss:              3.00701
                Total gradient norm:      0.31598
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.000
                
Iteration (35300/50001) took 0.252 seconds.
                Mean final reward:        9.6562
                Mean return:              5.7812
                Policy entropy:           0.5363
                Pseudo loss:              2.66840
                Total gradient norm:      0.27628
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.710
                
Iteration (35400/50001) took 0.114 seconds.
                Mean final reward:        10.0000
                Mean return:              5.5938
                Policy entropy:           0.5844
                Pseudo loss:              3.15514
                Total gradient norm:      0.29924
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.406
                
Iteration (35500/50001) took 0.059 seconds.
                Mean final reward:        10.0000
                Mean return:              5.6250
                Policy entropy:           0.5533
                Pseudo loss:              3.00625
                Total gradient norm:      0.30805
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.226
                
Iteration (35600/50001) took 0.063 seconds.
                Mean final reward:        9.3125
                Mean return:              4.6562
                Policy entropy:           0.6007
                Pseudo loss:              3.66346
                Total gradient norm:      0.36441
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.367
                
Iteration (35700/50001) took 0.094 seconds.
                Mean final reward:        9.3125
                Mean return:              4.3750
                Policy entropy:           0.6149
                Pseudo loss:              3.69792
                Total gradient norm:      0.32332
                Solved trajectories:      29 / 32
                Avg steps to solve:       5.517
                
Iteration (35800/50001) took 0.066 seconds.
                Mean final reward:        10.0000
                Mean return:              5.6562
                Policy entropy:           0.5543
                Pseudo loss:              3.12281
                Total gradient norm:      0.21740
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.194
                
Iteration (35900/50001) took 0.121 seconds.
                Mean final reward:        9.6562
                Mean return:              4.4062
                Policy entropy:           0.6001
                Pseudo loss:              3.53124
                Total gradient norm:      0.24870
                Solved trajectories:      30 / 32
                Avg steps to solve:       6.000
                
Iteration (36000/50001) took 0.125 seconds.
                Mean final reward:        10.0000
                Mean return:              6.0000
                Policy entropy:           0.5364
                Pseudo loss:              3.79536
                Total gradient norm:      0.36443
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.000
                
Iteration (36100/50001) took 0.104 seconds.
                Mean final reward:        10.0000
                Mean return:              5.4375
                Policy entropy:           0.5717
                Pseudo loss:              3.12089
                Total gradient norm:      0.36125
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.267
                
Iteration (36200/50001) took 0.125 seconds.
                Mean final reward:        10.0000
                Mean return:              5.8750
                Policy entropy:           0.5458
                Pseudo loss:              3.28639
                Total gradient norm:      0.28463
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.968
                
Iteration (36300/50001) took 0.107 seconds.
                Mean final reward:        9.6562
                Mean return:              5.3438
                Policy entropy:           0.5557
                Pseudo loss:              2.69872
                Total gradient norm:      0.25630
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.161
                
Iteration (36400/50001) took 0.128 seconds.
                Mean final reward:        10.0000
                Mean return:              5.0625
                Policy entropy:           0.5857
                Pseudo loss:              4.29101
                Total gradient norm:      0.31391
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.938
                
Iteration (36500/50001) took 0.121 seconds.
                Mean final reward:        8.2812
                Mean return:              2.5938
                Policy entropy:           0.6338
                Pseudo loss:              3.43800
                Total gradient norm:      0.32315
                Solved trajectories:      27 / 32
                Avg steps to solve:       6.074
                
Iteration (36600/50001) took 0.142 seconds.
                Mean final reward:        10.0000
                Mean return:              5.5312
                Policy entropy:           0.5791
                Pseudo loss:              3.30736
                Total gradient norm:      0.31430
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.469
                
Iteration (36700/50001) took 0.093 seconds.
                Mean final reward:        10.0000
                Mean return:              6.1562
                Policy entropy:           0.5524
                Pseudo loss:              3.14851
                Total gradient norm:      0.33353
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.844
                
Iteration (36800/50001) took 0.098 seconds.
                Mean final reward:        8.9688
                Mean return:              4.7500
                Policy entropy:           0.5688
                Pseudo loss:              2.11441
                Total gradient norm:      0.19079
                Solved trajectories:      29 / 32
                Avg steps to solve:       4.724
                
Iteration (36900/50001) took 0.124 seconds.
                Mean final reward:        9.6562
                Mean return:              4.6562
                Policy entropy:           0.5988
                Pseudo loss:              3.60085
                Total gradient norm:      0.27902
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.733
                
Iteration (37000/50001) took 0.109 seconds.
                Mean final reward:        10.0000
                Mean return:              5.5625
                Policy entropy:           0.5871
                Pseudo loss:              2.68631
                Total gradient norm:      0.30695
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.438
                
Iteration (37100/50001) took 0.090 seconds.
                Mean final reward:        10.0000
                Mean return:              5.4688
                Policy entropy:           0.5666
                Pseudo loss:              3.30882
                Total gradient norm:      0.29147
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.531
                
Iteration (37200/50001) took 0.122 seconds.
                Mean final reward:        9.6562
                Mean return:              5.2188
                Policy entropy:           0.5675
                Pseudo loss:              3.88254
                Total gradient norm:      0.32677
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.133
                
Iteration (37300/50001) took 0.081 seconds.
                Mean final reward:        8.6250
                Mean return:              3.0312
                Policy entropy:           0.6230
                Pseudo loss:              3.42108
                Total gradient norm:      0.27656
                Solved trajectories:      28 / 32
                Avg steps to solve:       6.107
                
Iteration (37400/50001) took 0.065 seconds.
                Mean final reward:        10.0000
                Mean return:              5.3438
                Policy entropy:           0.5769
                Pseudo loss:              3.13291
                Total gradient norm:      0.26768
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.656
                
Iteration (37500/50001) took 0.081 seconds.
                Mean final reward:        10.0000
                Mean return:              5.5000
                Policy entropy:           0.5876
                Pseudo loss:              2.75524
                Total gradient norm:      0.29128
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.500
                
Iteration (37600/50001) took 0.078 seconds.
                Mean final reward:        10.0000
                Mean return:              6.0000
                Policy entropy:           0.5536
                Pseudo loss:              3.05589
                Total gradient norm:      0.37025
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.839
                
Iteration (37700/50001) took 0.069 seconds.
                Mean final reward:        10.0000
                Mean return:              5.7812
                Policy entropy:           0.5764
                Pseudo loss:              2.61708
                Total gradient norm:      0.24727
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.219
                
Iteration (37800/50001) took 0.114 seconds.
                Mean final reward:        10.0000
                Mean return:              5.3438
                Policy entropy:           0.5775
                Pseudo loss:              3.25457
                Total gradient norm:      0.34948
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.516
                
Iteration (37900/50001) took 0.125 seconds.
                Mean final reward:        9.6562
                Mean return:              4.7500
                Policy entropy:           0.5789
                Pseudo loss:              3.51772
                Total gradient norm:      0.31028
                Solved trajectories:      29 / 32
                Avg steps to solve:       5.483
                
Iteration (38000/50001) took 0.148 seconds.
                Mean final reward:        10.0000
                Mean return:              5.5625
                Policy entropy:           0.5635
                Pseudo loss:              2.39566
                Total gradient norm:      0.23689
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.438
                
Iteration (38100/50001) took 0.108 seconds.
                Mean final reward:        9.6562
                Mean return:              4.7500
                Policy entropy:           0.5802
                Pseudo loss:              3.64755
                Total gradient norm:      0.27771
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.633
                
Iteration (38200/50001) took 0.078 seconds.
                Mean final reward:        10.0000
                Mean return:              5.1875
                Policy entropy:           0.5714
                Pseudo loss:              3.50212
                Total gradient norm:      0.32793
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.812
                
Iteration (38300/50001) took 0.077 seconds.
                Mean final reward:        9.3125
                Mean return:              4.5625
                Policy entropy:           0.5961
                Pseudo loss:              2.83156
                Total gradient norm:      0.40118
                Solved trajectories:      29 / 32
                Avg steps to solve:       5.310
                
Iteration (38400/50001) took 0.050 seconds.
                Mean final reward:        10.0000
                Mean return:              5.9688
                Policy entropy:           0.5438
                Pseudo loss:              3.24097
                Total gradient norm:      0.34582
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.871
                
Iteration (38500/50001) took 0.058 seconds.
                Mean final reward:        9.6562
                Mean return:              4.5000
                Policy entropy:           0.6208
                Pseudo loss:              2.96901
                Total gradient norm:      0.35213
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.900
                
Iteration (38600/50001) took 0.053 seconds.
                Mean final reward:        9.6562
                Mean return:              5.2812
                Policy entropy:           0.5758
                Pseudo loss:              2.79316
                Total gradient norm:      0.20991
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.226
                
Iteration (38700/50001) took 0.053 seconds.
                Mean final reward:        10.0000
                Mean return:              5.9375
                Policy entropy:           0.5510
                Pseudo loss:              2.86517
                Total gradient norm:      0.28384
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.903
                
Iteration (38800/50001) took 0.057 seconds.
                Mean final reward:        10.0000
                Mean return:              6.1250
                Policy entropy:           0.5474
                Pseudo loss:              2.61386
                Total gradient norm:      0.19565
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.875
                
Iteration (38900/50001) took 0.088 seconds.
                Mean final reward:        9.6562
                Mean return:              5.4062
                Policy entropy:           0.5722
                Pseudo loss:              2.91707
                Total gradient norm:      0.26602
                Solved trajectories:      29 / 32
                Avg steps to solve:       4.759
                
Iteration (39000/50001) took 0.061 seconds.
                Mean final reward:        10.0000
                Mean return:              5.3125
                Policy entropy:           0.5851
                Pseudo loss:              3.18052
                Total gradient norm:      0.32718
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.400
                
Iteration (39100/50001) took 0.069 seconds.
                Mean final reward:        9.6562
                Mean return:              5.2500
                Policy entropy:           0.5578
                Pseudo loss:              3.60252
                Total gradient norm:      0.38340
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.100
                
Iteration (39200/50001) took 0.068 seconds.
                Mean final reward:        10.0000
                Mean return:              5.9062
                Policy entropy:           0.5439
                Pseudo loss:              2.63663
                Total gradient norm:      0.22882
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.935
                
Iteration (39300/50001) took 0.086 seconds.
                Mean final reward:        10.0000
                Mean return:              5.5625
                Policy entropy:           0.5494
                Pseudo loss:              2.89591
                Total gradient norm:      0.25147
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.438
                
Iteration (39400/50001) took 0.061 seconds.
                Mean final reward:        9.6562
                Mean return:              4.8750
                Policy entropy:           0.5895
                Pseudo loss:              2.86356
                Total gradient norm:      0.26696
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.500
                
Iteration (39500/50001) took 0.057 seconds.
                Mean final reward:        10.0000
                Mean return:              5.4375
                Policy entropy:           0.5871
                Pseudo loss:              2.79193
                Total gradient norm:      0.27495
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.419
                
Iteration (39600/50001) took 0.066 seconds.
                Mean final reward:        9.3125
                Mean return:              4.1875
                Policy entropy:           0.5881
                Pseudo loss:              3.84367
                Total gradient norm:      0.29940
                Solved trajectories:      28 / 32
                Avg steps to solve:       5.571
                
Iteration (39700/50001) took 0.059 seconds.
                Mean final reward:        9.6562
                Mean return:              4.7812
                Policy entropy:           0.5882
                Pseudo loss:              3.16821
                Total gradient norm:      0.24450
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.600
                
Iteration (39800/50001) took 0.053 seconds.
                Mean final reward:        10.0000
                Mean return:              5.8438
                Policy entropy:           0.5477
                Pseudo loss:              3.09369
                Total gradient norm:      0.25158
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.156
                
Iteration (39900/50001) took 0.061 seconds.
                Mean final reward:        10.0000
                Mean return:              4.7188
                Policy entropy:           0.6033
                Pseudo loss:              4.14292
                Total gradient norm:      0.32432
                Solved trajectories:      28 / 32
                Avg steps to solve:       5.750
                
Iteration (40000/50001) took 0.037 seconds.
                Mean final reward:        9.6562
                Mean return:              5.3750
                Policy entropy:           0.5670
                Pseudo loss:              3.21431
                Total gradient norm:      0.27890
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.967
                
Iteration (40100/50001) took 0.033 seconds.
                Mean final reward:        10.0000
                Mean return:              6.5000
                Policy entropy:           0.5330
                Pseudo loss:              2.34143
                Total gradient norm:      0.22043
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.323
                
Iteration (40200/50001) took 0.058 seconds.
                Mean final reward:        10.0000
                Mean return:              5.5625
                Policy entropy:           0.5675
                Pseudo loss:              2.96657
                Total gradient norm:      0.23975
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.438
                
Iteration (40300/50001) took 0.055 seconds.
                Mean final reward:        9.6562
                Mean return:              5.3750
                Policy entropy:           0.5622
                Pseudo loss:              2.99905
                Total gradient norm:      0.26356
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.129
                
Iteration (40400/50001) took 0.042 seconds.
                Mean final reward:        9.6562
                Mean return:              5.3750
                Policy entropy:           0.5630
                Pseudo loss:              2.53053
                Total gradient norm:      0.21895
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.129
                
Iteration (40500/50001) took 0.047 seconds.
                Mean final reward:        10.0000
                Mean return:              5.3125
                Policy entropy:           0.5673
                Pseudo loss:              3.79142
                Total gradient norm:      0.48126
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.688
                
Iteration (40600/50001) took 0.037 seconds.
                Mean final reward:        9.6562
                Mean return:              4.6562
                Policy entropy:           0.5966
                Pseudo loss:              3.66031
                Total gradient norm:      0.30224
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.733
                
Iteration (40700/50001) took 0.037 seconds.
                Mean final reward:        8.9688
                Mean return:              4.0625
                Policy entropy:           0.5870
                Pseudo loss:              3.28950
                Total gradient norm:      0.35390
                Solved trajectories:      27 / 32
                Avg steps to solve:       5.148
                
Iteration (40800/50001) took 0.040 seconds.
                Mean final reward:        8.9688
                Mean return:              3.7188
                Policy entropy:           0.5976
                Pseudo loss:              3.71480
                Total gradient norm:      0.38741
                Solved trajectories:      24 / 32
                Avg steps to solve:       5.000
                
Iteration (40900/50001) took 0.035 seconds.
                Mean final reward:        9.3125
                Mean return:              4.8750
                Policy entropy:           0.5728
                Pseudo loss:              3.38407
                Total gradient norm:      0.31519
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.133
                
Iteration (41000/50001) took 0.042 seconds.
                Mean final reward:        10.0000
                Mean return:              4.6875
                Policy entropy:           0.6087
                Pseudo loss:              3.64275
                Total gradient norm:      0.31644
                Solved trajectories:      31 / 32
                Avg steps to solve:       6.194
                
Iteration (41100/50001) took 0.038 seconds.
                Mean final reward:        10.0000
                Mean return:              5.8125
                Policy entropy:           0.5450
                Pseudo loss:              3.02770
                Total gradient norm:      0.28997
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.188
                
Iteration (41200/50001) took 0.055 seconds.
                Mean final reward:        10.0000
                Mean return:              5.5000
                Policy entropy:           0.5762
                Pseudo loss:              3.38262
                Total gradient norm:      0.28185
                Solved trajectories:      29 / 32
                Avg steps to solve:       5.034
                
Iteration (41300/50001) took 0.041 seconds.
                Mean final reward:        9.6562
                Mean return:              5.0625
                Policy entropy:           0.5845
                Pseudo loss:              2.74819
                Total gradient norm:      0.23094
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.452
                
Iteration (41400/50001) took 0.036 seconds.
                Mean final reward:        9.6562
                Mean return:              4.8438
                Policy entropy:           0.5845
                Pseudo loss:              3.37605
                Total gradient norm:      0.31238
                Solved trajectories:      29 / 32
                Avg steps to solve:       5.379
                
Iteration (41500/50001) took 0.034 seconds.
                Mean final reward:        10.0000
                Mean return:              5.8750
                Policy entropy:           0.5582
                Pseudo loss:              2.81052
                Total gradient norm:      0.27936
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.968
                
Iteration (41600/50001) took 0.035 seconds.
                Mean final reward:        9.3125
                Mean return:              4.5625
                Policy entropy:           0.6018
                Pseudo loss:              3.43399
                Total gradient norm:      0.26942
                Solved trajectories:      28 / 32
                Avg steps to solve:       5.143
                
Iteration (41700/50001) took 0.041 seconds.
                Mean final reward:        9.6562
                Mean return:              4.3438
                Policy entropy:           0.5971
                Pseudo loss:              4.07241
                Total gradient norm:      0.38374
                Solved trajectories:      27 / 32
                Avg steps to solve:       5.630
                
Iteration (41800/50001) took 0.049 seconds.
                Mean final reward:        10.0000
                Mean return:              5.9688
                Policy entropy:           0.5619
                Pseudo loss:              2.45049
                Total gradient norm:      0.20797
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.871
                
Iteration (41900/50001) took 0.040 seconds.
                Mean final reward:        8.9688
                Mean return:              3.8438
                Policy entropy:           0.6022
                Pseudo loss:              3.11230
                Total gradient norm:      0.31274
                Solved trajectories:      29 / 32
                Avg steps to solve:       5.724
                
Iteration (42000/50001) took 0.052 seconds.
                Mean final reward:        10.0000
                Mean return:              5.6250
                Policy entropy:           0.5596
                Pseudo loss:              2.95434
                Total gradient norm:      0.29867
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.375
                
Iteration (42100/50001) took 0.050 seconds.
                Mean final reward:        8.9688
                Mean return:              4.9375
                Policy entropy:           0.5490
                Pseudo loss:              2.12423
                Total gradient norm:      0.23004
                Solved trajectories:      29 / 32
                Avg steps to solve:       4.517
                
Iteration (42200/50001) took 0.038 seconds.
                Mean final reward:        10.0000
                Mean return:              5.0625
                Policy entropy:           0.5550
                Pseudo loss:              3.92942
                Total gradient norm:      0.34191
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.667
                
Iteration (42300/50001) took 0.036 seconds.
                Mean final reward:        10.0000
                Mean return:              5.1875
                Policy entropy:           0.5629
                Pseudo loss:              3.98145
                Total gradient norm:      0.34445
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.812
                
Iteration (42400/50001) took 0.050 seconds.
                Mean final reward:        10.0000
                Mean return:              5.3125
                Policy entropy:           0.5780
                Pseudo loss:              3.23732
                Total gradient norm:      0.30951
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.400
                
Iteration (42500/50001) took 0.058 seconds.
                Mean final reward:        9.3125
                Mean return:              4.2500
                Policy entropy:           0.5880
                Pseudo loss:              3.98189
                Total gradient norm:      0.46203
                Solved trajectories:      28 / 32
                Avg steps to solve:       5.500
                
Iteration (42600/50001) took 0.038 seconds.
                Mean final reward:        10.0000
                Mean return:              5.1875
                Policy entropy:           0.5685
                Pseudo loss:              3.30805
                Total gradient norm:      0.32870
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.677
                
Iteration (42700/50001) took 0.057 seconds.
                Mean final reward:        10.0000
                Mean return:              5.0938
                Policy entropy:           0.5879
                Pseudo loss:              3.52329
                Total gradient norm:      0.31011
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.633
                
Iteration (42800/50001) took 0.039 seconds.
                Mean final reward:        9.6562
                Mean return:              4.5938
                Policy entropy:           0.5765
                Pseudo loss:              3.45710
                Total gradient norm:      0.32191
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.935
                
Iteration (42900/50001) took 0.036 seconds.
                Mean final reward:        9.6562
                Mean return:              5.0312
                Policy entropy:           0.5654
                Pseudo loss:              3.50741
                Total gradient norm:      0.31299
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.484
                
Iteration (43000/50001) took 0.033 seconds.
                Mean final reward:        10.0000
                Mean return:              5.8750
                Policy entropy:           0.5545
                Pseudo loss:              2.43348
                Total gradient norm:      0.24198
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.968
                
Iteration (43100/50001) took 0.039 seconds.
                Mean final reward:        9.6562
                Mean return:              4.6875
                Policy entropy:           0.5950
                Pseudo loss:              3.52364
                Total gradient norm:      0.27165
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.700
                
Iteration (43200/50001) took 0.059 seconds.
                Mean final reward:        9.6562
                Mean return:              5.0312
                Policy entropy:           0.5928
                Pseudo loss:              2.99720
                Total gradient norm:      0.23664
                Solved trajectories:      29 / 32
                Avg steps to solve:       5.172
                
Iteration (43300/50001) took 0.039 seconds.
                Mean final reward:        9.3125
                Mean return:              4.2812
                Policy entropy:           0.5829
                Pseudo loss:              4.16619
                Total gradient norm:      0.43570
                Solved trajectories:      29 / 32
                Avg steps to solve:       5.621
                
Iteration (43400/50001) took 0.053 seconds.
                Mean final reward:        10.0000
                Mean return:              5.6250
                Policy entropy:           0.5618
                Pseudo loss:              3.02410
                Total gradient norm:      0.23831
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.375
                
Iteration (43500/50001) took 0.040 seconds.
                Mean final reward:        10.0000
                Mean return:              5.1250
                Policy entropy:           0.5936
                Pseudo loss:              3.74471
                Total gradient norm:      0.29063
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.742
                
Iteration (43600/50001) took 0.036 seconds.
                Mean final reward:        10.0000
                Mean return:              6.0000
                Policy entropy:           0.5535
                Pseudo loss:              3.15852
                Total gradient norm:      0.32346
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.839
                
Iteration (43700/50001) took 0.035 seconds.
                Mean final reward:        9.6562
                Mean return:              5.4688
                Policy entropy:           0.5506
                Pseudo loss:              2.81041
                Total gradient norm:      0.24719
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.867
                
Iteration (43800/50001) took 0.048 seconds.
                Mean final reward:        10.0000
                Mean return:              6.2812
                Policy entropy:           0.5282
                Pseudo loss:              2.33936
                Total gradient norm:      0.31302
                Solved trajectories:      32 / 32
                Avg steps to solve:       4.719
                
Iteration (43900/50001) took 0.053 seconds.
                Mean final reward:        9.3125
                Mean return:              4.9375
                Policy entropy:           0.5677
                Pseudo loss:              2.82679
                Total gradient norm:      0.28434
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.067
                
Iteration (44000/50001) took 0.056 seconds.
                Mean final reward:        10.0000
                Mean return:              5.3438
                Policy entropy:           0.5701
                Pseudo loss:              3.24503
                Total gradient norm:      0.32105
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.516
                
Iteration (44100/50001) took 0.041 seconds.
                Mean final reward:        10.0000
                Mean return:              5.6562
                Policy entropy:           0.5685
                Pseudo loss:              3.02929
                Total gradient norm:      0.26945
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.344
                
Iteration (44200/50001) took 0.035 seconds.
                Mean final reward:        10.0000
                Mean return:              5.8438
                Policy entropy:           0.5571
                Pseudo loss:              3.04487
                Total gradient norm:      0.34193
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.000
                
Iteration (44300/50001) took 0.057 seconds.
                Mean final reward:        10.0000
                Mean return:              5.1875
                Policy entropy:           0.5947
                Pseudo loss:              3.32810
                Total gradient norm:      0.32759
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.533
                
Iteration (44400/50001) took 0.042 seconds.
                Mean final reward:        8.6250
                Mean return:              3.2812
                Policy entropy:           0.5995
                Pseudo loss:              3.56575
                Total gradient norm:      0.32975
                Solved trajectories:      28 / 32
                Avg steps to solve:       5.821
                
Iteration (44500/50001) took 0.043 seconds.
                Mean final reward:        9.6562
                Mean return:              5.5312
                Policy entropy:           0.5689
                Pseudo loss:              2.75300
                Total gradient norm:      0.30410
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.968
                
Iteration (44600/50001) took 0.036 seconds.
                Mean final reward:        9.6562
                Mean return:              5.0625
                Policy entropy:           0.5783
                Pseudo loss:              3.48274
                Total gradient norm:      0.20954
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.300
                
Iteration (44700/50001) took 0.045 seconds.
                Mean final reward:        9.6562
                Mean return:              4.3438
                Policy entropy:           0.6101
                Pseudo loss:              3.53490
                Total gradient norm:      0.27552
                Solved trajectories:      30 / 32
                Avg steps to solve:       6.067
                
Iteration (44800/50001) took 0.039 seconds.
                Mean final reward:        9.6562
                Mean return:              5.1875
                Policy entropy:           0.5857
                Pseudo loss:              2.62735
                Total gradient norm:      0.28298
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.323
                
Iteration (44900/50001) took 0.039 seconds.
                Mean final reward:        9.3125
                Mean return:              4.3125
                Policy entropy:           0.6069
                Pseudo loss:              3.35300
                Total gradient norm:      0.36045
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.733
                
Iteration (45000/50001) took 0.034 seconds.
                Mean final reward:        9.3125
                Mean return:              4.8438
                Policy entropy:           0.5752
                Pseudo loss:              2.97730
                Total gradient norm:      0.26125
                Solved trajectories:      29 / 32
                Avg steps to solve:       5.000
                
Iteration (45100/50001) took 0.037 seconds.
                Mean final reward:        9.3125
                Mean return:              4.7188
                Policy entropy:           0.5637
                Pseudo loss:              3.25003
                Total gradient norm:      0.33110
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.300
                
Iteration (45200/50001) took 0.039 seconds.
                Mean final reward:        9.6562
                Mean return:              4.8438
                Policy entropy:           0.5877
                Pseudo loss:              3.89872
                Total gradient norm:      0.43951
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.533
                
Iteration (45300/50001) took 0.040 seconds.
                Mean final reward:        10.0000
                Mean return:              5.1562
                Policy entropy:           0.5927
                Pseudo loss:              3.41490
                Total gradient norm:      0.38356
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.710
                
Iteration (45400/50001) took 0.032 seconds.
                Mean final reward:        10.0000
                Mean return:              5.5312
                Policy entropy:           0.5670
                Pseudo loss:              3.79370
                Total gradient norm:      0.45790
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.323
                
Iteration (45500/50001) took 0.033 seconds.
                Mean final reward:        10.0000
                Mean return:              5.6562
                Policy entropy:           0.5655
                Pseudo loss:              2.91693
                Total gradient norm:      0.33703
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.194
                
Iteration (45600/50001) took 0.035 seconds.
                Mean final reward:        10.0000
                Mean return:              5.3750
                Policy entropy:           0.5773
                Pseudo loss:              3.21504
                Total gradient norm:      0.40887
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.333
                
Iteration (45700/50001) took 0.036 seconds.
                Mean final reward:        10.0000
                Mean return:              5.3125
                Policy entropy:           0.5653
                Pseudo loss:              3.61048
                Total gradient norm:      0.33140
                Solved trajectories:      29 / 32
                Avg steps to solve:       5.241
                
Iteration (45800/50001) took 0.034 seconds.
                Mean final reward:        10.0000
                Mean return:              5.8438
                Policy entropy:           0.5692
                Pseudo loss:              3.06608
                Total gradient norm:      0.23185
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.156
                
Iteration (45900/50001) took 0.034 seconds.
                Mean final reward:        9.3125
                Mean return:              4.7500
                Policy entropy:           0.5977
                Pseudo loss:              2.58260
                Total gradient norm:      0.27716
                Solved trajectories:      28 / 32
                Avg steps to solve:       4.929
                
Iteration (46000/50001) took 0.035 seconds.
                Mean final reward:        9.3125
                Mean return:              4.6250
                Policy entropy:           0.5849
                Pseudo loss:              3.05933
                Total gradient norm:      0.28186
                Solved trajectories:      29 / 32
                Avg steps to solve:       5.241
                
Iteration (46100/50001) took 0.036 seconds.
                Mean final reward:        9.3125
                Mean return:              4.0000
                Policy entropy:           0.6089
                Pseudo loss:              3.43023
                Total gradient norm:      0.34886
                Solved trajectories:      30 / 32
                Avg steps to solve:       6.067
                
Iteration (46200/50001) took 0.036 seconds.
                Mean final reward:        9.6562
                Mean return:              4.6875
                Policy entropy:           0.6037
                Pseudo loss:              3.39989
                Total gradient norm:      0.29367
                Solved trajectories:      29 / 32
                Avg steps to solve:       5.552
                
Iteration (46300/50001) took 0.038 seconds.
                Mean final reward:        10.0000
                Mean return:              5.5000
                Policy entropy:           0.5676
                Pseudo loss:              3.23847
                Total gradient norm:      0.28168
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.500
                
Iteration (46400/50001) took 0.036 seconds.
                Mean final reward:        10.0000
                Mean return:              5.3750
                Policy entropy:           0.5626
                Pseudo loss:              3.02811
                Total gradient norm:      0.26366
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.625
                
Iteration (46500/50001) took 0.034 seconds.
                Mean final reward:        10.0000
                Mean return:              5.5625
                Policy entropy:           0.5743
                Pseudo loss:              3.31373
                Total gradient norm:      0.36268
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.290
                
Iteration (46600/50001) took 0.035 seconds.
                Mean final reward:        9.6562
                Mean return:              5.0938
                Policy entropy:           0.5676
                Pseudo loss:              2.91200
                Total gradient norm:      0.30465
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.419
                
Iteration (46700/50001) took 0.035 seconds.
                Mean final reward:        10.0000
                Mean return:              5.4062
                Policy entropy:           0.5651
                Pseudo loss:              3.43194
                Total gradient norm:      0.22665
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.594
                
Iteration (46800/50001) took 0.035 seconds.
                Mean final reward:        9.6562
                Mean return:              4.7812
                Policy entropy:           0.5821
                Pseudo loss:              3.71243
                Total gradient norm:      0.42028
                Solved trajectories:      28 / 32
                Avg steps to solve:       5.286
                
Iteration (46900/50001) took 0.035 seconds.
                Mean final reward:        9.6562
                Mean return:              4.8438
                Policy entropy:           0.5848
                Pseudo loss:              3.28675
                Total gradient norm:      0.31564
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.677
                
Iteration (47000/50001) took 0.031 seconds.
                Mean final reward:        9.3125
                Mean return:              5.2500
                Policy entropy:           0.5582
                Pseudo loss:              2.44449
                Total gradient norm:      0.36311
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.733
                
Iteration (47100/50001) took 0.037 seconds.
                Mean final reward:        9.6562
                Mean return:              4.7812
                Policy entropy:           0.5721
                Pseudo loss:              4.06888
                Total gradient norm:      0.34814
                Solved trajectories:      29 / 32
                Avg steps to solve:       5.448
                
Iteration (47200/50001) took 0.032 seconds.
                Mean final reward:        9.3125
                Mean return:              5.0938
                Policy entropy:           0.5798
                Pseudo loss:              2.06708
                Total gradient norm:      0.31613
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.900
                
Iteration (47300/50001) took 0.034 seconds.
                Mean final reward:        9.6562
                Mean return:              5.1875
                Policy entropy:           0.5663
                Pseudo loss:              2.89280
                Total gradient norm:      0.27186
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.167
                
Iteration (47400/50001) took 0.036 seconds.
                Mean final reward:        10.0000
                Mean return:              5.3125
                Policy entropy:           0.5652
                Pseudo loss:              3.74121
                Total gradient norm:      0.34892
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.688
                
Iteration (47500/50001) took 0.040 seconds.
                Mean final reward:        8.9688
                Mean return:              3.4688
                Policy entropy:           0.6167
                Pseudo loss:              3.07279
                Total gradient norm:      0.29750
                Solved trajectories:      26 / 32
                Avg steps to solve:       5.692
                
Iteration (47600/50001) took 0.038 seconds.
                Mean final reward:        9.6562
                Mean return:              4.6562
                Policy entropy:           0.5964
                Pseudo loss:              3.19256
                Total gradient norm:      0.37642
                Solved trajectories:      29 / 32
                Avg steps to solve:       5.586
                
Iteration (47700/50001) took 0.039 seconds.
                Mean final reward:        10.0000
                Mean return:              5.4688
                Policy entropy:           0.5615
                Pseudo loss:              3.12236
                Total gradient norm:      0.23465
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.387
                
Iteration (47800/50001) took 0.034 seconds.
                Mean final reward:        10.0000
                Mean return:              5.6250
                Policy entropy:           0.5588
                Pseudo loss:              2.89808
                Total gradient norm:      0.30091
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.375
                
Iteration (47900/50001) took 0.037 seconds.
                Mean final reward:        10.0000
                Mean return:              5.0625
                Policy entropy:           0.5872
                Pseudo loss:              2.88398
                Total gradient norm:      0.31928
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.667
                
Iteration (48000/50001) took 0.035 seconds.
                Mean final reward:        9.6562
                Mean return:              5.4688
                Policy entropy:           0.5540
                Pseudo loss:              2.78785
                Total gradient norm:      0.26819
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.867
                
Iteration (48100/50001) took 0.039 seconds.
                Mean final reward:        9.6562
                Mean return:              4.6250
                Policy entropy:           0.6001
                Pseudo loss:              3.74248
                Total gradient norm:      0.31996
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.767
                
Iteration (48200/50001) took 0.036 seconds.
                Mean final reward:        9.6562
                Mean return:              4.7812
                Policy entropy:           0.5802
                Pseudo loss:              3.35179
                Total gradient norm:      0.37139
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.600
                
Iteration (48300/50001) took 0.034 seconds.
                Mean final reward:        10.0000
                Mean return:              5.6250
                Policy entropy:           0.5569
                Pseudo loss:              2.83347
                Total gradient norm:      0.29990
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.067
                
Iteration (48400/50001) took 0.038 seconds.
                Mean final reward:        9.6562
                Mean return:              5.2500
                Policy entropy:           0.5721
                Pseudo loss:              3.22182
                Total gradient norm:      0.25815
                Solved trajectories:      29 / 32
                Avg steps to solve:       4.931
                
Iteration (48500/50001) took 0.034 seconds.
                Mean final reward:        9.6562
                Mean return:              5.0000
                Policy entropy:           0.5643
                Pseudo loss:              3.58241
                Total gradient norm:      0.36819
                Solved trajectories:      29 / 32
                Avg steps to solve:       5.207
                
Iteration (48600/50001) took 0.036 seconds.
                Mean final reward:        10.0000
                Mean return:              5.0312
                Policy entropy:           0.5835
                Pseudo loss:              4.31574
                Total gradient norm:      0.39764
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.839
                
Iteration (48700/50001) took 0.036 seconds.
                Mean final reward:        9.3125
                Mean return:              4.4688
                Policy entropy:           0.5897
                Pseudo loss:              2.91817
                Total gradient norm:      0.24811
                Solved trajectories:      28 / 32
                Avg steps to solve:       5.250
                
Iteration (48800/50001) took 0.041 seconds.
                Mean final reward:        10.0000
                Mean return:              4.9375
                Policy entropy:           0.5818
                Pseudo loss:              3.45165
                Total gradient norm:      0.42540
                Solved trajectories:      32 / 32
                Avg steps to solve:       6.062
                
Iteration (48900/50001) took 0.039 seconds.
                Mean final reward:        9.6562
                Mean return:              5.3125
                Policy entropy:           0.5677
                Pseudo loss:              2.90900
                Total gradient norm:      0.26915
                Solved trajectories:      31 / 32
                Avg steps to solve:       5.194
                
Iteration (49000/50001) took 0.035 seconds.
                Mean final reward:        10.0000
                Mean return:              5.4688
                Policy entropy:           0.5843
                Pseudo loss:              2.73932
                Total gradient norm:      0.30545
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.233
                
Iteration (49100/50001) took 0.035 seconds.
                Mean final reward:        9.3125
                Mean return:              4.8125
                Policy entropy:           0.5624
                Pseudo loss:              3.13964
                Total gradient norm:      0.37200
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.200
                
Iteration (49200/50001) took 0.037 seconds.
                Mean final reward:        10.0000
                Mean return:              5.4375
                Policy entropy:           0.5771
                Pseudo loss:              3.53669
                Total gradient norm:      0.29894
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.562
                
Iteration (49300/50001) took 0.031 seconds.
                Mean final reward:        10.0000
                Mean return:              6.1562
                Policy entropy:           0.5514
                Pseudo loss:              2.45946
                Total gradient norm:      0.22058
                Solved trajectories:      30 / 32
                Avg steps to solve:       4.500
                
Iteration (49400/50001) took 0.035 seconds.
                Mean final reward:        10.0000
                Mean return:              5.3438
                Policy entropy:           0.5737
                Pseudo loss:              3.73179
                Total gradient norm:      0.33537
                Solved trajectories:      32 / 32
                Avg steps to solve:       5.656
                
Iteration (49500/50001) took 0.038 seconds.
                Mean final reward:        9.3125
                Mean return:              4.0625
                Policy entropy:           0.5862
                Pseudo loss:              3.96304
                Total gradient norm:      0.35784
                Solved trajectories:      27 / 32
                Avg steps to solve:       5.556
                
Iteration (49600/50001) took 0.037 seconds.
                Mean final reward:        10.0000
                Mean return:              5.0312
                Policy entropy:           0.5861
                Pseudo loss:              3.47605
                Total gradient norm:      0.25908
                Solved trajectories:      30 / 32
                Avg steps to solve:       5.700
                
Iteration (49700/50001) took 0.030 seconds.
                Mean final reward:        9.6562
                Mean return:              5.8125
                Policy entropy:           0.5565
                Pseudo loss:              2.11811
                Total gradient norm:      0.26461
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.677
                
Iteration (49800/50001) took 0.038 seconds.
                Mean final reward:        9.3125
                Mean return:              4.4688
                Policy entropy:           0.5967
                Pseudo loss:              3.12442
                Total gradient norm:      0.28934
                Solved trajectories:      28 / 32
                Avg steps to solve:       5.250
                
Iteration (49900/50001) took 0.032 seconds.
                Mean final reward:        10.0000
                Mean return:              6.2500
                Policy entropy:           0.5447
                Pseudo loss:              2.72644
                Total gradient norm:      0.24043
                Solved trajectories:      31 / 32
                Avg steps to solve:       4.581
                
Iteration (50000/50001) took 0.037 seconds.
                Mean final reward:        9.6562
                Mean return:              5.3750
                Policy entropy:           0.5831
                Pseudo loss:              2.93459
                Total gradient norm:      0.37045
                Solved trajectories:      29 / 32
                Avg steps to solve:       4.793
                
Training took 5539.923 seconds.
